{"posts":[{"title":"Bayesian Statistics : A&#x2F;B Testing, Thompson sampling of multi-armed bandits, Recommendation Engines and more from Big Consulting","text":"A friend of mine recently asked me for advice in preparing for an interview that required Bayesian statistics in Consulting. They asked me if I had done anything in Bayesian statistics. I decided to compile a list of representative projects that encapsulate the general idea of Bayesian statistics in consulting. These projects are rudimentary but could serve as a useful interview guide if needed! Exploring Bayesian Methods: From A/B Testing to Recommendation EnginesBayesian methods are like the secret sauce of data science. They add a layer of sophistication to measuring the success of machine learning (ML) algorithms and proving their effectiveness. In this post, I’ll walk you through how Bayesian statistics have shaped my approach to A/B testing and recommendation engines. Whether you’re a data science enthusiast or a seasoned pro, there’s something here for you! Bayesian A/B Testing: A Real-World ExampleImagine you’re working on a project for a large telecommunications company, and your goal is to build a recommendation engine. This engine needs to send automated product recommendations to customers every 15 days. To test whether your new machine learning-based recommendations outperform the traditional heuristic approach, you decide to run an A/B test. Measuring SuccessIn this scenario, we’re interested in measuring whether our recommendation engine performs better than the traditional method. We use a Bayesian approach to compare the click-through rates (CTR) of both methods. Click-through rate is a common metric that measures how often users click on the recommendations they receive. We model the CTR using a beta distribution, which is parameterized by (the number of clicks) and (the number of ignored recommendations). For both the new and traditional campaigns, we start with prior distributions: 1234567891011121314151617181920212223# Bayesian A/B Testing using Beta Distributionimport numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import beta# 1. Introductionprint(\"Bayesian A/B Testing using Beta Distribution\")# 2. Setting Up the Experiment# Let's say that we have two teams inside a marketing company. One of them (A) # uses business intelligence and marketing experience to recommend products to # customers, and the other team (B) uses machine learning to recommend products. # Given a recommendation, a customer can either accept (successes) or reject it. # Each team also gets a sample of customers they can expose their # recommendations to (trials).# We have a binary outcome (success/failure) for variants A and B# 3. Generating Synthetic Data# Let's assume variant A has 200 trials with 60 successes, and variant B has 180 trials with 90 successesdata_A = {'successes': 60, 'trials': 200}data_B = {'successes': 90, 'trials': 180} Choosing a PriorChoosing the right prior for our Bayesian model was challenging. We had to match our new campaign against historical campaigns that were similar to ours and come up with appropriate values for and . This required some convincing, but ultimately, we got the client’s approval. 12345678# 4. Prior Distribution# From prior campaigns we know of a conversion rate that we can expect for # any campaign. This is of course can assumption, but in the marketing world, # conversion rates rarely look different, no matter how good your recommendations# are. The industry rates will always look standard. # Use a uniform prior, Beta(10, 190)alpha_prior = 85 # Let's say this is 85 conversions, per 285 trialsbeta_prior = 200 Updating BeliefsEvery 15 days, we collect new data and update our prior beliefs. The beta distribution is a conjugate prior, meaning its posterior distribution is also a beta distribution. Updating is straightforward: This allows us to continuously refine our model as new data comes in. 1234567# 5. Likelihood and Posterior Distribution# Update the posterior distribution with the dataalpha_post_A = alpha_prior + data_A['successes']beta_post_A = beta_prior + data_A['trials'] - data_A['successes']alpha_post_B = alpha_prior + data_B['successes']beta_post_B = beta_prior + data_B['trials'] - data_B['successes'] Simulating ResultsTo determine if our recommendation engine beats the traditional method, we simulate a pseudo p-value by drawing samples from the posterior. We check how often our recommendation engine outperforms the traditional one: This tells us how frequently our engine’s CTR exceeds the traditional method’s CTR. 123456789101112# 6. Posterior Predictive Checks# Generate samples from the posterior distributions# since the beta distribution is a conjugate prior, the posterior distribution # is also a beta distribution. We can find its parameters by adding the prior parameters.samples_A = beta.rvs(alpha_post_A, beta_post_A, size=10000)samples_B = beta.rvs(alpha_post_B, beta_post_B, size=10000)# 7. Decision Making# Calculate the probability that variant B is better than variant Aprob_B_better_than_A = np.mean(samples_B &gt; samples_A)print(f\"Probability that variant B is better than variant A: {prob_B_better_than_A:.2f}\") ConclusionWhile complex measurement campaigns might signal trouble in some machine learning setups, our Bayesian approach showed promising results. Initially, our probability of outperforming the traditional method was 70%, which improved to 76%. This increase, combined with the ability to recommend high-margin products, demonstrated a tangible uplift due to our recommendation engine. 123456789101112131415161718# 8. Visualization# Plot the prior and posterior distributionsx = np.linspace(0, 1, 1000)prior = beta.pdf(x, alpha_prior, beta_prior)posterior_A = beta.pdf(x, alpha_post_A, beta_post_A)posterior_B = beta.pdf(x, alpha_post_B, beta_post_B)plt.figure(figsize=(12, 6))plt.plot(x, prior, label='Prior', linestyle='--')plt.plot(x, posterior_A, label='Posterior A')plt.plot(x, posterior_B, label='Posterior B')plt.fill_between(x, 0, posterior_A, alpha=0.3)plt.fill_between(x, 0, posterior_B, alpha=0.3)plt.legend()plt.xlabel('Conversion Rate')plt.ylabel('Density')plt.title('Prior and Posterior Distributions')plt.show() Bayesian Recommendation Engines: Beyond A/B TestingBut Bayesian methods don’t stop at A/B testing. They also play a significant role in recommendation engines. Most of my work in consulting was in signal processing and recommendation engines. In many cases our clients did not even need very complex recommender systems, they could benefit from relatively simple Bayesian updation based on the users behavior. Exploring the Power of Thompson Sampling for Multi-Armed Bandit Problems with Named Arms: A Deep Dive IntroductionImagine you’re running a bustling food delivery app, and you want to personalize the dining experience for your users. With an ever-growing list of restaurant options, how do you decide which cuisines to promote on the homepage? Should you highlight the new trendy Sushi place, the reliable Pizza joint, or the popular Chinese restaurant? Making the right choice can significantly impact user engagement and satisfaction. Enter the world of multi-armed bandit problems—a fascinating realm of decision-making under uncertainty. In this blog post, we will explore how Thompson Sampling, a powerful Bayesian algorithm, can help us optimize our choices dynamically. We’ll dive into a practical implementation using named arms, representing different types of cuisine, to illustrate how this method can be a game-changer for your food delivery app. The Multi-Armed Bandit ProblemThe multi-armed bandit problem is a classic scenario in reinforcement learning and decision theory. Picture a row of slot machines (the “bandits”), each with an unknown payout probability. Your goal is to maximize your total reward by deciding which machine to play at each step. In our context, the “slot machines” are different types of cuisine, and the “payout” is the user’s engagement or order from that cuisine. The challenge lies in balancing exploration (trying out different cuisines to learn their popularity) and exploitation (promoting the currently known most popular cuisine). Thompson Sampling provides an elegant solution to this exploration-exploitation dilemma by using Bayesian inference to update our beliefs about each cuisine’s popularity based on user interactions. Why Thompson Sampling?Thompson Sampling stands out for its simplicity and effectiveness. Unlike other methods that require complex calculations or large amounts of data, Thompson Sampling leverages probabilistic models to guide decision-making. By maintaining a distribution of potential outcomes for each option, it allows for a natural and intuitive way to balance exploration and exploitation. Here’s how it works: for each cuisine, we maintain a Beta distribution representing our belief about its popularity. Each time a user makes an order, we update these distributions based on the observed data. When deciding which cuisine to promote next, we sample from these distributions and choose the one with the highest sampled value. This approach ensures that we are more likely to promote cuisines with higher expected rewards while still occasionally exploring less popular options to refine our estimates. Practical ImplementationLet’s dive into a practical example. We’ll use three named arms—Chinese, Pizza, and Sushi—to demonstrate how Thompson Sampling can dynamically optimize our homepage recommendations. We’ll start with an initial belief about the popularity of each cuisine and update these beliefs as users interact with the app. InitializationWe begin by setting up initial Beta distributions for each cuisine based on our prior knowledge or assumptions. For instance, if we believe Pizza is initially more popular, we can set higher alpha and beta parameters for its distribution. 12345678910111213141516171819202122232425262728293031import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import beta# Set random seed for reproducibilitynp.random.seed(42)# Define the number of armsn_arms = 3# Define the armsarms = [\"Chinese\", \"Pizza\", \"Sushi\"]# Initialize alpha and beta for each armalpha = np.array([1, 3, 2]) # Prior successes (Pizza is initially more popular)beta_params = np.array([3, 2, 2]) # Prior failures# Function to plot the priorsdef plot_beta_distributions(alpha, beta_params, title=\"\"): x = np.linspace(0, 1, 100) for a, b, label in zip(alpha, beta_params, arms): y = beta(a, b).pdf(x) plt.plot(x, y, label=f\"{label} (alpha={a}, beta={b})\", linestyle='--') plt.title(title) plt.xlabel(\"Probability\") plt.ylabel(\"Density\") plt.legend()plt.figure(figsize=(10, 6))plot_beta_distributions(alpha, beta_params, title=\"Prior Distributions of Cuisines\")plt.show() User Interaction SimulationWe simulate user interactions over a series of trials, where each interaction represents a user choosing a cuisine. Based on the observed choices, we update our Beta distributions to reflect the new data. 12345678910111213141516171819202122232425# Simulate user interactionsn_trials = 1000successes = np.zeros(n_arms)failures = np.zeros(n_arms)# Function to select an arm using Thompson Samplingdef thompson_sampling(alpha, beta_params): sampled_theta = np.random.beta(alpha, beta_params) return np.argmax(sampled_theta)# Simulate interactionsfor _ in range(n_trials): chosen_arm = thompson_sampling(alpha, beta_params) reward = np.random.binomial(1, [0.2, 0.6, 0.4][chosen_arm]) # True probabilities if reward == 1: successes[chosen_arm] += 1 else: failures[chosen_arm] += 1 alpha[chosen_arm] += reward beta_params[chosen_arm] += 1 - reward# Plot posterior distributionsplt.figure(figsize=(10, 6))plot_beta_distributions(alpha, beta_params, title=\"Posterior Distributions of Cuisines after User Interactions\")plt.show() Posterior AnalysisAfter a number of interactions, we analyze the posterior distributions to determine which cuisine is likely to be popular. We can then use this information to sort and display the cuisines on our app’s homepage. 12345678910111213141516171819# Sort arms based on the highest posterior meanposterior_means = alpha / (alpha + beta_params)sorted_indices = np.argsort(posterior_means)[::-1]print(\"Sorted Cuisines Based on Posterior Means:\")for idx in sorted_indices: print(f\"{arms[idx]}: {posterior_means[idx]:.2f}\")# Visualize sorted cuisinessorted_arms = [arms[i] for i in sorted_indices]sorted_means = [posterior_means[i] for i in sorted_indices]plt.figure(figsize=(10, 6))plt.bar(sorted_arms, sorted_means, color=['blue', 'orange', 'green'])plt.xlabel(\"Cuisine\")plt.ylabel(\"Posterior Mean Probability\")plt.title(\"Sorted Cuisines Based on Posterior Means\")plt.show() Visualizing the ResultsTo make our analysis more intuitive, we plot the prior and the posterior distributions for each cuisine. These visualizations help us understand how our beliefs evolve over time and provide a clear picture of the most likely user preferences. 1234567891011121314151617181920# Function to plot both prior and posterior distributionsdef plot_prior_and_posterior(alpha_prior, beta_prior, alpha_post, beta_post): x = np.linspace(0, 1, 100) for a_prior, b_prior, a_post, b_post, label in zip(alpha_prior, beta_prior, alpha_post, beta_post, arms): y_prior = beta(a_prior, b_prior).pdf(x) y_post = beta(a_post, b_post).pdf(x) plt.plot(x, y_prior, label=f\"{label} Prior (alpha={a_prior}, beta={b_prior})\", linestyle='--') plt.plot(x, y_post, label=f\"{label} Posterior (alpha={a_post}, beta={b_post})\") plt.xlabel(\"Probability\") plt.ylabel(\"Density\") plt.legend()alpha_prior = np.array([1, 3, 2])beta_prior = np.array([3, 2, 2])plt.figure(figsize=(10, 6))plot_prior_and_posterior(alpha_prior, beta_prior, alpha, beta_params)plt.title(\"Prior and Posterior Distributions of Cuisines\")plt.show() Through this implementation, we can see how Thompson Sampling dynamically adapts to user behavior, continually refining our recommendations to maximize user satisfaction. Final Thoughts on Thompson SamplingThompson Sampling offers a powerful, flexible approach to solving the multi-armed bandit problem, particularly in dynamic and uncertain environments like a food delivery app. By leveraging Bayesian inference, it enables us to make data-driven decisions that balance exploration and exploitation effectively. Whether you are managing a food app, an e-commerce platform, or any other recommendation system, Thompson Sampling can help you optimize user engagement and drive better outcomes. So the next time you are faced with the challenge of deciding which options to promote, think of Thompson sampling! Bayesian Matrix Factorization with Side InformationBayesian Matrix Factorization (BMF) with side information is a powerful extension of traditional matrix factorization. It incorporates additional context about users and items to improve recommendations. Here’s a breakdown: Likelihood FunctionThe likelihood function models the probability of observing user-item interactions given latent factors: where and are latent factor vectors for users and items, respectively. Prior DistributionsWe assume Gaussian prior distributions for the latent factors: Incorporating Side InformationSide information about users and items is integrated by conditioning the prior distributions: Posterior DistributionThe posterior distribution of the latent factors given the observed data and side information is derived using Bayes’ theorem: Approximate inference methods like Variational Inference or MCMC are used to estimate these factors. Main Equation SummaryThe core Bayesian matrix factorization model with side information can be summarized as: This model integrates additional context into the matrix factorization process, enhancing the accuracy and robustness of the recommendation system. ConclusionBayesian methods offer a versatile toolkit for data scientists, enhancing both A/B testing and recommendation engines. As computational constraints lessen, Bayesian approaches are gaining traction. I’m always eager to explore new use cases and dive deeper into the specifics of applying these methods in different scenarios. If you have a challenging problem or an interesting use case, I’d love to hear about it! References[Great Stanford tutorial on Thompson sampling] (https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)[YouTube Video that formed the basis of my example] (https://www.youtube.com/watch?v=nRLI_KbvZTQ&amp;t=321s)[Overview of bandits] (https://eugeneyan.com/writing/bandits/)[Contextual Bandits] (https://gdmarmerola.github.io/ts-for-contextual-bandits/)[Bayesian Matrix Factorization] (https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf)","link":"/2024/07/19/bayesian-statistics/"},{"title":"UChicago Economics Grad School Alumni Interview","text":"Main Interview Please tell us your name, your MA program and year of graduation, as well as your current role and title.Francisco Mendes, MACSS Economics, 2022. I am currently a Senior AI Engineer at Renesas Electronics America Inc. Can you tell us about your journey after MAPSS-how you secured your current role?I think I had a pretty long journey to get here, but the short version is that I was recruited by on campus Consulting right out of our program. I spent about a year in consulting but having already had 5 years of Consulting experience I decided it was time for a change. Having worked primarily in recommender systems and signal processing in my entire consulting career, I decided to look for a career in that realm and here I am. Did you always know that you wanted to seek placement in the academic or professional realm? Did this journey change at all for you?For me, I was really committed to go into the PhD program directly after MACSS. I think the opportunity was there to do so provided grades and research interests lined up. After attempting to transfer internally I was offered a pre-doc position to strengthen my resume for application to the Econ PhD program. To me a pre-doc program was not a feasible route to a PhD, I have several personal misgivings about the pre-doc system in Economics (it encourages primarily students from wealthy backgrounds to enter academia, it is not a competitive wage market equilibrium etc) How do you feel that the MAPSS-Econ/MACSS-Econ programs prepared you for the role that you are in now?For me, my prior education in pure mathematics and statistics was probably most relevant to my current role. Econ programs in UChicago are a great way to learn some of those mathematical skills that are transferable to other domains. If you are certain you want to transition out of academia, I would highly recommend taking classes that reflect the domain you are interested in. Is there anything that surprised you during your post-graduate pursuits.I think the one thing that surprised me (and continues to surprise me) is what a closed circle economics is as an academic field and how far it is from the economic and social realities of America. While you are in UChicago it may seem like Economics is all pervasive field that impacts the lives of many Americans, but reality is quite different, most of what we study here is very distant from the lives of everyday Americans. Even fairly well informed people will struggle to name leading figures in academic economics and most academic economics is irrelevant to policy making. Can you tell us any important advice you have for graduating MAPSS-Econ an MACSS-Econ students?The first thing I would say is enjoy your academic life, single-mindedly pursuing a field of study and answering a research question that motivates you. If you have decided that you want to try moving into industry then prepare accordingly. Either way take charge of your career! Other Random ThoughtsExpanding on some of the answers above ended up being worthy of a subsection, please find these below. Pre-doc ProgramsEconomics is probably the most competitive grad school program in America right now. The number of applicants to the number of spots is quite high. The pre-doc system is a way to get a foot in the door, but it is not a guarantee of a spot in the PhD program. I think the pre-doc system is a cheap way to get labor for the university while parading itself as a way to get a better signal of ability. A pre-doc is no better signal of ability than grades and the Master’s research thesis. I believe if you are from a background that allows you to stay out of the job market for 2-3 years in your early twenties, then the pre-doc route does not cost you much. If you are older or have financial constraints, I would recommend against the pre-doc route. Pre-docs add, at best, a marginal signal to your application (outside of UChicago) because no one knows what you did. If you are not given a PhD at the place you did your PhD, other universities will look at you like a lemon (if you are not good enough for your own university, why would we want you?). In fact UChicago does this with pre-docs from Stanford and Harvard, they are not given PhDs at UChicago. It is very rare for a pre-doc to substantially increase their chances of a PhD at any place other than the place they did the pre-doc. Transitioning to IndustryI think the most important thing to do is to take classes that reflect the domain you are interested in. If you are interested in AI, take AI classes. If you are interested in finance, take finance classes. Most of the economics program prepares you for one thing, a career in economics. Do not mistake the difficulty of the program for its usefulness and transferability. While the mathematical and statistical skills you learn, definitely make you better than a plug and play data scientist, they are not enough to get you a job in industry. You need to send a strong signal to the market that you are interested in a particular domain and have immediately applicable skills. The code you write in your classes is not enough to signal this. You need to have a portfolio of projects that you can show to potential employers. These days the coding requirements for most AI roles are quite high, you need to be able to code in Python and C++ at a minimum. Economics as a FieldNone of what follows is meant to be a coherent argument for or against the field of economics or the flavor of economics at UChicago. I think there are many benefits to both. What follows is simply a summary of my observations about how different conversations with fellow econs are versus everyday people (Americans for the most part). I had the habit of names dropping famous economists to make a point in everyday conversation, such as “as Milton Friedman said…” or “as Gary Becker said…” or “as John List said…”. I quickly realized that no one knows who these people are. I think the field of economics is quite closed off from the rest of the world. The field is not as influential as it thinks it is. That is for both better and for the worse, I think the field should be more relevant and more engaged with people outside the field itself. But in many ways it is for the better, I think that while you are in economics (especially at UChicago) you do not realize how ideological some of the classes are. For instance, price theory is really an ideological class that is not grounded in reality. Some of it might be true, but large portions are ideologies and assumptions about how markets work. When engaging with fairly well-informed individuals about economics, I realized that the goal of research at a deeply tactical level ends up being finding a niche and churning out papers in that niche. This in itself is not a bad thing, but if you zoom out (even a little bit) from that niche you realize that the research is not relevant in everyday conversation. I can think of two great examples of this. The first is any experimental behavioral economics research study, while many of the conclusions are interesting and unique, they remain just that interesting and unique. They are not generalizable to the population at large. And of course economists are the first to admit this, but this is a much bigger impediment to everyday conversation than they realize. I guess the main misgiving people have is that to the field of economics, every problem seems to be a nail (prices, wages, incentives) and the solution is always a hammer (money). Thus, every societal problem can be framed as one that can be solved using the right incentive structure. But this is a bigger ideological hurdle for most people than the field admits. The second would be the idea that markets work, this is purely a UChicago “fantasy”. Markets work in many cases and in many cases they do not. In either case, to the average American, the idea that markets work is not a given. While we spend a lot of time working out the math after establishing a set of assumptions, the assumptions we make are not as easy to establish to a jury of your peers, as it were. As an econ you are drawn to complex mathematical structures and their solutions (a matching is a lattice and a stable matching is a fixed point on the lattice, who wouldn’t be excited?!), the actual solution matters less than the assumptions and narrative around the problem. Post MortemI enjoyed my time at UChicago and definitely enjoy the field of economics. I think the program offered me a lot of opportunities to grow and learn. I think the program is a great way to learn some of the mathematical skills that are transferable to other domains. But there are certainly some things you need to do (on your own) to get the most out of it professionally. Especially if your previous degrees are not relevant to industry.","link":"/2024/05/09/MACSS-econ/"},{"title":"Book Review (and reflections): The Cold War, A World History by Odd Arne Westad","text":"IntroductionThere are few book s that balance breadth with detail like The Cold War: A World History does. Spanning from 1945-1989, this book covers in , well researched, detail the events of the Cold War along with important historical context. Given that the Cold War is often framed variously as a duel of sorts between The West and The East, Communism and Capitalism, Democracy and Authoritarianism, this book is singularly unique in that it is almost perfectly devoid of opinion. In this sense it is the amateur historian’s dream. However, I will endeavour to color the subsequent sections with some of my reflections while providing references to things I have read in the book. This article should be read as opinion piece based on an extremely factual (bordering on bland) book. Having said that, I believe that my Reflection 4, is a fairly accurate representation of Westad’s epilogue of the book. Reflection One: The Cold War was not an ideological battleThe Cold War is often summarized by the ideological tensions between the West and the USSR . These ideological tensions are then used to impute the motivations of each of the actors in the conflict. However, after reading this book, I realize that the Cold War was a much broader conflict involving more people and broader struggles than just Europe. Viewing the broader context of the Cold War it is difficult to ascribe purely ideological notions to the interventions by the West in various countries. Perhaps the greatest example of this would be the blatant support for (by the US) of two groups that are ideologically not aligned with either free market ideals or democratic ideals. The Christian/ Democratic Socialists - understandably, this political leaning might be a contradiction in terms to most familiar with US politics due to the close ties between the Right and Christian groups. However, in Europe this is not the case, several parties exist, most notably in Germany. Angela Merkel’s party the Christian Democratic Union, which was formed by multiple splinter groups varying from Marxist/ Leninist groups to Social Democrat parties. These parties espoused fairly left leaning views and often had nothing in common with the West, other than a mutual dislike of Communism. Which, in the case of these parties, was more religiously motivated than anything else. Dictatorships - Western support for dictatorships over democratically elected Leftist or Communist governments in various newly forming countries (more on this later). Saddan Hussein, Syngman Rhee, Ngo Dinh Diem to name just two among countless others. Some of these went on to establish the most brutal dictatorships that continued much after the Cold War. This was not obviously different to the promotion of dictatorships propped up by the USSR (Tito, Castro and Assad). Not to mention the support of royal families and theocracies in the Middle East over the promotion of true democracy. Thus, expedience, convenience and prudence were the primary motivating factors for support of governments, not pure ideological motivations. An apologist would argue that such regimes were “better” than a Communist regime, given the 40 odd years of history for some of those regimes, I am not so sure that the USSR of Gorbachev was so much worse than the brutal dictatorship of Saddam Hussein, who ironically was deposed and executed during the Second Iraq War by the US. Reflection Two: The Battlefields of the Cold WarThere was nothing cold about the Cold War other than the often uneasy, but nevertheless, staged peace of Europe. On other battlefields both the US and USSR played out an often bloody game of proxy wars that resulted in millions of casualties. The world in 1945 was a strange and varied place and looks nothing like the world of today, to make sense of this, the book implicitly categorizes the world into five regions that differ in historical context, (subsequent interpretations are mine, not in the book), Europe - Most commonly analyzed centre stage of the Cold War. Not much more can be learned about Europe by even a cursory review of the events of the Cold War (but some useful interpretations of the events following the cold war necessitate reflection 3, below) Islamic majority countries of the Middle East and North Africa, these countries partly due to their proximity to Europe (and being battlefields for WWII) and partly due to their citizens fleeing Europe (Israel) were perhaps the first scene of tension between the USSR and the US, outside of Europe. Non-Aligned Afro-Asian countries, many of these countries had been fighting for independence prior to the Second World War and as of 1945 were on the verge of independence. Thus in the years following 1945 many of these countries would be newly independent and looking for ways to shake off their colonial pasts and forge ahead into a more industrialized future. India, Ghana, Indonesia, Vietnam were such countries. Latin America China and its neighbouring states, Korea, Vietnam, Laos, Cambodia (scenes of some of the more direct interventions by the US). For 2 and 3, these countries were in the nascent stages of development and were seeking and forming governments for the first time. In the case of 2, these Arab countries were simultaneously undergoing a class struggle that made Communism especially attractive to members of those countries. Among the early claimants for power in these countries were often the revolutionaries themselves. These revolutionaries had a wide spectrum of ideological views, ranging from weakly pro-Western and pro free market to openly sympathetic to Communism. Early on the USSR often seemed unwilling to support all but the most ardent communists, the most likely cause of this is probably the lack of any economic incentive to do so and the lack of economic heft to pay the upfront cost of an intervention when an economic incentive did present itself. This did not prevent the US from often getting involved to preempt the formation of a Communist government by revolutionaries, by any means necessary. This tug of war led to an important distinction between 2, the Middle Eastern countries and the other post colonial countries, 3. In the case of the former Islamic theocracy arose. Often the radical Islamists labelled both the US and USSR as “devils” and strove to form Islamic countries heavily influenced by religious law. The outcomes for this were varied, either the West openly supported the prevailing dictatorship as in the case of Iran, or openly supported the dictatorship that, in turn, openly supported theocracy (as in the case of Saudi Arabia). If theocracy was the “third option” that leaned to the West. Then Non-alignment was the “third option” that leaned to the East. India, Indonesia and Egypt formed the basis of the non-aligned movement. This movement defied the narrow definitions of the Cold War and formed the early basis of the emerging BRICS phenomenon that we see today. I will not go into further detail on this here, but the Non-Aligned Movement is, to me atleast, the best way to understand the Cold War and what it meant (and did not mean). It is perhaps also the one that is most relevant today. In Latin America, the US played a vigorous role via the support for various governments, either via clandestine support or overt financial support. Often this had disastrous consequences such as Pinochet in Chile. In almost all cases at least some economic incentive can be attached. The Soviet Union (other than in Cuba) played the role of an active bystander. It is unclear if this was due to the absence of “true” Communism or because by the time Latin America became relevant certain structural weaknesses became apparent in the flawed economy of the Soviet Union and its satellites. Reflection Three: The Uniqueness of China“the communist revolution was a kind of cleansing: it might have used methods that were incomprehensible or even inhuman, but the revolution gave them the opportunity to immerse themselves in something bigger than the individual, something meaningful, something that would eventually set China right” China and her neighbors deserve an article all to themselves. I do not wish to express any particularly controversial opinion here when I say that, the Cold War in the countries of Vietnam, Cambodia and Laos and the subsequent atrocities and casualties that occurred there were more a face off between China and the West than the USSR. While it is a matter of historical record that the Soviets did get involved in the Korean War, it is very clear that they were far more reluctant to expand it into the mess it eventually became than were the Chinese. While more cursory readings of history often place the Chinese firmly within the camp of Communism. It is my belief that Maoism and the unique cultural circumstances that led to Communism in China merit their own space on the ideological spectrum. It is truly difficult to fairly represent Maoism via a Western lens, the inherent weight-age given to individual freedoms in the West are meaningless when viewed in the context of China’s cultural revolution. Nothing exemplifies this more than the fact that China can be simultaneously argued to be both the most powerful Eastern bloc country left (if it can be indeed included in that bloc) while also being the least centralized (and with the largest free market) of any previously existing Communist country. In a beautiful twist of fate, my next book is On China by Henry Kissinger. Hopefully, I will have more insights to share on this then! Reflection Four: Who “Won” the Cold War?If Communism was such a good system, why are there no Communist countries left? Any vigorous discussion of Communism on internet forums inevitably leads to this question being asked. And I think this book goes quite a long way in answering it. Perhaps, first we must ask, who won the Cold War? The resounding answer must certainly be the European middle classes. While the often quoted answer is that the Free market won the Cold War, this is not obvious when we analyze the end of the Cold War. After the Second World War most of Europe lay in ruins, and the impressive victory of communism over the Germans inspired multiple people to genuinely believe that communism offered a road to prosperity that capitalism had not provided so far. This book reminds us that the while fall of Communism seemed imminent in the 80s, it was far less so in the early 50s and 60s. Life expediencies for Eastern Bloc were increasing, for perhaps the first time in their histories. The two wars in Europe and the economic depression that free market capitalism brought with it seemed to unfairly target the lower classes who seemed to bear a disproportionate share of the economic collapse and casualties of war. Thus Western Communism was healthy, alive and well, well into the 50s and 60s (definitely in France, Portugal, Spain and Italy). When looking back after the Cold War however, the reason Communism did not take hold in Western Europe was because there was nothing to take over. Large social welfare programs, free healthcare and trade unions effectively gave many of the same benefits that communism promised. These were effectively under written by the US. These programs were often implemented by centre-left (Labour, in the UK) or outright left parties (French Communist Party). It unlikely that such programs could have even gotten off the ground without programs like the Marshall plan and the absence of technology transfer from the US. This makes me believe that the victory of capitalism of in Western Europe was effectively a truce, paid for by a huge cash injection to countries that have highly educated populations that require little to no assistance in setting up systems and processes of governance and do not have to deal with issues such as extreme poverty and decolonization. Which is perhaps why Third-World countries did not look to free-market solutions to provide reasonable paths to development. Thus the West much much more eager to claim victory in Europe, than on the other battlefields of the world. The answer to why there are not many “purely” Communist countries in the world today is perhaps because the Communist revolution inspired changes in modern Western Europe that obviated the need for a more grandiose revolution. In a way, this “silent” revolution led to its demise and failure to capture the minds of Western Europeans, even though sympathies for it existed for much of the 20th century, and even till today. ConclusionThis is a wonderful book, and I highly recommend it. I am sure opinions will vary when reading this book. However, the added context of countries outside the European mainland makes it hard to view the Cold War as either “cold”, it was very much as bloody or violent as any other war, just no European lives were lost. Or that it was a “war” that was won or lost by a particular ideology. Like most things in history the answer is complicated. I particularly appreciated the importance ascribed to non-European issues, events and viewpoints. Eurocentric views of the Cold War are not only incomplete and lacking in historical veracity, they have led to two important foreign policy failures : The rise of radical Islam, directly sponsored by the short-sighted funding of radical Islamic groups that initially fought Communism, but in turn established radical Islamic regimes much more brutal and oppressive than any Communist government. The lack of cognisance of these movements has led to a larger conflict which makes the Cold War seem almost manageable by contrast. The lack of understanding of the BRICS movement in context of the Russia-Ukraine conflict and its future significance. The Non-Aligned movement referred to in Reflection 2, has taken on new relevance in light of the Ukraine conflict. For a short period after the Cold War, economics sanctions by the West almost certainly would mean economic ruin. However, the continued rise of BRICS has led to the formation of a very clear “Third Front”. This group of nations does not view the world in as black and white terms as before. And is unwillingly to take sides in battles it no longer views as relevant. It does so wile simultaneously maintaining economic and strategic relationships with both sides (Russia and NATO, in this case). In this way, most economic sanctions are toothless because of the existence of these large trading partners. India and China have both remained large economic and strategic partners for both Russia and the US since the start of the conflict. While India has enjoyed new strategic partnerships with the US due to its proximity to and adverse relationship with, China. While simultaneously buying weapons from Russia. The us-and-them mentality that was so pervasive in the Cold War was due in large part to the way Western and Eastern Bloc governments spoke about each other. What should have been an ideological war often became a real one, in countries that could least afford it. These countries should have been free to choose their systems of government without outside interference (from either side).Let me conclude with this final quote that summarizes much of the Western approach to the Cold War, “The moral certainties, the eschewal of dialogue, the faith in purely military solutions …the doctrinaire belief in free-market messages or the top-down approach to social ills.”","link":"/2024/01/06/cold-war/"},{"title":"Book Review: East of Eden by John Steinbeck","text":"An odyssey borne out of the oldest tale in the oldest book in the world, Cain and Abel. Very rarely are well worn fables resurrected like new, but this book succeeded in telling an age old tale of fraternal rivalry across several generations, with a far more generous view of Cain. The characters in the book represent major moral themes, from pure Biblical evil, as represented by Cathy. Pure angelic goodness in Adam and Aron. And finally, human moral frailty in Charles and Cal. The characters absorb you in their machinations, their trials and their triumphs till you are finally hanging on to every page. Yet, this book is no railway station page turner, it draws you in with the sheer weight of its story telling. The sheer beauty of its mundane moments. And the intellectual heft of characters like Sam Hamilton and Lee. We are treated to deep moral debates about each of the characters actions and Lee, in particular, draws on several pagan sources to supplement this very Christian tale. One cannot help but feel this book rewrites Genesis through Cain’s eyes. And one feels for the rejected offering, one also feels the anger and jealousy that inevitably come with being the less anointed child. The titanic internal struggle for goodness against these carnal feelings. But only in this darkness can human nature be born. Our visceral dislike of Abel’s unnatural goodness shows that we (I) are Cain’s progeny after all. The language in this book is simple, and spills off the pages. At times chapters seem written in frenzied haste and at others each word is weighed as if by St. Peter himself. This book must have been a Herculean task, but the author proved more than equal to it. More Steinbeck to come!","link":"/2025/02/09/east-of-eden/"},{"title":"The Management Consulting Playbook for AB Testing (with an emphasis on Recommender Systems)","text":"IntroductionAlthough I’ve focused much more on the ML side of consulting projects—and I really enjoy it—I’ve often had to dust off my statistician hat to measure how well the algorithms I build actually perform. Most of my experience in this area has been in verifying that recommendation engines, once deployed, truly deliver value. In this article, I’ll explore some key themes in AB Testing. While, I tried to be as general as possible, I did drill down on specific concepts that are particularly salient to recommender systems. I thoroughly enjoy the “measurement science” behind these challenges; it’s a great reminder that classic statistics is far from obsolete. In practice, it also lets us make informed claims based on simulations, even if formal proofs aren’t immediately available. I’ve also included some helpful simulations. Basic Structure of AB TestingAB Testing begins on day zero, often in a room full of stakeholders, where your task is to prove that your recommendation engine, feature (like a new button), or pricing algorithm really works. Here, the focus shifts from the predictive power of machine learning to the causal inference side of statistics. (Toward the end of this article, I’ll also touch briefly on causal inference within the context of ML.) Phase 1: Experimental Context Define the feature under analysis and evaluate whether AB testing is necessary. Sometimes, if a competitor is already implementing the feature, testing may not be essential; you may simply need to keep pace. Establish a primary metric of interest. In consulting projects, this metric often aligns closely with engagement fees, so it’s critical to define it well. Identify guardrail metrics—these are typically independent of the experiment (e.g., revenue, profit, total rides, wait time) and represent key business metrics that should not be negatively impacted by the test. Set a null hypothesis, (usually representing a zero effect size on the main metric). Consider what would happen without the experiment, which may involve using non-ML recommendations or an existing ML recommendation in recommendation engine contexts. Specify a significance level, , which is the maximum probability of rejecting the null hypothesis when it is true, commonly set at 0.05. This value is conventional but somewhat arbitrary, and it’s challenging to justify because humans often struggle to assign accurate probabilities to risk. Define the alternative hypothesis, , indicating the minimum effect size you hope to observe. For example, in a PrimeTime pricing experiment, you’d specify the smallest expected change in your chosen metric, such as whether rides will increase by hundreds or by 1%. This effect size is generally informed by prior knowledge and reflects the threshold at which the feature becomes worthwhile. Choose a power level, , usually set to 0.8. This means there is at least an 80% chance of rejecting the null hypothesis when is true. Select a test statistic with a known distribution under both hypotheses. The sample average of the metric of interest is often a good choice. Determine the minimum sample size required to achieve the desired power level with all given parameters. Before proceeding, it’s crucial to recognize that many choices, like those for and , are inherently subjective. Often, these parameters are predefined by an existing statistics or measurement science team, and a “Risk” team may also weigh in to ensure the company’s risk profile remains stable. For instance, if you’re testing a recommendation engine, implementing a new pricing algorithm, and cutting costs simultaneously, the risk team might have input on how much overall risk the company can afford. This subjectivity often makes Bayesian approaches appealing, driving interest in a Bayesian perspective for AB Testing. Phase 2: Experiment DesignWith the treatment, hypothesis, and metrics established, the next step is to define the unit of randomization for the experiment and determine when each unit will participate. The chosen unit of randomization should allow accurate measurement of the specified metrics, minimize interference and network effects, and account for user experience considerations.The next couple of sections will dive deeper into certain considerations when designing an experiment, and how to statistically overcome them. In a recommendation engine context, this can be quite complex, since both treatment and control groups share the pool of products, it is possible that increased purchases from the online recommendation can cause the stock to run out for people who physically visit the store. So if we see the control group (i.e. the group not exposed to the new recommender system) buying more competitor products (competitors to the products you are recommending) this could simply be because the product was not available and the treatment was much more effective than it seemed! Unit of Randomization and InterferenceNow that you have approval to run your experiment, you need to define the unit of randomization. This can be tricky because often there are multiple levels at which randomization can be carried out for example, you can randomize your app experience by session, you could also randomize it by user. This leads to our first big problem in AB testing. What is the best unit of randomization? And what are the pitfalls of picking the wrong unit? Sometimes, the unit is picked for you, you simply may not have recommendation engine data at the exact level you want. A unit is often hard to conceptualize, it is easy to think that it is one user. But one user at different points in their journey through the app can be treated as different units. Example of InterferenceInterference is a huge problem in recommendation engines for most retail problems. Let me walk you through an interesting example we saw for a large US retailer. We were testing whether a certain product (high margin obviously!) was being recommended to users. The treatment group was shown the product and the control group was not. The metric of interest was the number of purchases of a basket of high margin products. The control group purchased the product at a rate of and the treatment group purchased the product at a rate of . The experiment was significant at the level. However, after the experiment we noticed that the difference in sales closed up to . This was because the treatment group was buying up the stock of the product and the control group was not because they could not. Sometimes the act of being recommended a product was a kind of treatment in itself. This is a non-classical example of interference. This is a good reason to use a formal causal inference framework to measure the effect of the treatment. One way to do this is DAGs, which I will discuss later. The best way to run an experiment like this is to randomize by region. However, this is not always possible since regions share the same stock. But I think you get the idea. Robust Standard Errors in AB TestsYou can fix interference by clustering at the region level but very often this leads to another problem of its own. The unit of treatment allocation is now fundamentally bigger than the unit at which you are conducting the analysis. We do not really recommend products at the store level, we recommend products at the user level. So while we assign treatment and control at the store level we are analyzing effects at the user level. As a consequence we need to adjust our standard errors to account for this. This is where robust standard errors come in. In such a case, the standard errors you calculate for the average treatment effect arelower than what they truly are. And this has far-reaching effects for power, effect size and the like. Recall, the variance of the OLS estimator You can analyze the variance matrix under various assumptions to estimate, Under homoscedasticity, Under heteroscedasticity (Heteroscedastic robust standard errors), And finally under clustering, The cookbook, for estimating is therefore multiplying your matrix with some kind of banded matrix that represents your assumption , Range of Clustered Standard Errors Where the left boundary is where no clustering occurs and all errors are independent and the right boundary is where the clustering is very strong but variance between clusters is zero. It is fair to ask, why we need to multiply by a matrix of assumptions at all, the answer is that the assumptions scale the error to tolerable levels, such that the error is not too large or too small. By pure coincidence, it is possible to have high covariance between any two observations, whether to include it or not is predicated by your assumption matrix . Power AnalysisI have found that power analysis is an overlooked part of AB Testing, in Consulting you will probably have to work with the existing experimentation team to make sure the experiment is powered correctly. There is usually some amount of haggling and your tests are likely to be underpowered. There is a good argument to be made about overpowering your tests (such a term does not exist in statistics, who would complain about that), but this usually comes with some risk to guardrail metrics, thus you are likely to under power your tests when considering a guardrail metric. This is OKAY, because remember the level is a convention, and the power level is also a convention that by definition err on the side of NOT rejecting the null. So if you see an effect with an underpowered test you do have some latitude to make a claim while reducing the significance level of your test. Power analysis focuses on reducing the probability of accepting the null hypothesis when the alternative is true. To increase the power of an A/B test and reduce false negatives, three key strategies can be applied: Effect Size: Larger effect sizes are easier to detect. This can be achieved by testing bold, high-impact changes or trying new product areas with greater potential for improvement. Larger deviations from the baseline make it easier for the experiment to reveal significant effects. Sample Size: Increasing sample size boosts the test’s accuracy and ability to detect smaller effects. With more data, the observed metric tends to be closer to its true value, enhancing the likelihood of detecting genuine effects. Adding more participants or reducing the number of test groups can improve power, though there’s a balance to strike between test size and the number of concurrent tests. Reducing Metric Variability: Less variability in the test metric across the sample makes it easier to spot genuine effects. Targeting a more homogeneous sample or employing models that account for population variability helps reduce noise, making subtle signals easier to detect. Finally, experiments are often powered at 80% for a postulated effect size — enough to detect meaningful changes that justify the new feature’s costs or improvements. Meaningful effect sizes depend on context, domain knowledge, and historical data on expected impacts, and this understanding helps allocate testing resources efficiently. In an A/B test, the power of a test (the probability of correctly detecting a true effect) is influenced by the effect size, sample size, significance level, and pooled variance. The formula for power,, can be approximated as follows for a two-sample test: Where, is the Minimum Detectable Effect (MDE), representing the smallest effect size we aim to detect. is the critical z-score for a significance level (e.g., 1.96 for a 95% confidence level). is the **pooled standard deviation** of the metric across groups, representing the combined variability. is the sample size per group. is the **cumulative distribution function** (CDF) of the standard normal distribution, which gives the probability that a value is below a given z-score. Understanding the Role of Pooled Variance Power decreases as the pooled variance () increases. Higher variance increases the \"noise\" in the data, making it more challenging to detect the effect (MDE) relative to the variation. When pooled variance is low, the test statistic (difference between groups) is less likely to be drowned out by noise, so the test is more likely to detect even smaller differences. This results in higher power for a given sample size and effect size. Practical ImplicationsIn experimental design: Reducing (e.g., by choosing a more homogeneous sample) improves power without increasing sample size. If is high due to natural variability, increasing the sample size compensates by lowering the standard error , thereby maintaining power. Difference in DifferenceRandomizing by region to solve interference can create a new issue: regional trends may bias results. If, for example, a fast-growing region is assigned to the treatment, any observed gains may simply reflect that region’s natural growth rather than the treatment’s effect. In recommender system tests aiming to boost sales, retention, or engagement, this issue can be problematic. Assigning a growing region to control and a mature one to treatment will almost certainly make the treatment group appear more effective, potentially masking the true impact of the recommendations. Linear Regression Example of DiDTo understand the impact of a new treatment on a group, let’s consider an example where everyone in group receives a treatment at time . Our goal is to measure how this treatment affects outcomes over time. First, we’ll introduce some notation: Define 𝟙, which indicates if belongs to a specific set : Let , which represents the period after treatment. We can use this to set up a few key indicators: 𝟙 if the time is after the treatment, and otherwise. 𝟙 if an individual is in group , meaning they received the treatment. if they are both then they refer to those in the treatment group during the post-treatment period. Using these indicators, we can build a simple linear regression model: 𝟙𝟙𝟙𝟙 In this model, the coefficient is the term we’re most interested in. It represents the Difference-in-Differences (DiD) effect: how much the treatment group’s outcome changes after treatment compared to the control group’s change in the same period. In other words, provides a clearer picture of the treatment’s direct impact, isolating it from other factors. For this model to work reliably, we rely on the parallel trends assumption: the control and treatment groups would have followed similar paths over time if there had been no treatment. Although the initial levels of can differ between groups, they should trend together in the absence of intervention. Testing the Parallel Trends AssumptionYou can always test whether your data satisfies the parallel trends assumption by looking at it. In a practical environment, I have never really tested this assumption, for two big reasons (it is also why I personally think DiD is not a great method): If you need to test an assumption in your data, you are likely to have a problem with your data. If it is not obvious from some non-statistical argument or plot etc you are unlikely to be able to convince a stakeholder that it is a good assumption. The data required to test this assumption, usually invalidates its need. If you have data to test this assumption, you likely have enough data to run a more sophisticated model than DiD (like CUPED). Having said all that, here are some ways you can test the parallel trends assumption: Visual Inspection: Plot the average outcome variable over time for both the treatment and control groups, focusing on the pre-treatment period. If the trends appear roughly parallel before the intervention, this provides visual evidence supporting the parallel trends assumption. Make sure any divergence between the groups only occurs after the treatment. Placebo Test: Pretend the treatment occurred at a time prior to the actual intervention and re-run the DiD analysis. If you find a significant “effect” before the true treatment, this suggests that the parallel trends assumption may not hold. Use a range of pre-treatment cutoff points and check if similar differences are estimated. Consistent non-zero results may indicate underlying trend differences unrelated to the actual treatment. Event Study Analysis (Dynamic DiD): Extend the DiD model by including lead and lag indicators for the treatment. If pre-treatment coefficients (leads) are close to zero and non-significant, it supports the parallel trends assumption. Large or statistically significant leads could indicate violations of the assumption. Formal Statistical Tests: Run a regression on only the pre-treatment period, introducing an interaction term between time and group to test for significant differences in trends: If the coefficient on the interaction term is close to zero and statistically insignificant, this supports the parallel trends assumption. A significant would indicate a pre-treatment trend difference, which would challenge the assumption. Covariate Adjustment (Conditional Parallel Trends): If parallel trends don’t hold unconditionally, you might adjust for observable characteristics that vary between groups and influence the outcome. This is a more relaxed “conditional parallel trends” assumption, and you could check if trends are parallel after including covariates in the model. If you can make all this work for you, great, I never have. In the dynamic world of recommendation engines (especially always ‘’online’’ recommendation engines) it is very difficult to find a reasonably good cut-off point for the placebo test. And the event study analysis is usually not very useful since the treatment is usually ongoing. Peeking and Early StoppingYour test is running, and you’re getting results—some look good, some look bad. Let’s say you decide to stop early and reject the null hypothesis because the data looked good. What could happen? Well, you shouldn’t. In short, you’re changing the power of the test. A quick simulation can show the difference: with early stopping or peeking, your rejection rate of the null hypothesis is much higher than the 0.05 you intended. This isn’t surprising since increasing the sample size raises the chance of rejecting the null when it’s true. The benefits of early stopping aren’t just about self-control. It can also help prevent a bad experiment from affecting critical guardrail metrics, letting you limit the impact while still gathering needed information. Another example is when testing expendable items. Think about a magazine of bullets: if you test by firing each bullet, you’re guaranteed they all work—but now you have no bullets left. So you might rephrase the experiment as, How many bullets do I need to fire to know this magazine works? In consulting you are going to peek early, you have to live with it. For one reason or another, a bug in production, an eager client whatever the case, you are going to peek, so you better prepare accordingly. Simulated Effect of Peeking on Experiment Outcomes (a) Without Peeking: reject null, (b) With Peeking: reject null, Under a given null hypothesis, we run 100 simulations of experiments and record the z-statistic for each. We do this once without peeking and let the experiments run for observations. In the peeking case, we stop whenever the z-statistic crosses the boundary but only after th observation. Sequential Testing for PeekingThe Sequential Probability Ratio Test (SPRT) compares the likelihood ratio at the -th observation, given by: where and are the likelihood functions under the null hypothesis and the alternative hypothesis , respectively. The test compares the likelihood ratio to two thresholds, and , and the decision rule is: The thresholds and are determined based on the desired error probabilities. For a significance level (probability of a Type I error) and power (probability of detecting a true effect when is true), the thresholds are given by: Normal DistributionThis test is in practice a lot easier to carry out for certain distributions like the normal distribution, assume an unknown mean and known variance The sequential rule becomes the recurrent sum, (with ) With the stopping rule : Accept : Accept : continue There is another elegant method outlined in Evan Miller’s blog post, which I will not go into here but just state it for brevity (it is also used at Etsy, so there is certainly some benefit to it). It is a very good read and I highly recommend it. At the beginning of the experiment, choose a sample size . Assign subjects randomly to the treatment and control, with 50% probability each. Track the number of incoming successes from the treatment group. Call this number . Track the number of incoming successes from the control group. Call this number . If reaches , stop the test. Declare the treatment to be the winner. If reaches , stop the test. Declare no winner. If neither of the above conditions is met, continue the test. Using these techniques you can “peek” at the test data as it comes in and decide to stop as per your requirement. This is very useful as the following simulation using this more complex criteria shows. Note that what you want to verify is two things, Does early stopping under the null hypothesis, accept the null in approximately fraction of simulations once the stopping criteria is reached and does it do sofast. Does early stopping under the alternative reject the null hypothesis in fraction of simulations and does it do sofast. The answer to these two questions is not always symmetrical, and it seems that we need more samples to reject the null (case 2) versus accept it case 1. Which is as it should be! But in both cases, as the simulations below show, you need a significantly fewer number of samples than before. CUPED and Other Similar TechniquesRecall, our diff-in-diff equation, Diff in Diff is nothing but CUPED for . I state this without proof. I was not able to find a clear one any where. Consider the auto-regression with control variates regression equation, This is also NOT equivalent to CUPED, nor is it a special case. Again, I was not able to find a good proof anywhere. Multiple HypothesesIn most of the introduction, we set the scene by considering only one hypotheses. However, in real life you may want to test multiple hypotheses at the same time. You may be testing multiple hypotheses even if you did not realize it, such as over time. In the example of early stopping you are actually checking multiple hypotheses. One at every time point. You truly want to test multiple features of your product at the same time and want to run one test to see if the results got better. Regression Model SetupWe consider a regression model with three treatments, , , and , to study their effects on a continuous outcome variable, . The model is specified as: where: is the outcome variable, , , and are binary treatment indicators (1 if the treatment is applied, 0 otherwise), is the intercept, , , and are the coefficients representing the effects of treatments , , and , respectively, is the error term, assumed to be normally distributed with mean 0 and variance . Hypotheses SetupWe aim to test whether each treatment has a significant effect on the outcome variable . This involves testing the null hypothesis that each treatment coefficient is zero. The null hypotheses are formulated as follows: Each null hypothesis represents the assumption that a particular treatment (either , , or ) has no effect on the outcome variable , implying that the treatment coefficient for that treatment is zero. Multiple Hypothesis TestingSince we are testing three hypotheses simultaneously, we need to control for the potential increase in false positives. We can use a multiple hypothesis testing correction method, such as theBonferroni correction or the Benjamini-Hochberg procedure. Bonferroni CorrectionWith the Bonferroni correction, we adjust the significance level for each hypothesis test by dividing it by the number of tests . If we want an overall significance level of , then each individual hypothesis would be tested at: Benjamini-Hochberg ProcedureAlternatively, we could apply the Benjamini-Hochberg procedure to control the False Discovery Rate (FDR). The procedure involves sorting the p-values from smallest to largest and comparing each p-value with the threshold: where is the rank of the p-value and is the total number of tests. We declare all hypotheses with p-values meeting this criterion as significant. This framework allows us to assess the individual effects of , , and while properly accounting for multiple hypothesis testing. Variance Reduction: CUPEDWhen analyzing the effectiveness of a recommender system, sometimes your metrics are skewed by high variance in the metric i.e. . One easy way to fix this is by using the usual outlier removal suite of techniques. However, outlier removal is a difficult thing to statistically define, and very often you may be losing “whales”. Customers who are truly large consumers of a product. One easy way to do this would be to normalize the metric by its mean, i.e. . Any even better way to do this would be to normalize the metric by that users own mean, i.e. . This is the idea behind CUPED. Consider, the regression form of the treatment equation, Assume you have data about the metric from before, and have values . Where the subscript denoted the individuals outcome, before the experiment was even run, . This is like running a regression of on . Now, use those residuals in the treatment equation above, And then estimate the treatment effect. The statistical theory behind CUPED is fairly simple and setting up the regression equation is not difficult. However, in my experience, choosing the right window for pre-treatment covariates is extremely difficult, choose the right window and you reduce your variance by a lot. The right window depends a lot on your business. Some key considerations, Sustained purchasing behavior is a key requirement. If the is not a good predictor of for the interval to then the variance of will be high. Defeating the purpose. Longer windows come with computational costs. In practice, because companies are testing things all the time you could have noise left over from a previous experiment that you need to randomize/ control for. Simulating CUPEDOne way you can guess a good pre-treatment window is by simulating the treatment effect for various levels of MDEs (the change you expect to see in ) and plot the probability of rejecting the alternative hypothesis if it is true i.e. Power. So you read off your hypothesized MDE and Power, and then every point to the left of that is a good window. As an example, lets say you know your MDE to be and you want a power of , then your only option is the 16 week window. Analogously, if you have an MDE of and you want a power of , then the conventional method (with no CUPED) is fine as you can attain an MDE of with a power of . Finally, if you have an MDE of and you want a power of then a 1 week window is fine. Finally, you can check that you have made the right choice by plotting the variance reduction factor against the pre-period (weeks) and see if the variance reduction factor is high. CUPED is a very powerful technique, but if I could give one word of advice to anyone trying to do it, it would be this:get the pre-treatment windowright. This has more to do with business intelligence than with statistics. In this specific example longer windows gave higher variance reduction, but I have seen cases where a “sweet spot” exists. Variance Reduction: CUPACAs it turns out we can control variance, by other means using the same principle as CUPED. The idea is to use a control variate that is not a function of the treatment. Recall, the regression equation we ran for CUPED, Generally speaking, this is often posed as finding some that is uncorrelated with the treatment but correlated with . You could useany that is uncorrelated with the treatment but correlated with . An interesting thing to try would be to fit a highly non-linear machine learning model to (such as random forest, XGBoost) using a set of observable variables , call it . Then use as your . Notice here two things, - that is not a function of but is a function of . - that does not (necessarily) need any data from to be calculated, so it is okay, ifno pre-treatment dataexists! - if pre-treatment data exists then you can use it to fit and then use it to predict at as well, so it can only enhance the performance of your fit and thereby reduce variance even more. If you really think about it, any process to create pre-treatment covariates inevitably involves finding some highly correlated with outcome and uncorrelated with treatment and controlling for that. In CUPAC we just dump all of that into one ML model and let the model figure out the best way to control for variance using all the variables we threw in it. I highly recommend CUPAC over CUPED, it is a more general technique and can be used in a wider variety of situations. If you really want to, you can throw into the mix as well! A Key Insight: Recommendation Engines and CUPAC/ CUPEDTake a step back and think about what isreally saying in context of a recommender system, it is saying given some can I predict my outcome metric. Let us say the outcome metric is some , where is sales. What is a recommender system? It takes some and predicts . This basically means that a pretty good function to control for variance is a recommender system itself! Now you can see why CUPAC is so powerful, it is a way to control for variance using a recommender system itself. You have all the pieces ready for you. HOWEVER! You cannot use the recommender system you are currently testing as your , that would be mean that is correlated with and that would violate the assumption of uncorrelatedness. Usually, the existing recommender system (the pre-treatment one) can be used for this purpose. The finally variable then has a nice interpretation it is not the difference between what peopletruly did and the recommended value, but rather the difference between the two recommender systems! Any model is a variance reduction model, it is just a question of how much variance it reduces. Since the existing recommender system is good enough it is likely to reduce a lot of variance. If it is terrible (which is why they hired you in the first place) then this approach is unlikely to work. But in my experience, existing recommendations are always pretty good in the industry it is a question of finding those last few drops of performance increase. ConclusionThe above are pretty much all you can expect to find in terms of evaluating models in Consulting. In my experience considering all the possibilities that would undermine your test are worth thinking about before embarking on the AB test.","link":"/2024/11/08/consulting-ab-testing/"},{"title":"Unifying Tensor Factorization and Graph Neural Networks: Review of Mathematical Essentials for Recommender Systems","text":"IntroductionWhen do I use “old-school” ML models like matrix factorization and when do I use graph neural networks? Can we do something better than matrix factorization? Why can’t we use neural networks? What is matrix factorization anyway? These are just some of the questions, I get asked whenever I start a recommendation engine project. Answering these questions requires a good understanding of both algorithms, which I will try to outline here. The usual way to understand the benefit of one algorithm over the other is by trying to prove that one is a special case of the other. While it can be shown that a Graph Neural Network can be expressed as a matrix factorization problem. This matrix is not easy to interpret in the usual sense. Contrary to popular belief, matrix factorization (MF) is not “simpler” than a Graph Neural Network (nor is the opposite true). To make matters worse, the GCN is actually more expensive to train since it takes far more cloud compute than does MF. The goal of this article is to provide some intuition as to when a GCN might be worthwhile to try out. This article is primarily aimed at data science managers with some background in linear algebra (or not, see next sentence) who may or may not have used a recommendation engine package before. Having said that, if you are not comfortable with some proofs I have a key takeaways subsection in each section that should form a good basis for decision making that perhaps other team members can dig deep into. Key Tenets of Linear Algebra and Graphs in Recommendation Engine designThe key tenets of design come down to the difference between a graph and a matrix. The linking between graph theory and linear algebra comes from the fact that ALL graphs come with an adjacency matrix. More complex versions of this matrix (degree matrix, random walk graphs) capture more complex properties of the graph. Thus you can usually express any theorem in graph theory in matrix form by use of the appropriate matrix. The Matrix Factorization of the interaction matrix (defined below) is the most commonly used form of matrix factorization. Since this matrix is the easiest to interpret. Any Graph Convolutional Neural Network can be expressed as the factorization of some matrix, this matrix is usually far removed from the interaction matrix and is complex to interpret. For a given matrix to be factorized, matrix factorization requires fewer parameters and is therefore easier to train. Graphical structures are easily interpretable even if matrices expressing their behavior are not. Tensor Based MethodsIn this section, I will formulate the recommendation engine problem as a large tensor or matrix that needs to be “factorized”.In one of my largest projects in Consulting, I spearheaded the creation of a recommendation engine for a top 5 US retailer. This project presented a unique challenge: the scale of the data we were working with was staggering. The recommendation engine had to operate on a 3D tensor, made up of products × users × time. The sheer size of this tensor required us to think creatively about how to scale and optimize the algorithms. Let us start with some definitions, assume we have and , users, products and time points respectively. User latent features, given by matrix of dimension and each index of this matrix is Products latent features, given by matrix , of dimensions and each index of this matrix is Time latent features given by Matrix , of dimensions and each index of this matrix is Interaction given by in the tensor case, and in the matrix case. Usually this represents either purchasing decision, or a rating (which is why it is common to name this ) or a search term. I will use the generic term “interaction” to denote any of the above. In the absence of a third dimension one could look at it as a matrix factorization problem, as shown in the image below, Increasingly, however, it is important to take other factors into account when designing a recommendation system, such as context and time. This has led to the tensor case being the more usual case. This means that for the th user, th product at the th moment in time, the interaction is functionally represented by the dot product of these matrices, An interaction can take a variety of forms, the most common approach, which we follow here will be, , if the th user interacted with the th product at that th instance. Else, . But other more complex functional forms can exist, where we can use the rating of an experience at that moment, where instead of we can have a more general form . Thus this framework is able to handle a variety of interaction functions. A question we often get is that this function is inherently linear since it is the dot product of multiple matrices. We can handle non-linearity in this framework as well, via the use of non-linear function (a.k.a an activation function). Or something along those lines. However, one of the attractions of this approach is that it is absurdly simply to set up. Side InformationVery often in a real word use case, our clients often have information that they are eager to use in a recommendation system. These range from user demographic data that they know from experience is important, to certain product attribute data that has been generated from a different machine learning algorithm. In such a case we can integrate that into the equation given above, Where, are attributes for users and products that are known beforehand. Each of these vectors are rows in , that are called “side-information\" matrices. OptimizationWe can then set up the following loss function, Where: and are regularization terms for the alignment with side information. controls the regularization of the latent matrices , , and . The first term is the reconstruction loss of the tensor, ensuring that the interaction between users, products, and time is well-represented. The second and third terms align the latent factors with the side information for users and products, respectively. Tensor Factorization LoopFor each iteration: Compute the predicted tensor using the factorization: Compute the loss using the updated loss function. Perform gradient updates for , , and . Regularize the alignment between , with and Repeat until convergence. Key TakeawayMatrix factorization allows us to decompose a matrix into two low-rank matrices, which provide insights into the properties of users and items. These matrices, often called embeddings, either embed given side information or reveal latent information about users and items based on their interaction data. This is powerful because it creates a representation of user-item relationships from behavior alone. In practice, these embeddings can be valuable beyond prediction. For example, clients often compare the user embedding matrix with their side information to see how it aligns. Interestingly, clustering users based on can reveal new patterns that fine-tune existing segments. Rather than being entirely counter-intuitive, these new clusters may separate users with subtle preferences, such as distinguishing between those who enjoy less intense thrillers from those who lean toward horror. This fine-tuning enhances personalization, as users in large segments often miss out on having their niche behaviors recognized.Mathematically, the key takeaway is the following equation (at the risk of overusing a cliche, this is the of the recommendation engines world) Multiplying the lower dimensional representation of the th user and the th item together yields a real number that represents the magnitude of the interaction. Very low and its not going to happen, and very high means that it is. These two vectors are the “deliverable”! How we got there is irrelevant. Turns out there are multiple ways of getting there. One of them is the Graph Convolutional Network. In recommendation engine literature (particularly for neural networks) embeddings are given by , in the case of matrix factorization, is obtained by stacking and , ExtensionsYou do not need to stick to the simple multiplication in the objective function, you can do something more complex, The above objective function is the LINE embedding. Where is some non-linear function. Interaction Tensors as GraphsOne can immediately view a the interactions between users and items as a bipartite graph, where an edge is present only if the user interacts with that item. It is immediately obvious that we can embed the interactions matrix inside the adjacency matrix, noting that there are no edges between users and there are no edges between items. The adjacency matrix can be represented as: Recall, the matrix factorization , where: is the user-item interaction matrix (binary values: 1 if a user has interacted with an item, 0 otherwise), is the transpose of , representing item-user interactions. For example, if is the following binary interaction matrix: Note, here that could have contained real numbers (such as ratings etc.) but the adjacency matrix is strictly binary. Using the weighted adjacency matrix is perfectly “legal”, but has mathematical implications that we will discuss later. Thus, the adjacency matrix becomes: Matrix Factorization of Adjacency MatrixNow you could use factorize, And then use the embeddings and , but now represents embeddings both for users and items (as does ). However, this matrix is much bigger than since the top left and bottom right block matrix are . You are much better off using the formulation to quickly converge on the optimal embeddings. The key here is that factorizing this matrix is roughly equivalent to factorizing the matrix. This is important because the adjacency matrix plays a key role in the graphical convolutional network. What are the REAL Cons of Matrix FactorizationMatrix factorization offers key advantages in a consulting setting by quickly assessing the potential of more advanced methods on a dataset. If the user-item matrix performs well, it indicates useful latent user and item embeddings for predicting interactions. Additionally, regularization terms help estimate the impact of any side information provided by the client. The resulting embeddings, which include both interaction and side information, can be used by marketing teams for tasks like customer segmentation and churn reduction.First, let me clarify some oft quoted misconceptions about matrix factorization disadvantages versus GCNs, User item interactions are a simple dot product () and is therefore not linear. This is not true, even in the case of a GCN the final prediction is given by a simple dot product between the embeddings. Matrix factorization cannot use existing features . This is probably due to the fact that matrix factorization was popularized by the simple Netflix case, where only user-item matrix was specified. But in reality, very early in the development of matrix factorization, all kinds of additional regularization terms such as bias, side information etc. were introduced. The side information matrices are where you can specify existing features (recall, ). Cannot handle cold start Neither matrix factorization nor neural networks can handle the cold start problem very well. However, this is not an unfair criticism as the neural network is better, but this is more as a consequence of its truly revolutionary feature, which I will discuss under its true advantage. Higher order interactions this is also false, but it is hard to see it mathematically. Let me outline a simple approach to integrate side information. Consider the matrix adjacency matrix , gives you all edges with length , such that represents all nodes that are at most edges away. You can then factorize this matrix to get what you want. This is not an unfair criticism either as multiplying such a huge matrix together is not advised and neither is it the most intuitive method. The biggest problem with MF is that a matrix is simply not a good representation of how people interact with products and each other. Finding a good mathematical representation of the problem is sometimes the first step in solving it. Most of the benefits of a graph convolutional neural network come as a direct consequence of using a graph structure not from the neural network architecture. The graph structure of a user-item behavior is the most general form of representation of the problem. Complex Interactions - In this structure one can easily add edges between users and between products. Note in the matrix factorization case, this is not possible since is only users x items. To include more complex interactions you pay the price with a larger and larger matrix. Graph Structure - Perhaps the most visually striking feature of graph neural networks is that they can leverage graph structure itself (see Figure 4). Matrix factorization cannot do so easily Higher order interactions can be captured more intuitively than in the case of matrix factorization Before implementing a GCN, it’s important to understand its potential benefits. In my experience, matrix factorization often provides good results quickly, and moving to GCNs makes sense only if matrix factorization has already shown promise. Another key factor is the size and richness of interactions. If the graph representation is primarily bipartite, adding user edges may not significantly enhance the recommender system. In retail, edges sometimes represented families, but these structures were often too small to be useful—giving different recommendations to family members like and is acceptable since family ties alone don’t imply similar consumption patterns. However, identifying influencers, such as nodes with high degrees connected to isolated nodes, could guide targeted discounts for products they might promote. I would be remiss, if I did not add that ALL of these issues with matrix factorization can be fixed by tweaking the factorization in some way. In fact, a recent paper Unifying Graph Convolutional Networks as Matrix Factorization by Liu et. al. does exactly this and shows that this approach is even better than a GCN. Which is why I think that the biggest advantage of the GCN is not that it is “better” in some sense, but rather the richness of the graphical structure lends itself naturally to the problem of recommending products, even if that graphical structure can then be shown to be equivalent to some rather more complex and less intuitive matrix structure. I recommend the following experiment flow : A Simple GCN modelLet us continue on from our adjacency matrix and try to build a simple ML model of an embedding, we could hypothesize that an embedding is linearly dependent on the adjacency matrix. The second additive term bears a bit of explaining. Since the adjacency matrix has a diagonal, a value of get multiplied with the node’s own features . To avoid this we add the node’s own feature matrix using the diagonal matrix. We need to make another important adjustment to , we need to divide each term in the adjacency matrix by the degree of each node. At the risk of abusing notation, we redefine as some normalized form of the adjacency matrix after edges connecting each node with itself have been added to the graph. I like this notation because it emphasizes the fact that you do not need to do this, if you suspect that normalizing your nodes by their degree of connectivity is not important then you do not need to do this step (though it costs you nothing to do so). In retail, the degree of a user node refers to the number of products they consume, while the degree of a product node reflects the number of customers it reaches. A product may have millions of consumers, but even the most avid user node typically consumes far fewer, perhaps only hundreds of products. Here ]. Here we can split the equations by the subgraphs for which they apply to, Note the equivalence the matrix case, in the matrix case we have to stack it ourselves because of the way we set up the matrix, but in the case of a GCN is already and represents embeddings of both users and items. The likelihood of an interaction is, The loss function is, We can substitute the components of to get a tight expression for optimizing loss, This is the main “result” of this blog post that you can equally look at this one layer GCN as a matrix factorization problem of the user-item interaction matrix but with the more complex looking low rank matrices on the right. In this sense, you can always create a matrix factorization that equates to the loss function of a GCN. You can update parameters using SGD or some other technique. I will not get into that too much in this post. Understanding the GCN equationEquations 1 and 2 are the most important equations in the GCN framework. is some set of weights that learn how to embed or encode the information contained in into . For this one layer model, we are only considering values from the nodes that are one edge away, since the value of is only dependent on all the ’s that are directly connected to it and its own . However, if you then apply this operation again, now has all the information contained in all the nodes connected to it in its own but also so does every other nodes . More succinctly, Equivalence to Matrix Factorization for a one layer GCNYou could just as easily have started with two random matrices and and optimize them using your favorite optimization algorithm and end up with the likelihood for interaction function, So you get the same outcome for a one layer GCN as you would from matrix factorization. Note that, it has been proved that even multi-layer GCNs are equivalent to matrix factorization but the matrix being factorized is not that easy to interpret. Key TakeawaysThe differences between MF and GCN really begin to take form when we go into multi-layerd GCNs. In the case of the one layer GCN the embeddings of are only influenced by the nodes connected to it. Thus the features of a customer node will be only influenced by the products that they buy, similarly, the product node will be only influenced by the customers who by them. However, for deeper neural networks : 2 layer: every customer’s embedding is influenced by the embeddings of the products they consume and the embeddings of other customers of the products they consume. Similarly, every product is influenced by the customers who consume that product as well as by the products of the customers who consume that product. 3 layer: every customers embedding is influenced by the products they consume, other customers of the products they consume and products consumed by other customers of the products they consume. Similarly, every product is influenced by the consumers of that product, as well as products of consumers of that product as well as products consumed by consumers of that product. You can see where this is going, in most practical applications, there are only so many levels you need to go to get a good result. In my experience is the bare minimum (because is unlikely to do better than an MF, in fact they are equivalent) and is about how deep you can feasibly go without exploding the number of training parameters. That leads to another critical point when considering GCNs, you really pay a price (in blood, mind you) for every layer deep you go. Consider the one layer case, you really have and parameters to learn, because you have to learn both the weight matrix and the matrix of embeddings . But the MF case you directly learn . So if you were only going to go one layer deep you might as well use matrix factorization. Going the other way, if you are considering more than layers the reality of the problem (in my usual signal processing problems this would be “physical” laws) i.e. the behavioral constraints mean that more than 3 degrees deep of influence (think about what point 3 would mean for a layer network) is unlikely to be supported by any theoretical evidence of consumer behavior. Final Prayer and BlessingI would like for the reader of this to leave with a better sense of the relationship between matrix factorization and GCNs. Like most neural network based models we tend to think of them as a black box and a black box that is “better”. However, in the one layer GCN case we can see that they are equal, with the GCN in fact having more learnable parameters (therefore more cost to train).Therefore, it makes sense to use layers or more. But when using more, we need to justify them either behaviorally or with expert advice. How to go from MF to GCNs Start with matrix factorization of the user-item matrix, maybe add in context or time. If it performs well and recommendations line up with non-ML recommendations (using base segmentation analysis), that means the model is at least somewhat sensible. Consider doing a GCN next if the performance of MF is decent but not great. Additionally, definitely try GCN if you know (from marketing etc) that the richness of the graph structure actually plays a role in the prediction. For example, in the sale of Milwaukee tools a graph structure is probably not that useful. However, for selling Thursday Boots which is heavily influenced by social media clusters, the graph structure might be much more useful. Interestingly, the MF matrices tend to be very long and narrow (there are usually thousands of users and most companies have far more users than they have products. This is not true for a company like Amazon (300 million users and 300 million products). But if you have a long narrow matrix that is sparse you are not too concerned with computation since at worst you have , it does not matter much whether you do MF or GCN, but when , for such a case the matrix approach will probably give you a faster result. It is worthwhile in a consulting environment to always start with a simple matrix factorization, the GCN for simplicity of use and understanding but then find a matrix structure that approximates only the most interesting and rich aspects of the graph structure that actually influence the final recommendations. Referenceshttps://jonathan-hui.medium.com/graph-convolutional-networks-gcn-pooling-839184205692https://tkipf.github.io/graph-convolutional-networks/ https://openreview.net/forum?id=HJxf53EtDrhttps://distill.pub/2021/gnn-intro/ https://jonathan-hui.medium.com/graph-convolutional-networks-gcn-pooling-839184205692","link":"/2024/09/28/graph-convolutional-neural-network-and-matrix-factorization/"},{"title":"Book Review: Kafka On The Shore: Haruki Murakami","text":"IntroductionA while ago I came across a collection of Murakami’s short stories in a quaint New York bookstore (that was going out of business, no less). This was my first experience with a Murakami novel. For the unininitiaed(like I was) Murakami’s style is a blend of magical realism, surrealism and a heavy dose of the every day banality that makes up a large chunk of the human lived experience.Overall the experience was good enough for me to grab a Murakami novel off my aunt’s bookstand and read my way through it over the 4th of July holiday. What follows are some of my thoughts aboutKafka on the Shore. PlotThe book has two interrelated plots, one of Kafka Tamura the titular and main character. The other is Saturo Nakata. The story opens with Kafka running away from home and changing his name (he changes his name to Kafka and we never really learn his real name).What follows are his adventures after leaving home, we are informed of a troubled past wherein his mother left with his sister thereby abandoning Kafka.Each chapter of Kafka’s adventure is interleaved with the experience of Nakata, an old man who has the strange ability to speak with cats. The result of a childhood affliction following a strange celestial event.The two stories are on a collision course throughout the novel. We learn just enough of both stories to learn how they might be connected, and indeed Nakata is inexorably drawn to Kafka and spends much of the novel trying to reach him (even as Kafka tries to run away). AnalysisHere, I must confess that I had several issues with Murakami’s style. While the blend of magical realism and surrealism make for quite compelling reading, I fear that the page turning nature of the book was rather more due to the pace and unanswered questions of the story rather that the style of writing itself.Murakami gives the reader multiple blank checks, for instance. The mysterious event in the Yamanashi Prefecture that inexplicably leads to Nakata’s ability to speak to cats The entrace stone that draws Nakata to it and the creature that crawls out of it The multitude of garishly outlandish characters such as Colonel Sanders (whom you may know from KFC) and Johnnie Walker (another rather well known figure, I am told. I wouldn’t know). The connection between Kafka and Nakata For 1,2 and 3 we are given no explanation, sadly these checks could not be cashed, so to speak. While magical realism and surrealism are Murakami’s metiere, it felt almost like the story was not believable even unto itself. To me this is an invoiolable rule of story telling.A story must be real to itself, if not necessarily to its readers. One must acknowledge that these literary elements are all part of some mystical whole. Unfortunately, this missed me by some margin.In effect, it felt like a surrealist drama that played out in front of an audience and then abruptly ended. As the book does, with the last couple of chapters a hurried conclusion. To be honest, had it not been for the introduction of the worm like creature that exited the entrance stone, I probably would have forgiven the book its (many) faults.However, the sudden introduction of that creature along with the lack of explanation for many other plot points led me to almost fling the book down in disgust.Magical realism was meant to use fantastical settings to explore deeper themes, however, Murakami uses them as a device to advance the plot without reconciling them together. In the absence of such an effort, it became difficult for me as the reader to accept the “magical” elements as “real”, even in the context of the book. Things were not all bad however, for the first three quarters of the book, the magic felt real. ConclusionAll in all a good book, if lacking in some real substance. Maybe this is the point of magical realism. I dont know.While I love philosophizing about books, there is a point at which one engages in such activity too much.","link":"/2025/08/03/kafka-on-the-shore/"},{"title":"Where Have All The Left Backs Gone?","text":"As the season draws to a close, no matter whether we win the league or not, I’m happy with where Arsenal are as a club. Although, I’d better get my feelings off my chest before Championship Sunday. I cannot help but feel that our left backs cost us the league. Football seems symmetric to the untrained eye but really it’s never been more asymmetric, and no position exemplifies that like left back. And no team has suffered because of their left back as much as Arsenal have. Zinchenko is our best offensive option, but comes with very clear (though sometimes overblown) defensive limitations. Z just doesn’t have the instincts of a defensive left back, in fact Z doesn’t have many instincts of a defender at all. On some European nights he delighted us with an inch perfect left footed pass that scythed through a low block but on other nights he was taken to the cleaners by a virtually unknown European right back. In England, he was profligate with his passing and rendered unplayable versus the top right backs in England (unlike left back, attacking right backs seem to be sprouting all over England like an unwelcome disease). Our other options are all giants of men, who in their hearts know not whether they can string a pass together with the same careless abandon that Z can. Kiwior and Tomiyasu are both impressive defensive specimens but their quality on the ball shows that they’re really centre backs in hiding. As Gary Neville said (I think), if you’re taller than 6 feet you’d sooner play basketball than fullback. Which is good news since our secret weapon, all 5 feet 10 inches of Jurrien Timber will be available for selection next season. Though Timber will have to fight off doubts that he too is just a smaller centre back with incisive passing , such as he never showed in Holland. Arsenal is not unique in their search for a quality left back. The famine of quality players in the position meant that only Destiny Udogie (of all people) was the only consistently featured “true” left back in the top 6 in England. That Gary Neville’s selection of him to his Premier League Team of the Season was met with raised eyebrows should tell you all need to know about the position. Manchester City have effectively given up on the position, playing centre backs (Akanji, Ake, Gvardiol) at the position, I don’t know that they’ve had a true left back on the roster in a while. Due apologies to Gvardiol, a goal in Madrid does not a full back make. Andy Robertson stands out as the best left back in England (for some years now) and a true representation of what the position can become when played with the fluency and elegance of someone who has both a left foot, football intelligence and about 3-4 sets of lungs. Across Europe things don’t look any better. If you’d gone to get a cup of tea during the one forward run that Mendy made during their dalliance with City you’d struggle to name Madrid’s left back. You’d struggle to name his replacement too. A certain Welshman could walk into the squad right now if he played his boyhood position. I’ve always had a soft spot for Os Navegadores’ Nuno Mendes, he’s an incredible player, much like Robertson, but Sancho kept him up at night and ran rings around him during the day. Alphonso Davies stands out at the best player at the position in the world, how ironic then that it was his goal that spelt the beginning of the end of our European adventure. If a Scotsman and a Canadian are the best at left back in world football right now, surely our Brazilian cousins and Portuguese ancestors must be positively rotating in their graves. All in all, I think left backs have fallen off since the glory days of the “wing back”, we still miss you Roberto Carlos. Arsenal have always fielded some greats at the position, from the flamboyant English left back of my childhood, Ashley Cole to the Frenchman Gael Clichy. Honorable mention to Patrice Evra and Leighton-bloody-Baines. Maybe the position has an evolution coming or maybe a mass extinction to be replaced by soulless centre backs who couldn’t make it. Or maybe we’ll figure out a way to play strikers at the position so that when we have the ball they can beat the trap and run into crosses. Oh wait…never mind.","link":"/2024/05/16/left-backs-premier-league/"},{"title":"Part II :  Shrinking Neural Networks for Embedded Systems Using Low Rank Approximations (LoRA)","text":"Code Follow AlongThe code can be found in the same tensor rank repository :https://github.com/FranciscoRMendes/tensor-rank/blob/main/CNN_Decomposition.ipynb Convolutional Layer CaseThe primary difference between the fully connected layer case and the convolutional layer case is the fact that the convolutional kernel is a tensor. We say that the number of multiplications in an operation depends on the size of the dimensions of the tensors involved in the multiplication. It becomes critical to approximate one large multi-dimensional kernel with multiple smaller kernels of lower dimension. Working ExampleA convolution operation maps a tensor from one dimension to a tensor of another (possibly) lower dimension. If we consider an input tensor of size then a convolution layer will map this to a tensor, of size using a kernel tensor, of size . Using similar logic as defined before, the number of multiplies required for this operation in . Thus a convolution layer is simply a multiplication operation over a subset of a tensor yielding a smaller tensor. We can approximate the kernel using a series of \"smaller\" multiplication operations by reducing the dimensions we need to multiply across. Consider the following decomposition, This is similar to the case of the SVD with an important difference the core is a 4 dimensional tensor and not a matrix. Where the first term in each bracket is the size and the second term is the number of multiplies. The size is of one kernel whereas the number of multiplies is the total number of multiplies to generate the complete output (i.e. if the operation denoted by the red arrows was calculated for the entire input tensor to generate the entire output tensor, it is given by the formula, product of kernel dimensions product of output tensor dimensions ). : ( , ) : (, ) : (, ) Which makes the transformation function (with number of multiplies), The number of multiplies in this operation is which is less than if and . In the figure above each of the red arrows indicates a one convolution operation, in figure (a) you have one fairly large filter and one convolution, whereas in the figure (b) you have 3 convolution operations using 3 smaller filters. Let’s break down the space and size savings for this. Recall, for our fully connected layers this formula was, Rank SelectionSo we are ready to go with a decomposition, all that remains is finding out the optimal rank. Again, if the rank is too low we end up with high compression but possibly low accuracy, and if the rank is too high we end up with very low compression. In embedded systems we need to be quite aggressive in compression since this can be a hard constraint on solving a problem. In our previous example, we could \"locally\" optimize rank but in the case of CNNs this is not possible since at the very least we would have at least one FC and one convolutional layer and we must compress both somewhat simultaneously. Brute force quickly becomes hard, as for even modest tensor combinations such as and we can end up with far too many combinations to try. Binary search is also not possible since it is not clear if simultaneously lowering both ranks lowers accuracy. Naive Rank Selection AlgorithmThe intuition is that if a layer is more sensitive to decomposition it should have higher ranks. Our algorithm is proprietary but the intuition is as follows. We start with a rank of 1 and increase the rank by 1 until the accuracy drops below a certain threshold. We then decrease the rank by 1 and continue until the accuracy drops below a certain threshold. We then take the rank that gave the highest accuracy. This is a naive algorithm and can be improved upon.","link":"/2024/04/24/lora-2/"},{"title":"Part III :  What does Low Rank Factorization of a Convolutional Layer really do?","text":"Decomposition of a Convolutional layerIn a previous post I described (in some detail) what it means to decompose a matrix multiply into a sequence of low rank matrix multiplies. We can do something similar for a tensor as well, this is somewhat less easy to see since tensors (particularly in higher dimensions) are quite hard to visualize.Recall, the matrix formulation,Where and are the left and right singular vectors of respectively. The idea is to approximate as a sum of outer products of and of lower rank.Now instead of a weight matrix multiplication we have a kernel operation, where is the convolution operation. The idea is to approximate as a sum of outer products of and of lower rank.Interestingly, you can also think about this as a matrix multiplication, by creating a Toplitz matrix version of , call it and then doing . But this comes with issues as is much much bigger than . So we just approach it as a convolution operation for now. Convolution OperationAt the heart of it, a convolution operation takes a smaller cube subset of a “cube” of numbers (also known as the map stack) multiplies each of those numbers by a fixed set of numbers (also known as the kernel) and gives a single scalar output. Let us start with what each “slice” of the cube really represents. Now that we have a working example of the representation, let us try to visualize what a convolution is. A convolution operation takes a subset of the RGB image across all channels and maps it to one number (a scalar), by multiplying the cube of numbers with a fixed set of numbers (a.k.a kernel, not pictured here) and adding them together.A convolution operation multiplies each pixel in the image across all channels with a fixed number and add it all up. Low Rank Approximation of ConvolutionNow that we have a good idea of what a convolution looks like, we can now try to visualize what a low rank approximation to a convolution might look like. The particular kind of approximation we have chosen here does the following 4 operations to approximate the one convolution operation being done. Painful Example of Convolution by handConsider the input matrix : Input slice: Kernel: Element-wise multiplication and sum: Now repeat that by moving the kernel one step over (you can in fact change this with the stride argument for convolution). Low Rank Approximation of convolutionNow we will painfully do a low rank decomposition of the convolution kernel above. There is a theorem that says that a matrix can be approximated by a sum of 2 outer products of two vectors. Say we can express as, We can easily guess . Consider, This is easy because I chose values for the kernel that were easy to break down. How to perform this breakdown is the subject of the later sections. Consider the original kernel matrix and the low-rank vectors: The input matrix is: Convolution with Original KernelPerform the convolution at the top-left corner of the input matrix: Convolution with Low-Rank VectorsUsing the low-rank vectors: Step 1: Apply (filter along the columns):** Step 2: Apply (sum along the rows):** Comparison Convolution with Original Kernel: -2 Convolution with Low-Rank Vectors: 0 The results are different due to the simplifications made by the low-rank approximation. But this is part of the problem that we need to optimize for when picking low rank approximations. In practice, we will ALWAYS lose some accuracy PyTorch ImplementationBelow you can find the original definition of AlexNet. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Net(nn.Module): def __init__(self): super().__init__() self.layers = nn.ModuleDict() self.layers['conv1'] = nn.Conv2d(3, 6, 5) self.layers['pool'] = nn.MaxPool2d(2, 2) self.layers['conv2'] = nn.Conv2d(6, 16, 5) self.layers['fc1'] = nn.Linear(16 * 5 * 5, 120) self.layers['fc2'] = nn.Linear(120, 84) self.layers['fc3'] = nn.Linear(84, 10) def forward(self,x): x = self.layers['pool'](F.relu(self.layers['conv1'](x))) x = self.layers['pool'](F.relu(self.layers['conv2'](x))) x = torch.flatten(x, 1) x = F.relu(self.layers['fc1'](x)) x = F.relu(self.layers['fc2'](x)) x = self.layers['fc3'](x) return xdef evaluate_model(net): import torchvision.transforms as transforms batch_size = 4 # [4, 3, 32, 32] transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) # prepare to count predictions for each class correct_pred = {classname: 0 for classname in classes} total_pred = {classname: 0 for classname in classes} # again no gradients needed with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predictions = torch.max(outputs, 1) # collect the correct predictions for each class for label, prediction in zip(labels, predictions): if label == prediction: correct_pred[classes[label]] += 1 total_pred[classes[label]] += 1 # print accuracy for each class for classname, correct_count in correct_pred.items(): accuracy = 100 * float(correct_count) / total_pred[classname] print(f'Original Accuracy for class: {classname:5s} is {accuracy:.1f} %') Now let us decompose the first convolutional layer into 3 simpler layers using SVD 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495def slice_wise_svd(tensor,rank): # tensor is a 4D tensor # rank is the target rank # returns a list of 4D tensors # each tensor is a slice of the input tensor # each slice is decomposed using SVD # and the decomposition is used to approximate the slice # the approximated slice is returned as a 4D tensor # the list of approximated slices is returned num_filters, input_channels, kernel_width, kernel_height = tensor.shape kernel_U = torch.zeros((num_filters, input_channels,kernel_height,rank)) kernel_S = torch.zeros((input_channels,num_filters,rank,rank)) kernel_V = torch.zeros((num_filters,input_channels,rank,kernel_width)) approximated_slices = [] reconstructed_tensor = torch.zeros_like(tensor) for i in range(num_filters): for j in range(input_channels): U, S, V = torch.svd(tensor[i, j,:,:]) U = U[:,:rank] S = S[:rank] V = V[:,:rank] kernel_U[i,j,:,:] = U kernel_S[j,i,:,:] = torch.diag(S) kernel_V[i,j,:,:] = torch.transpose(V,0,1) # print the reconstruction error print(\"Reconstruction error: \",torch.norm(reconstructed_tensor-tensor).item()) return kernel_U, kernel_S, kernel_Vdef svd_decomposition_conv_layer(layer, rank): \"\"\" Gets a conv layer and a target rank, returns a nn.Sequential object with the decomposition \"\"\" # Perform SVD decomposition on the layer weight tensorly. layer_weight = layer.weight.data kernel_U, kernel_S, kernel_V = slice_wise_svd(layer_weight,rank) U_layer = nn.Conv2d(in_channels=kernel_U.shape[1], out_channels=kernel_U.shape[0], kernel_size=(kernel_U.shape[2], 1), padding=0, stride = 1, dilation=layer.dilation, bias=True) S_layer = nn.Conv2d(in_channels=kernel_S.shape[1], out_channels=kernel_S.shape[0], kernel_size=1, padding=0, stride = 1, dilation=layer.dilation, bias=False) V_layer = nn.Conv2d(in_channels=kernel_V.shape[1], out_channels=kernel_V.shape[0], kernel_size=(1, kernel_V.shape[3]), padding=0, stride = 1, dilation=layer.dilation, bias=False) # store the bias in U_layer from layer U_layer.bias = layer.bias # set weights as the svd decomposition U_layer.weight.data = kernel_U S_layer.weight.data = kernel_S V_layer.weight.data = kernel_V return [U_layer, S_layer, V_layer] class lowRankNetSVD(Net): def __init__(self, original_network): super().__init__() self.layers = nn.ModuleDict() self.initialize_layers(original_network) def initialize_layers(self, original_network): # Make deep copy of the original network so that it doesn't get modified og_network = copy.deepcopy(original_network) # Getting first layer from the original network layer_to_replace = \"conv1\" # Remove the first layer for i, layer in enumerate(og_network.layers): if layer == layer_to_replace: # decompose that layer rank = 1 kernel = og_network.layers[layer].weight.data decomp_layers = svd_decomposition_conv_layer(og_network.layers[layer], rank) for j, decomp_layer in enumerate(decomp_layers): self.layers[layer + f\"_{j}\"] = decomp_layer else: self.layers[layer] = og_network.layers[layer] def forward(self, x): x = self.layers['conv1_0'](x) x = self.layers['conv1_1'](x) x = self.layers['conv1_2'](x) x = self.layers['pool'](F.relu(x)) x = self.layers['pool'](F.relu(self.layers['conv2'](x))) x = torch.flatten(x, 1) x = F.relu(self.layers['fc1'](x)) x = F.relu(self.layers['fc2'](x)) x = self.layers['fc3'](x) return x Decomposition into a list of simpler operationsThe examples above are quite simple and are perfectly good for simplifying neural networks. This is still an active area of research. One of the things that researchers try to do is try to further simplify each already simplified operation, of course you pay the price of more operations. The one we will use for this example is one where the operations is broken down into four simpler operations. (Green) Takes one pixel from the image across all channels and maps it to one value (Red) Takes one long set of pixels from one channel and maps it to one value (Blue) Takes one wide set of pixels from one channel and maps it to one value (Green) takes one pixel from all channels and maps it to one value Intuitively, we are still taking the subset “cube” but we have broken it down so that in any given operation only dimension is not . This is really the key to reducing the complexity of the initial convolution operation, because even though there are more such operations each operations is more complex. PyTorch ImplementationIn this section, we will take AlexNet (Net), evaluate (evaluate_model) it on some data and then decompose the convolutional layers. Declaring both the original and low rank networkHere we will decompose the second convolutional layer, given by the layer_to_replace argument. The two important lines to pay attention to are est_rank and cp_decomposition_conv_layer. The first function estimates the rank of the convolutional layer and the second function decomposes the convolutional layer into a list of simpler operations. 12345678910111213141516171819202122232425262728293031323334353637class lowRankNet(Net): def __init__(self, original_network): super().__init__() self.layers = nn.ModuleDict() self.initialize_layers(original_network) def initialize_layers(self, original_network): # Make deep copy of the original network so that it doesn't get modified og_network = copy.deepcopy(original_network) # Getting first layer from the original network layer_to_replace = \"conv2\" # Remove the first layer for i, layer in enumerate(og_network.layers): if layer == layer_to_replace: # decompose that layer rank = est_rank(og_network.layers[layer]) decomp_layers = cp_decomposition_conv_layer(og_network.layers[layer], rank) for j, decomp_layer in enumerate(decomp_layers): self.layers[layer + f\"_{j}\"] = decomp_layer else: self.layers[layer] = og_network.layers[layer] # Add the decomposed layers at the position of the deleted layer def forward(self, x, layer_to_replace=\"conv2\"): x = self.layers['pool'](F.relu(self.layers['conv1'](x))) # x = self.layers['pool'](F.relu(self.laye['conv2'](x) x = self.layers['conv2_0'](x) x = self.layers['conv2_1'](x) x = self.layers['conv2_2'](x) x = self.layers['pool'](F.relu(self.layers['conv2_3'](x))) x = torch.flatten(x, 1) x = F.relu(self.layers['fc1'](x)) x = F.relu(self.layers['fc2'](x)) x = self.layers['fc3'](x) return x Evaluate the ModelYou can evaluate the model by running the following code. This will print the accuracy of the original model and the low rank model. 12345678910111213141516171819202122decomp_alexnet = lowRankNetSVD(net)# replicate with original modelcorrect_pred = {classname: 0 for classname in classes}total_pred = {classname: 0 for classname in classes}# again no gradients neededwith torch.no_grad(): for data in testloader: images, labels = data outputs = decomp_alexnet(images) _, predictions = torch.max(outputs, 1) # collect the correct predictions for each class for label, prediction in zip(labels, predictions): if label == prediction: correct_pred[classes[label]] += 1 total_pred[classes[label]] += 1# print accuracy for each classfor classname, correct_count in correct_pred.items(): accuracy = 100 * float(correct_count) / total_pred[classname] print(f'Lite Accuracy for class: {classname:5s} is {accuracy:.1f} %') Let us first discuss estimate rank. For a complete discussion see the the references by Nakajima and Shinchi. The basic idea is that we take the tensor, “unfold” it along one axis (basically reduce the tensor into a matrix by collapsing around other axes) and estimate the rank of that matrix.You can find est_rank below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114from __future__ import divisionimport torchimport numpy as np# from scipy.sparse.linalg import svdsfrom scipy.optimize import minimize_scalarimport tensorly as tldef est_rank(layer): W = layer.weight.data # W = W.detach().numpy() #the weight has to be a numpy array for tl but needs to be a torch tensor for EVBMF mode3 = tl.base.unfold(W.detach().numpy(), 0) mode4 = tl.base.unfold(W.detach().numpy(), 1) diag_0 = EVBMF(torch.tensor(mode3)) diag_1 = EVBMF(torch.tensor(mode4)) # round to multiples of 16 multiples_of = 8 # this is done mostly to standardize the rank to a standard set of numbers, so that # you do not end up with ranks 7, 9 etc. those would both be approximated to 8. # that way you get a sense of the magnitude of ranks across multiple runs and neural networks # return int(np.ceil(max([diag_0.shape[0], diag_1.shape[0]]) / 16) * 16) return int(np.ceil(max([diag_0.shape[0], diag_1.shape[0]]) / multiples_of) * multiples_of)def EVBMF(Y, sigma2=None, H=None): \"\"\"Implementation of the analytical solution to Empirical Variational Bayes Matrix Factorization. This function can be used to calculate the analytical solution to empirical VBMF. This is based on the paper and MatLab code by Nakajima et al.: \"Global analytic solution of fully-observed variational Bayesian matrix factorization.\" Notes ----- If sigma2 is unspecified, it is estimated by minimizing the free energy. If H is unspecified, it is set to the smallest of the sides of the input Y. Attributes ---------- Y : numpy-array Input matrix that is to be factorized. Y has shape (L,M), where L&lt;=M. sigma2 : int or None (default=None) Variance of the noise on Y. H : int or None (default = None) Maximum rank of the factorized matrices. Returns ------- U : numpy-array Left-singular vectors. S : numpy-array Diagonal matrix of singular values. V : numpy-array Right-singular vectors. post : dictionary Dictionary containing the computed posterior values. References ---------- .. [1] Nakajima, Shinichi, et al. \"Global analytic solution of fully-observed variational Bayesian matrix factorization.\" Journal of Machine Learning Research 14.Jan (2013): 1-37. .. [2] Nakajima, Shinichi, et al. \"Perfect dimensionality recovery by variational Bayesian PCA.\" Advances in Neural Information Processing Systems. 2012. \"\"\" L, M = Y.shape # has to be L&lt;=M if H is None: H = L alpha = L / M tauubar = 2.5129 * np.sqrt(alpha) # SVD of the input matrix, max rank of H U, s, V = torch.svd(Y) U = U[:, :H] s = s[:H] V[:H].t_() # Calculate residual residual = 0. if H &lt; L: residual = torch.sum(torch.sum(Y ** 2) - torch.sum(s ** 2)) # Estimation of the variance when sigma2 is unspecified if sigma2 is None: xubar = (1 + tauubar) * (1 + alpha / tauubar) eH_ub = int(np.min([np.ceil(L / (1 + alpha)) - 1, H])) - 1 upper_bound = (torch.sum(s ** 2) + residual) / (L * M) lower_bound = np.max([s[eH_ub + 1] ** 2 / (M * xubar), torch.mean(s[eH_ub + 1:] ** 2) / M]) scale = 1. # /lower_bound s = s * np.sqrt(scale) residual = residual * scale lower_bound = float(lower_bound * scale) upper_bound = float(upper_bound * scale) sigma2_opt = minimize_scalar(EVBsigma2, args=(L, M, s, residual, xubar), bounds=[lower_bound, upper_bound], method='Bounded') sigma2 = sigma2_opt.x # Threshold gamma term threshold = np.sqrt(M * sigma2 * (1 + tauubar) * (1 + alpha / tauubar)) pos = torch.sum(s &gt; threshold) if pos == 0: return np.array([]) # Formula (15) from [2] d = torch.mul(s[:pos] / 2, 1 - (L + M) * sigma2 / s[:pos] ** 2 + torch.sqrt( (1 - ((L + M) * sigma2) / s[:pos] ** 2) ** 2 - \\ (4 * L * M * sigma2 ** 2) / s[:pos] ** 4)) return torch.diag(d) You can find the EVBMF code on my github page. I do not go into it in detail here. Jacob Gildenblatt’s code is a great resource for an in-depth look at this algorithm. ConclusionSo why is all this needed? The main reason is that we can reduce the number of operations needed to perform a convolution. This is particularly important in embedded systems where the number of operations is a hard constraint. The other reason is that we can reduce the number of parameters in a neural network, which can help with overfitting. The final reason is that we can reduce the amount of memory needed to store the neural network. This is particularly important in mobile devices where memory is a hard constraint.What does this mean mathematically? Fundamentally it means that neural networks are over parameterized i.e. they have far more parameters than the information that they represent. By reducing the rank of the matrices needed carry out a convolution, we are representing the same operation (as closely as possible) with a lot less information. References [Low Rank approximation of CNNs] (https://arxiv.org/pdf/1511.06067) [CP Decomposition] (https://arxiv.org/pdf/1412.6553) Kolda &amp; Bader “Tensor Decompositions and Applications”in SIAM REVIEW, 2009 [1] Nakajima, Shinichi, et al. “Global analytic solution of fully-observed variational Bayesian matrix factorization.” Journal of Machine Learning Research 14.Jan (2013): 1-37. [2] Nakajima, Shinichi, et al. “Perfect dimensionality recovery by variational Bayesian PCA.” [Python implementation of EVBMF] (https://github.com/CasvandenBogaard/VBMF) [Accelerating Deep Neural Networks with Tensor Decompositions - Jacob Gildenblat] (https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning) [Python Implementatioon of VBMF] (https://github.com/CasvandenBogaard/VBMF) [Similar article that is more high level] (https://medium.com/@anishhilary97/low-rank-approximation-for-4d-kernels-in-convolutional-neural-networks-through-svd-65b30dc55f6b)","link":"/2024/09/13/lora-3/"},{"title":"Fixed Points of Many to Many matching","text":"The Gale Shapley Algorithm (Deferred Acceptance Algorithm or DAA)The DAA algorithm for one to one matching is a stable matching algorithm that is guaranteed to find a stable matching for any set of preferences.While the one to one matching is well known, the many to many matching is not as well known.The many to many matching is a generalization of the one to one matching, where each agent can be matched to multiple agents. CodeBlair matching and DAA matching notebooks at:https://github.com/FranciscoRMendes/matching-algorithms Fixed Points of Many to Many matchingDefine the rejection functions Define the map This map is arecursive function on the underlying sets, like so .Example of iteration,Claim : The fixed points of are a stable matching. Worked Out Example Many to Many DAA MatchingWorkersFirmsStep 1:Step 2:Step 3:\\","link":"/2022/01/10/many-to-many-matching/"},{"title":"Part I :  Shrinking Neural Networks for Embedded Systems Using Low Rank Approximations (LoRA)","text":"MotivationAs someone who implements deep learning models on embedded systems, an important consideration is often the size of the model.There are several notions of size, but the two major ones are : Number of elements to save, usually in the form of a matrix or tensor. Number of operations to perform, usually in the form of a matrix multiplication.The first affects the amount of memory required to store the model, while the second affects the amount of computation required to run the model. I have worked extensively in matrix factorization before, mostly factorizing sparse matrices for recommendation systems.Unfortunately, while there are many LoRA walk throughs using code, I was not able to find a simple succint explanation of the problem and the solution. And why it works. This article aims to address that gap by providing an elementary explanation of the problem, how to set up the optimization problem and how to solve it, in possibly linear time.This is part I, that deals with factorizing a fully connected layer. Part II will deal with factorizing a convolutional layer. Code Follow AlongThe Jupyter Notebook for this is at https://github.com/FranciscoRMendes/tensor-rank Introduction to MatricesFirst we start with basic definitions of matrices and tensors.If you are reading this article you probably know what a matrix is, but here is one anyway. Again, consider a matrix multiplication with a vector, see addendum for a tensor multiplication. SizeGiven the matrix above the main focus of this article will be to reduce the matrix size so that we never actually store all 12 elements of the matrix. With that said here is a more precise definition of how much space the matrix above takes on hardware memory.In IEEE 754 single-precision format: 1 bit is used for the sign (positive or negative). 8 bits are used for the exponent. 23 bits are used for the significand (also called mantissa or fraction). The total of 32 bits (or 4 bytes) is used to represent a single floating-point number. So for this above matrix we need 12 memory locations or bytes. So if you were to save this matrix on a piece of hardware it would take up 4 bytes. The same applies for a tensor with 12 elements, which looks like this, While it is useful to think of tensors as a list of matrices, it is important to note that they have some important differences as mathematical objects. It is perhaps more useful to think of matrices as a “special case” of a tensor.For this introduction, we will stick to matrices. In a following article, I will build the equivalent intuition but for tensors. However, I will provide points of equivalence between the two, wherever possible. Number of OperationsFor the given operation, we’re performing a matrix multiplication of a 3×43×4 matrix with a 4×14×1 column vector. The resulting matrix will be a 3×13×1 column vector. To compute each element of the resulting vector, we perform the dot product of each row of the matrix with the column vector. Since each row has 4 elements, this involves 4 multiplications and 3 additions. Therefore, for the entire operation: There are 3 rows in the matrix. For each row, there are 4 multiplications and 3 additions. Hence, the total number of multiplications is 3×4=123×4=12, and the total number of additions is 3×3=93×3=9. So, in total, there are 12 multiplications and 9 additions involved in the matrix-vector multiplication operation. For two matrices being multiplied of dimensions and , there are multiplications. And the number of additions is . Notice that the number of multiplies is always greater than the number of sums. Neural Networks as sequences of matrix/ tensor operationsA Neural Network is simply a sequence of tensor operations. In this section we will outline a simple neural network and illustrate this concept.Input (3 nodes) –&gt; Hidden Layer (2 nodes, ReLU) –&gt; Output Layer (2 nodes, Sigmoid) Hidden layer: Output layer: We can combine these equations into one : In embedded systems its common to just feed an extra input of 1’s to the input and drop the biases. If you are familiar with matrix formulations of linear regression, you are probably familiar with this, but if not, you can see this clearly by the following, Thus, after adding in the 1s to X and the column of biases to we get, This X and W are modified versions of the X, W mentioned before but we will stick to this notation for the rest of this article. You can generalize this to as many layers as you want. Computational Complexity and SizeRecall that we already defined how much memory is occupied by a matrix. So our matrix requires 6 x 4bytes of memory. For simplicity I will only refer to the number of elements i.e. 6.To give you a sense of how big this can be consider an input of 8192 FFT coefficients, and a second layer of size , . On embedded systems everything is always a power of two (I wonder why). Number of multiplies is number of additions (see above).For a signal/ image processing problem, usuallySince our input data is usually either images or signals of the order of etc but the classes are usually several orders of magnitude smaller usually but at most . This is usually a very small matrix. Simpler Problem Statement of Size ReductionLet us start by trying to find a smaller matrix , that does the job of the bigger matrix. In more formal terms this means finding a matrix such that but where is smaller in some sense than .Fortunately this is a very well defined problem in mathematics and can be solved by taking an SVD of and choosing the largest singular values, where (usually) where are the dimensions of .The more perceptive of you will have noticed that we have two options to replace the matrix , we can use the fully multiplied out version or the components . SizeFirst, let’s analyze the two in terms of size. The size of is the same as . What about the size of ?Where are the dimensions of and is how many singular values were chosen.How much space did we save? Recall, We can in fact, place an upper bound on for an SVD to save space. Where usually this upper bound is never tight. In our example, this value is , Usually, we aim to reduce this number by 50% or more i.e. a rank of around 64. But in many ofour use cases we see ranks of the order etc. as being enough. MultiplicationIf the concept of size is clear, then it immediately becomes clear why the number ofmultiplications also reduces.When multiplying by , where has dimensions and has dimensions , each element of the resulting matrix is obtained by taking the dot product of a row from with a column from . Since has dimensions and has dimensions , each dot product involves multiplications. Therefore, the total number of multiplications for is . Formulating The Indirect Optimization ProblemOkay so we know what to do, we need to find an that keeps the original activations as close to the original value as possible. Lets say the original activations wereI will explain later what the “indirect” means.The new activations are For a given metric, of distance and a given tolerance between the two activations, we have the following optimization problem, Minimize subject to the distance between the activation values being low. Obviously the maximum distance will be when , is an aggressive low rank (=1) approximation of . And minimal distance will be when . However, it is very hard to define a good delta, since even if is low it is possible that that value gets amplified for the rest of the network. To combat this we can reformulate the problem in terms of the final prediction by plugging in to the original network leaving all the other layers unchanged. Thus we can optimize our final prediction directly. Using the equation for an arbitrary number of layers , Now we can use any classification metric, on the real data to measure our performance. Thus, we need to find the smallest such that the difference in error is below the tolerance level. Note, 2 things The problem is easily interpret-able now, find the smallest s.t. the difference in accuracy is at most 0.1. Second, we is generally much larger than every other matrix in the problem, so while you could run more complex optimizations involving combinations of and finding the ranks, for each one that maintains a certain difference in accuracy. But in practice this is not necessary since the overall space savings for all the matrices are heavily dominated by the size of the first matrix. In other words, if the largest matrix is reduced in size by 70 percent and every other one is reduced in size by 98 percent, the overall space savings is still 70 percent. Optimization AlgorithmOne obvious way to do this would be a brute force approach, that is, to simply start at and choose smaller and smaller ranks and check if the constraint is satisfied. But for most of my use cases, this turned out to be too long. In our example it would mean trying all the way from to . Interestingly, where you choose to start at or at depends on where you expect to find a rank that is close enough to your desired accuracy.There is an easier way to achieve this, but the proof is left as an exercise for the reader. Hint : study the properties of the optimizationalgorithm at and and see if you can find where your next choice of should be. Formulating the Direct Optimization ProblemThe indirect optimization problem is a purely mathematical problem that does not “know” that the matrix comes from a neural network.Now we know that the matrix comes from a neural network, we can use this information to our advantage.Recall, the general formulation of a neural network, We can use the fact that the matrix comes from a neural network to our advantage. We can use the loss function of the neural network to directly optimize the matrices .Here, we must be careful to change the notation slightly. We will denote the matrices as to indicate that they are the matrices that come from the first layer of the neural network.But these are not necessarily the matrices that come from the SVD of i.e. when we multiply them together we may not get .The reason for this is that the neural network learns the best that minimize the loss function of the neural network, not the best that minimize the distance between the activations of the original and the new matrix.This is a subtle but important distinction. This is the approach that LoRA uses. LoRA optimization, where U,S and V are defined for a suitable choice of , is a direct optimization problem. However, there are some important disadvantages to this approach. Let us start with the advantages, Easy to understand, we want low rank matrices instead of our first layer, so why not start with them and optimize them directly. The optimization problem is now a neural network optimization problem, which is a well studied field. Training is much faster than if you trained on a full rank matrix in the first layer. This is important if you want to run many trials with other hyperparameters. Binary search as known issues.Disadvantages, You need to know the rank of the matrix you are looking for. This is not always easy to know. If you want to find the rank you need to run the optimization problem for every rank you want to try.And this optimization problem is more expensive than the one above. The optimization problem is non-convex, so you may not find the global minimum for the delta you want. Randomness between runs could result in a different weight matrix every time. ConclusionFor us a neat work around was to get a sense of a usable rank by running the indirect optimization problem and then using that rank to run the direct optimization problem. This way we could get a good rank in a reasonable amount of time.Let us recap some of the important concepts we have learned in this article. The size of a matrix is the number of elements it has. The number of operations in a matrix multiplication is the number of multiplications and additions involved. The rank of a matrix is the number of linearly independent rows or columns it has. The SVD of a matrix is a way to factorize it into three matrices. The low rank approximation of a matrix is a way to approximate it with a smaller matrix. The low rank approximation of a matrix can be found by taking the SVD of the matrix and choosing the largest singular values. There are two ways to find the low rank approximation of a matrix: the indirect optimization problem and the direct optimization problem. References LoRA: Low-Rank Adaptation of Large Language Models Low Rank Approximation of a Matrix Singular Value Decomposition Matrix Multiplication AddendumTensor multiplication is a generalization of matrix multiplication. In tensor multiplication, the order of the tensors matters. For example, the product of a 3×4×2 tensor with a 2×4×1 tensor is a 3×4×1 tensor. The number of multiplications and additions involved in tensor multiplication is the same as in matrix multiplication, but the dimensions of the tensors are different. For example, the product of a 3×4×2 tensor with a 2×4×1 tensor involves 3×4×2=243×4×2=24 multiplications and 3×3=93×3=9 additions.Example of tensor multiplication of two tensors and , for ease of exposition is actually an identity matrix. Tensor : Tensor : Resulting Tensor : The calculations are as follows: $$ \\begin{align*}C[0,0,0] &amp;= 11 + 20 = 1 + 0 = 1 \\C[0,0,1] &amp;= 10 + 21 = 0 + 2 = 2 \\C[0,1,0] &amp;= 31 + 40 = 3 + 0 = 3 \\C[0,1,1] &amp;= 30 + 41 = 0 + 4 = 4 \\C[1,0,0] &amp;= 51 + 60 = 5 + 0 = 5 \\C[1,0,1] &amp;= 50 + 61 = 0 + 6 = 6 \\C[1,1,0] &amp;= 71 + 80 = 7 + 0 = 7 \\C[1,1,1] &amp;= 70 + 81 = 0 + 8 = 8 \\\\end{align*} $$","link":"/2024/04/03/lora/"},{"title":"Matching MATLAB&#39;s resample function in Python","text":"Matching MATLAB’s resample function in PythonIt is rather annoying that a fast implementation of MATLAB’s resample function does not exist in Python with minimal theoretical knowledge of signal processing. This post aims to provide a simple implementation of MATLAB’s resample function in Python. With, you guessed it, zero context and therefore no theoretical knowledge of signal processing. The function ha been tested against MATLAB’s resample function using a simple example. I might include that later. I had originally answered this on StackExchange, but it is lost because the question was deleted. 1234567891011121314151617181920212223242526272829303132import numpy as npfrom scipy.signal import resample_polyfrom math import gcddef matlab_resample(x, resample_rate, orig_sample_rate): \"\"\" Resample a signal by a rational factor (p/q) to match MATLAB's `resample` function. Parameters: x (array-like): Input signal. p (int): Upsampling factor. q (int): Downsampling factor. Returns: array-like: Resampled signal. \"\"\" p = resample_rate q = orig_sample_rate factor_gcd = gcd(int(p), int(q)) p = int(p // factor_gcd) q = int(q // factor_gcd) # Ensure input is a numpy array x = np.asarray(x) # Use resample_poly to perform efficient polyphase filtering y = resample_poly(x, p, q, window=('kaiser', 5.0)) # Match MATLAB's output length behavior output_length = int(np.ceil(len(x) * p / q)) y = y[:output_length] return y","link":"/2024/12/17/matching-matlabs-resample/"},{"title":"On Comparing Newtonian and Einsteinian Gravity using Mars&#39; Orbit","text":"IntroductionLike most people I was taught the field equations for Newtonian gravity in high school in order to prepare for the national selection exams. I went on to study mathematics and differential equations and learned more field equations for voltage etc. Since then, I have always read, mostly in popular science books, that the Einstein field equations are ‘’better’’ than Newton’s. Popular science books are great for getting a general idea of what very complex concepts but all too often they end up being a collection of mismatched metaphors and comparisons (think of Einstein’s rubber sheet preceded by Newton’s Apple) that often leave one wondering what mathematical framework could possibly juxtapose the two. As always, I try to create a minimum working example that would help me (you, us) understand the problem better. In this post I will compare the two theories using the example of Mars’ orbit.Before we begin, this essay was never meant to be an exhaustive explanation of the math, rather it was meant to be a reasonably sophisticated roadmap that would allow one to see an equation in action and then intelligently Google it’s origins. With that end, starting from the Schwarzschild metric is a good point to start from, since you only need differential equations from that point on to solve things. I have skipped several steps from the Schwarzschild metric to the final differential equation, but most of those steps are just chugging through the math, they do not really help you understand the problem. Basically, my ‘recipe’ is as follows : you have a system of equations (Einstein’s field equations) you have a solution to that system (Schwarzschild metric) that makes some simplifying assumptions on the constants you plug in values for the constants and solve the differential equations for your system (Mars’ orbit) You can use this recipe for any field equations, not just gravity. And indeed for any other field theory of physics. General RelativityStarting from the first principles of Newtonian gravity one can solve field equations for gravity that yield the following solution Where is the distance of a planet from a star. The general solution looks like . From which you can generate perfectly elliptical orbits as well as those that are not. Recall that Newton did not provide a reason for why masses attracted each other. Einstein proposed that mass warped space-time and this caused gravity as a secondary effect, which is why his field equations are not directly comparable to Newton’s but in summary one can choose solutions based on some symmetrical conditions, since we are looking at a large spherical star we can choose the Scwarzchild metric, All that finally gives us an equation like the one for Newton, Which can be written as , For comparison here is Newton’s equation, Orbits are perfectly elliptical in the first case i.e. every year you would expect Mars to be in the same place. But Einstein’s equations give a spiralling orbit such that every year Mars ends up moved or slightly precessed. It is exactly this phenomenon, called the precession of Mars, that proves that Einstein’s equations are considered a better explanation for gravity than Newton’s. ConclusionWell, that is all there is to it. Reasonable next steps for the interested reader would involve some combination of the following : Understand the origins of Einstein’s field equations (this can get quite philosophical) Understand the Schwarzschild metric and how it is derived as a solution to the field equations Understand the precession of Mars and how it is used to validate Einstein’s field equations via the error in the orbit of Mars as predicted by Newton’s equations (extra credit) Understand the Schwarzschild metric in the context of the Kerr metric (rotating black holes) and the Reissner-Nordstrom metric (charged black holes) Addendum : What does “choosing a solution” to the Einstein metric mean?Infuriatingly, often one hears of Einsteins equations but no one really ever shows you what they are. Here is a brief overview of the primary concepts. Every equation is a tensor equation, which means that it is a set of equations that are invariant under a change of coordinates. You can find them in the non tensor form too, no you dont need to know what tensors are to understand them. Einstein’s field equations are a set of ten interrelated differential equations in Albert Einstein’s general theory of relativity. They describe how mass and energy in the universe influence the curvature of space-time, which in turn affects the motion of objects. The equations can be summarized as (in tensor notation) Here, represents the Einstein tensor, which describes the curvature of space-time, and is the stress-energy tensor, representing the distribution of mass, energy, and momentum in space-time. The equations essentially state that the curvature of space-time(left-hand side) is directly related to the energy and momentum of matter and radiation (right-hand side). This formulation has profound implications for our understanding of gravity, leading to predictions such as the bending of light by massive objects, the slowing of time in gravitational fields, and the existence of black holes. Choosing a solution to Einstein’s field equations involves finding solutions that satisfy both the mathematical requirements of the equations and physical constraints. The Schwarzschild metric is one such solution that describes the geometry of space-time outside a spherically symmetric, non-rotating mass, such as a black hole or a massive celestial body (literally 90% of the solutions you ever need are this case, charged black holes are still an open problem in almost all modern equations, other metrics exist but for the most part you only need this one to get an idea). The Schwarzschild metric is given by: Where: - is the spacetime interval, - is the time interval, - is the radial interval, - is the solid angle element, - is the gravitational constant, - is the mass of the central object, - is the speed of light, and - is the radial coordinate. The Schwarzschild metric satisfies Einstein’s field equations for vacuum solutions (i.e., regions of space-time where there is no matter or energy), which means . The RHS of the equation above goes to zero. This solution describes the curvature of space-time around a spherically symmetric mass. It exhibits features such as gravitational time dilation, gravitational redshift, and the existence of an event horizon for , beyond which no information can escape (defining the boundary of a black hole). The Schwarzschild metric was initially derived for the case of a non-rotating, uncharged mass. However, it serves as a fundamental solution in general relativity and has been foundational in understanding the behavior of space-time around massive objects. References Schwarzschild metric Einstein field equations General relativity Schwarzschild black hole Excellent channel : https://www.youtube.com/@ScienceClicEN Derive the Einstein tensor : https://www.youtube.com/watch?v=4g1xZNKw2cc BUT WHAT IS A TENSOR : https://www.youtube.com/watch?v=Hf-BxbtCg_A Difference in orbitals, full proof : https://nhsjs.com/2023/the-difference-in-orbits-of-planets-in-terms-of-newtonian-mechanics-and-einsteinian-mechanics/","link":"/2023/11/03/newton-einstein-mars-orbit-comparison/"},{"title":"On The Class Struggle.","text":"My father studied war, sailing (the rough kind) and carpentry that I may study mathematics and politics that my children may study art and literature that their children (my grandchildren) might one day move to Vancouver to study peace, sailing (the genteel kind) and carpentry (is there a kind?). They say the Soviet class struggle is lost, and so is ours, but I say my father fought to a draw. A bloody draw, an armistice that I must protect at all costs. We fought for (upward) social mobility and our reward was the downward social mobility of the intelligentsia (of which I am now part). There was a time when you could tell the bourgeoisie by their little (in more sense than one) badges of honor. LV, Ralph Lauren and so on. But I just walked past a boy driving an old car with combat trousers that had to have been from Vietnam and a badge on his car which said Grinell College and Math club. The bourgeoisie now have more clandestine badges, veganism, Peter Attia and a very bright young child who might attend an East Coast school one day. The operario (proletariat) save to buy jeans on Michigan Avenue. My high school physics teacher who would ask us to count the leaves on a tree when we asked him a difficult question. We always hoped he had a clever answer at the end. But he always said, “No. that’s it. None of this means anything. Just like 382 leaves do not mean anything”. So maybe the class struggle is over.","link":"/2024/08/01/on-the-class-struggle/"},{"title":"A Manual Implementation of Quantization in PyTorch - Single Layer","text":"IntroductionThe packaging of extremely complex techniques inside convenient wrappers in PyTorch often makes quick implementations fairly easy, it also removes the need to understand the inner workings of the code. However, this obfuscates the theory of why such things work and why they are important to us. For instance, for neither love or money, could I figure out what a QuantStub and a DeQuant Stub really do and how to replicate that using pen and paper. In embedded systems one often has to code up certain things from scratch, as it were and sometimes PyTorch’s “convenience” can be a major impediment to understanding the underlying theory. In the code below, I will show you how to quantize a single layer of a neural network using PyTorch. And explain each step in excruciating detail. At the end of this article you will be able to implement quantization in PyTorch (or indeed any other library) but crucially, you will be able to do it without using any quantize layers, you can essentially use the usual “vanilla” layers. But before that we need to understand how or why quantization is important. QuantizationThe process of quantization is the process of reducing the number of bits that represent a number. This usually means we want to use an integer instead of a real number, that is, you want to go from using a floating point number to an integer. It is important to note that the reason for this is because of the way we multiply numbers in embedded systems. This has to do with both the physics and the chemistry of a half-adder and a full adder. It just takes longer to multiply two floats together than it does to multiply two integers together. For instance, multiplying is a much more complex operation than multiplying . So it is not simply a consequence of reducing the “size” of the number. In the future, I will write a blog post about why physics has a lot to do with why this is. OutlineI start with the intuition behind Quantization using a helpful example. And then I outline a manual implementation of quantization in PyTorch. So what exactly does “manual” mean? I will take a given, assumed pre-trained, PyTorch model (1 Fully connected layer with no bias) that has been quantized using PyTorch’s quantization API. I will extract the weights of the layer and quantize them manually using the scale and zero point from the PyTorch quantization API. I will quantize the input to the layer manually, using the same scale and zero point as the PyTorch quantization API. I will construct a “vanilla” fully connected layer (as opposed to the quantized layer in step 1) and multiply the quantized weights and input to get the output. I will compare the output of the quantized layer from step 1 with the output of the “vanilla” layer from step 4. This will inherently allow you to understand the following : How to quantize a layer in PyTorch and what quantizing in PyTorch really means. Some potentially confusing issues about what is being quantized, how and why. What does the QuantStub and DeQuantStub really do and how to replicate that using pen and paper. At the end of this article you should be able to : Understand Quantization conceptually. Understand PyTorch’s quantization API. Implement quantization manually in PyTorch. Implement a Quantized Neural Network in PyTorch without using PyTorch’s quantization API. Intuition behind QuantizationThe best way to think about quantization is to think of it through an example. Let’s say you own a store, and you are printing labels for the prices of objects, but you want to economize on the number of labels you print. Assume here for simplicity that you can print a label that shows a price lower than the price of the product but not more. If you print tags for 0.20 cents, you get the following table, which shows a loss of 0.97 by printing 6 labels. This obviously didn’t save you much as you might as well have printed labels with the original prices and lost in sales. Price Tags Loss 1.99 1.8 -0.19 2.00 2 0.00 0.59 0.4 -0.19 12.30 12 -0.30 8.50 8.4 -0.10 8.99 8.8 -0.19 6 -0.97 Maybe we can be more aggressive, by choosing tags rounded to the nearest dollar instead, we can obviously lose more money but we save on one whole tag! Price Tags Loss 1.99 1 -0.99 2.00 2 0.00 0.59 0 -0.59 12.30 12 -0.30 8.50 8 -0.50 8.99 8 -0.99 5 -3.37 How about an even more aggressive one? We round to the nearest dollars and use just two tags. But then we are stuck with a massive loss of dollars. Price Tags Loss 1.99 0 -1.99 2.00 0 -2.00 0.59 0 -0.59 12.30 10 -2.30 8.50 0 -8.50 8.99 0 -8.99 2 -24.37 In this example, the price tags represent memory units and each price tag printed costs a certain amount of memory. Obviously, printing as many price tags as there are goods results in no loss of money but also the worst possible outcome as far as memory is concerned. Going the other way reducing the number of tags results in the largest loss in money. Quantization as an (Unbounded) Optimization ProblemClearly, this calls for an optimization problem, so we can set up the following one : let be the quantization function , then the loss is as follows, Where is a count of the unique values that over the entire interval of, . Issues with finding a solutionA popular assumption is to assume that the function is a rounding of a linear transformation. The constraint that minimizes is difficult since the function is unbounded. We could solve this if we knew at least two points at which we knew the expected output for the quantization problem, but we do not, since there is no bound on the highest tag we can print. If we could impose a bound on the problem, we could evaluate the function at the two bounds and solve it. Thus setting a bound seems to solve both problems. Quantization as Bounded Optimization ProblemIn the previous section, our goal was to reduce the number of price tags we print, but it was not a bounded problem. In your average grocery story prices could run between dollars and a dollars. Using the scheme above you could certainly print fewer labels. But you could also end up printing a large number of labels in absolute terms. You could do one better by pre-determining the number of labels you want to print. Let us then, set some bounds on the number of labels we want to print, consider the labels you want to print as , this is fairly aggressive. Again we can set up the optimization problem as follows (there is no need to minimze , the count of unique labels for now, since we are defining that ourselves), where is the scale and is the zero point. It must be true that, Evaluating the above equations gives us the general solution This gives us the solution, . Price Label Loss 1.99 0 -1.99 2 0 -2 0.59 -1 -1.59 12.3 2 -10.3 8.5 1 -7.5 8.99 1 -7.99 4 -31.37 This gives the oft quoted quantization formula, Similarly, we get reverse the formula to get the dequantization formula i.e. starting from a quantized value we can guess what the original value must have been, This is obviously lossy. Implication of QuantizationWe have shown that given some prices, we can quantize them to a smaller set of labels. Thus saving on the cost of labels. What if you remembered and and then you used the dequantization formula to guess what the original price was and charge the customer that amount? This way you can save on the number of labels, but you can get closer to the original price by just writing down and and using the dequantization formula. We can actually do a better job with prices as well as saving on the number of labels. However, this is lossy, and you will lose some money. In this example, we notice that we consider charging more or less than the actual price as a loss both ways, to keep things simple. Price Label Loss DeQuant De-q loss 1.99 0 1.99 3.90 1.91 2.00 0 2.00 3.90 1.90 0.59 -1 1.59 0.00 0.59 12.30 2 10.3 11.71 0.59 8.50 1 7.50 7.80 0.69 8.99 1 7.99 7.80 1.18 4 31.37 6.87 Quantization of Matrix MultiplicationUsing this we can create a recipe for quantization to help us in the case of neural networks. Recall that the basic unit of a neural network is the operation, We can apply quantization to the weights and the input (). We can then use dequantization to get the output. Our goal of trying to avoid the floating point multiplication between can now be achieved by replacing them with their respective quantized values and scaling and subtracting the zero point to get the final output. Here, and are quantized matrices and thus the multiplication operation (after multiplying it out) is now not between two floating point matrices and but between and . Which are both integer matrices. This allows us to save on memory and computation since it is cheaper to multiply integers together than it is to multiple floats. However, in practice since, are also integers, is also an integer multiplication, so we just use that mulitplication instead of multiplying out the whole thing. CodeConsider the following original, 123456789101112131415161718class M(torch.nn.Module): def __init__(self): super(M, self).__init__() # QuantStub converts tensors from floating point to quantized self.quant = torch.quantization.QuantStub() self.fc = torch.nn.Linear(2, 2, bias=False) # DeQuantStub converts tensors from quantized to floating point self.dequant = torch.quantization.DeQuantStub() def forward(self, X): # manually specify where tensors will be converted from floating # point to quantized in the quantized model X = self.quant(X) x = self.fc(X) # [[124., 36.] # manually specify where tensors will be converted from quantized # to floating point in the quantized model x = self.dequant(x) return x Now consider, the manual quantization of the weights and the input. model_int8 represents the quantized model. The QuantM2 class is the manual quantization of the model. The prepare_model function uses PyTorch convenience functions for quantization of the weights and the input i.e. get , from this model and compute the other steps. You can calculate these yourself as well, using the distributions of the input data and activation functions. The quantize_tensor_unsigned function is the manual quantization of the input tensor. The pytorch_result function is that computes the output of the fully connected layer of the PyTorch quantized model. The forward function is the manual quantization of the forward pass of the model. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def prepare_model(model_fp32, input_fp32): # model must be set to eval mode for static quantization logic to work model_fp32.eval() model_fp32.qconfig = torch.quantization.QConfig( activation=MinMaxObserver.with_args(dtype=torch.quint8), weight=MinMaxObserver.with_args(dtype=torch.qint8) ) # Prepare the model for static quantization. This inserts observers in # the model that will observe activation tensors during calibration. model_fp32_prepared = torch.quantization.prepare(model_fp32) model_fp32_prepared(input_fp32) model_int8 = torch.quantization.convert(model_fp32_prepared) return model_int8def quantize_tensor_unsigned(x, scale, zero_point, num_bits=8): # This function mocks the PyTorch QuantStub function which quantizes the input tensor qmin = 0. qmax = 2. ** num_bits - 1. q_x = zero_point + x / scale q_x.clamp_(qmin, qmax).round_() q_x = q_x.round().byte() return QTensor(tensor=q_x, scale=scale, zero_point=zero_point) class QuantM2(torch.nn.Module): def __init__(self, model_fp32, input_fp32): super(QuantM2, self).__init__() self.fc = torch.nn.Linear(2, 2, bias=False) self.model_int8 = prepare_model(model_fp32, input_fp32) # PyTorch automatically quantizes the model for you, we will use those weights to compute a forward pass W_q = self.model_int8.fc.weight().int_repr().double() z_w = self.model_int8.fc.weight().q_zero_point() self.fc.weight.data = (W_q - z_w) @staticmethod def pytorch_result(model_int8, input_fp32): pytorch_res = model_int8.fc(model_int8.quant(input_fp32)).int_repr().float() return pytorch_res def forward(self, x): input_fp32 = x s_x = self.model_int8.quant(input_fp32).q_scale() z_x = self.model_int8.quant(input_fp32).q_zero_point() quant_input_unsigned = quantize_tensor_unsigned(input_fp32, s_x,z_x) z_x = quant_input_unsigned.zero_point s_x = quant_input_unsigned.scale s_w = self.model_int8.fc.weight().q_scale() x1 = self.fc(quant_input_unsigned.tensor.double() - z_x) # this next step is equivalent to dequantizing the output of the fully connected layer # it not exactly equivalent since I already subtracted the two zero points # you can derive a much longer quantization formula that multiplies W_q * X_q and has additional terms # you can then put W_q in the fc layer and X_q in the forward pass # and then use all those additional terms in the below step to requantize # in embedded systems its easy to use the formulation here x1 = x1 * (s_x * s_w) return x1 Sample run code of the above code is as follows, 123456789cal_dat = torch.randn(1, 2)model = M()# graph mode implementationsample_data = torch.randn(1, 2)model(sample_data)quant_model= QuantM2(model_fp32=model, input_fp32=sample_data)quant_model(sample_data)quant_model.model_int8(sample_data) # this is the quantized model, M2 should match it exactly, M is the original non quantized model. For small data sets there is usually no divergence.# but in practice, the quantized model will be faster and use less memory, but will lose some accuracy Let us start by analyzing the output of a quant layer of our simple model. The output of the int_models quantized layer is (somewhat counter-intuitively) always a float, this does not mean it is not quantized, it simply means you are shown the non-quantized value. If you look at the output, you will notice, it has dtype, quantization_scheme, scale and zero_point. You can view the value that will actually be used when it is called within the context of a quant layer by calling its int representation. 123456789101112#recreate quant layerint_model = quant_model.model_int8default_pytorch_quant_layer_output = int_model.quant(sample_data)# OUTPUT# tensor([[0.1916, 0.5428]], size=(1, 2), dtype=torch.quint8,# quantization_scheme=torch.per_tensor_affine, scale=0.0021285151597112417,# zero_point=0)actual_pytorch_quant_layer_output = int_model.quant(sample_data).int_repr()# OUTPUT# tensor([[90, 255]], dtype=torch.uint8) Our manual quantization layer is a bit different, it outputs a QTensor object, which contains the tensor, the scale and the zero point. We get the scale and the zero point from the PyTorch quantized model’s quant layer (again, we could easily have done this by ourselves using the sample data). 123manual_quant_layer_output = quantize_tensor_unsigned(sample_data, int_model.quant(sample_data).q_scale(), int_model.quant(sample_data).q_zero_point())# OUTPUT# QTensor(tensor=tensor([[ 90, 255]], dtype=torch.uint8), scale=0.0021285151597112417, zero_point=0) Now let us look at the output of the quant layer AND the fully connected layer. 12345#recreate the fully connected layer operationpytorch_fc_quant_layer_output = int_model.dequant(int_model.fc(int_model.quant(sample_data)))# tensor([[-0.7907, 0.6919]])manual_fc_quant_layer_output = quant_model(sample_data)# tensor([[-0.7886, 0.6932]], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;) It is worthwhile to point out a few things. First, the following two commands seem to give the same values but are very different. The first is a complete tensor object that gives float values but is actually quantized, look at dtype, it is actually quint.8. 1234int_model.fc(int_model.quant(sample_data))# tensor([[-0.7907, 0.6919]], size=(1, 2), dtype=torch.quint8,# quantization_scheme=torch.per_tensor_affine, scale=0.0058139534667134285,# zero_point=136) The output of this is a truly a float tensor, it not only shows as float values (same as before) but contains no quantization information. 12int_model.dequant(int_model.fc(int_model.quant(sample_data))) # tensor([[-0.7907, 0.6919]]) Thus, in order to recreate a quantization operation from PyTorch in any embedded system you do not need to implement a de-quant layer. You can simply multiply and subtract zero points from your weight layers appropriately. Look for the long note inside the forward pass of the manually quantized model for more information. A Word on PyTorch and QuantizationPyTorch’s display in the console is not always indicative of what is happening in the back end, this section should clear up some questions, you may have (since I had them). The fundamental unit of data that goes between layers in PyTorch is always a Tensor, that is always displayed as a float. This is fairly confusing since when we think of a vector/tensor as quantized we see all the data as integers. But PyTorch works differently, when a tensor is quantized it is still displayed as a float, but its quantized data type and quantization scheme to get to that data type is stored as additional attributes to the tensor object. Thus, do not be confused if you still see float values displayed, you must look at the dtype to get a clear understanding of what the values are. In order to view a quantized tensor as a int, you need to call int_repr() on the tensor object. Note, this throws an error if the tensor has not been quantized in the first place. Also, note that when PyTorch encounters a quantized tensor, it will carry out multiplication on the quantized values automatically and thus the benefits of quantization will be realized even if you do not actually see them. When exporting the model this information is packaged as well, no need for anything extra to be done. A Word on Quant and DeQuant StubsThis is perhaps the most confusing of all things about quantization in PyTorch, the QuantStub and DeQuantStub. The job of de-quantizing something is automatically taken care of by the previous layer, as mentioned above. Thus when you come to a DeQuant Layer all it seems to do is just strip away the memory of having ever been quantized and ensures that the floating point representation is used. That is what is meant by the statement “DeQuantStub is stateless”, it literally needs nothing to function, all the information it needs to function will be packaged with the input tensor you feed into it. The Quant Stub, on the other hand, is stateful it needs to know the scale and the zero point of what is being fed into it, and the network has no knowledge of the input data, which is why you need to feed data into the Neural Network to get this going, if you knew the scale and zero point of your data already you could directly input that information into the QuantStub. The QuantStub and DeQuantStub are not actually layers, they are just functions that are called when the model is quantized. Another huge misconception is when and where to call these layers, every example on the PyTorch repo will have the Quant and DeQuant stub sandwiching the entire network, this leads people to think that the entire network is quantized. This is not true see the following section for more information. Do you need to insert a Quant and DeQuant Stub after every layer in your model?Unless you know exactly what you are doing, then YES you do. In most cases, especially for first time users, you usually want to dequantize immediately after quantizing. If you want to “quantize” every multiplication operation but dequantize the result (i.e. try to bring it back to your original scale of data) then yes, you do. The Quant and DeQuant Stub is “dumb” in the sense that it does not know what the previous layer was, if you feed it a quantized tensor it dequantizes it. It has no view of your network as a whole and does not modify the behavior of the network as a whole. Recall the mathematics of what we are trying to do. We want to replace a matrix multiplication, with . Now what if you want to replace this across multiple layers i.e. you want to quantize the following expression : Your first layer weights are and the second layer weights are . You want to quantize the entire expression. Ask yourself what do you really want to do, in most cases what you really want to do, is quantize the two matrix multiplies, you inevitably have to do, so that they do not occur in float representation but rather occur in integer. This means you want to replace with and thus replacing the whole expression with . If you do not dequantize after every layer you will end up executing the following equation, as the entire first layer will be quantized, its output will be recorded and then that quantized value in int8 will flow to the next layer. After this, all the quantization information will be lost i.e. the scale and zero point will be lost. The DeQuant layer will simply use the information from the previous layer to dequantize the output, so only the most recent layers output will be dequantized. When do you not need to put Quant and DeQuant Stubs after every layer?Dequantizing comes with a cost, you need to compute floating point multiplications, in order to multiply the weights with the matrix. This is certainly less than the floating point operations from the original matrix multiplication itself (a lot less than storing the output from another whole floating point matrix), but it is still a lot. However, in many of my use cases, I could get away with not dequantizing. While the real reasons are still not clear to me (like most things in neural networks), I would guess that for some of my layers the weights were not that important to getting my overall accuracy. I was also in the Quantize Aware Framework, maybe I will do a post about this too. ConclusionIn this blog post we covered some important details about PyTorch’s implementation of quantization that are not immediately obvious. We then went on to manually implement a quantized layer and a quantized model. We then showed how to use the quantized layer and the quantized model to get the same results as the PyTorch quantized model. We also showed that the PyTorch quantized model is not actually quantized in the sense that the values are integers, but that the values are quantized in the sense that the values are stored as tensor objects (that store their quantization parameters with them) and the operations are carried out on the integers. This is a very important distinction to make. Additionally, in inference mode, you can just take out the quantized weights, and skip the fc layer step as well, you can just multiply the two matrices together. This is what I will be doing in the embedded system case. In my next posts, I will show you how to quantize a model and the physics behind why multiplying two floats is more expensive than multiplying a two integers.","link":"/2024/05/16/quantization-1/"},{"title":"Are Values Passed Between Layers Float or Int in PyTorch Post Quantization?","text":"IntroductionA common question I had when I first started working with quantization was, how exactly are values passed between layers post quantization? Looking at the quantization equations it was not immediately obvious to me how to do away with ALL floating point operations and what values were used exactly in the forward pass operation.In this article, I will address two main questions, How does PyTorch pass values between layers post quantization? And what are the quantization equations used. What exactly makes floating point operations slower than integer operations? In short, PyTorch passes integer values between layers when in INT8 only quantization. It does so using a few neat tricks of fixed point and floating point math. When in other quantization modes, values passed between layers can be float. Quantized Matrix MultiplicationThe overall goal of quantization is to make this multiplication operation simpler in some sense, There are two ways to make this simpler, Carry out the multiplication operation in integers, as of now are floats save as an integers We can achieve both ends by, replacing by adding subtracting terms to get back the original value We can use the well known quantization scheme to carry out the quantization for each of these objects. As a recap, they are outlined again below. Personally, I just took these functions as given and “magically” converted between integers and floats, it is not quite that important to understand their inner workings at this stage. Using the quantization scheme, and de-quantization scheme, We can write the above multiplication as, Notice that now, instead of multiplying we have an expression where we eventually we get the integer multiply . In fact, is also an integer multiply so we can leave it at that stage instead. Notice that is still float, thus, Thus, we can write, Notice that, each of the matrix multiplies are integers, but the preceding coefficients are floats. This is a problem, since if our embedded system only supports integer multiplies we will not be able to do this operation. Notice also that bias’ quantization prevents us from efficiently doing the operation in one step without a multiply in between. We will solve this second issue first, Notice that instead of choosing the scale and range “correctly\" we can choose them somewhat arbitrarily as long as they work. In particular we can choose to quantize such that and . This usually under-quantizes the bias, but its good for two reasons, Bias tends to be more important for accuracy than weights do, so it is in fact better if its higher in accuracy Even though they are bigger than they need to be they account for a fraction of the parameters of the neural network. For the first issue, we use a pretty neat trick. Remember that is constant and we know it at the time of compiling, so we can consider it to be a fixed point operation, we can write it as where is always a fixed number determined at the time of compilation (this is not true for floating point). Thus the entire expression, can be carried out with integer arithmetic and all values exchanged between layers are integer values. Data Types Passed Between LayersUsing the matrix multiplication example before, are the weights and biases of one fully connected layer. A question I often had was, how exactly are values passed between layers. This is because FULL INT8 quantization essentially means you can deploy a neural network on a board that does not support ANY floating point operations. It is in fact the that is passed between layers when you do INT8 quantization. However, if you just need the weights and the multiplies to be quantized but not the activations, it means that you are getting the benefits of quantization for saving space of the weights and by using integer multiply BUT are choosing to pass values between the layers as floats. For this case, PyTorch and Keras can also spit out the floating point values, to be passed between layers, and it does this by simply omitting the de-quantization step. Here again, we can choose but I am not sure if this additional assumption is needed, since the board has the ability to do floating point multiplications it does not matter if one or more float multiplies are needed. To summarize, For full INT8 quantization i.e. when the embedded device does not support any floating point multiplies use For partial INT8 quantization i.e. you want the activations to be in float but weights and integer multiplies to be done in/ saved as INT8 use the equation for . Why Exactly Does Floating Point Slow Things Down?Another paint point for me was the lack of reasoning as to why multiplying two floating point numbers together takes longer/ is more difficult than multiplying two INTs together. The reason has to do with physics, and we will come to it in a minute. For now let us consider two floating point numbers and their resulting multiplication. Recall, that a floating point number is always of the form . Where the leading is compulsory, if any calculation results in values such as , you need to divide the mantissa by and keep adding it to the exponent. Additionally, means that you can only store values past the radix point (general word for what the point ‘’ is binary system). Consider, and Add, the exponents Multiply, the mantissas Re-normalize, by dividing by , exponent is now , mantissa is 1.00011 Sign, here both numbers are positive so the sign bit is Truncate, As you can see, multiplying two floating point numbers takes quite a bit of steps. In particular, re-normalization could potentially take multiple steps. For contrast, consider a fixed point multiplication, and . In this case is forced to use , so in memory this step is simply omitted. For multiplication is automatically assumed. Add, exponents Multiply, the mantissas 0.100011 Re-normalize, Sign, here both numbers are positive so the sign bit is Even though the re-normalization stage seems the same, it is actually always the same number of steps, whereas for the floating point case it can be arbitrarily long and needs to check whether there is a leading .pandoc version 3.2 ConclusionIn this article, we discussed how values are passed between layers post quantization in PyTorch. We also discussed why floating point operations are slower than integer operations. I hope this article was helpful in understanding the inner workings of quantization and the differences between floating point and integer operations. Again, PyTorch makes things very simple by doing things for you but if you need to understand the underlying concepts then you need to open things up and verify. Referenceshttps://arxiv.org/pdf/1712.05877https://arxiv.org/pdf/1806.08342","link":"/2024/05/16/quantization-layer-details/"},{"title":"No, You Cannot RCT Your Way to Policy","text":"The Bayesian Policy Maker Ah they say, so here is what you do, you see its very simple. You gather data, you gather all the facts, and then you do the statistics you see, and then you make your decision. You see, a modern policymaker shouldn’t bother with the inconveniences of a ideology and emotions et cetera, that stuff is for amateurs you see. Also, you attach a token picture of poor people being poor in a 3rd world country on a website for RCTs (the cover image above is taken from one such website, not sure why it is relevant to their study) and you are well on your way to success! Rarefied air of RCTs In the hallowed halls of economics, evidence-based policy has long been the order of the day, with statisticians and economists working hand in glove to unravel the mysteries of various policies. This delightful dance of data was often accompanied by the sage nods of experts. Enter the Randomized Controlled Trial (RCT), purportedly requiring nary an assumption nor a whisper of prior knowledge. Ah, but herein lies the rub! Some social scientists, in their wide-eyed admiration, have crowned RCTs as the veritable holy grail of evidence, declaring that any nugget of knowledge gleaned from an RCT is the unvarnished truth, thus tossing aside the cumbersome baggage of expert opinion. Combine this with the dazzling allure of Bayesian epistemology, and we have a recipe for an unearned swagger in the land of causal inference. This article aims to lift the veil, to show that the RCT, for all its bravado, is not above the same constraints and foibles that bedevil other studies. The RCT is not a knight in shining armor, but a gallant figure subject to the same trials and tribulations as its more scholarly counterparts. RCTs A HistoryRCTs have their roots in clinical and epidemiological studies. This is perhaps its first impediment to their use in economics and the social sciences. Social issues are often more complex and have more than one causal pathway as opposed to drugs which usually have one casual pathway and a very easily verifiable target (a bacteria or a virus). The second impediment is that while both medicine and social sciences often use overlapping terms they often use quite different language when referring to RCTs. Thus what is known in medicine is not often known and and is almost never salient when considering an RCT in economics/ social sciences. We consider two issues : Average Treatment effect and why they are not the truth How to use an RCT’s results once we have them Bias and PrecisionLet us clarify two important terms: bias and precision. To a non-technical audience, the term \"unbiased\" often carries an unusually high status, perhaps because it is commonly associated with impartiality in political opinions. However, in statistics, being \"unbiased\" simply means that on average, the results are correct. It does not imply accuracy in every instance. Each individual RCT might produce highly erroneous results in either direction, but these errors tend to cancel out when averaged. Consequently, the fact that an RCT is unbiased provides limited value. The second term, precision, refers to the degree of correctness on average. In the context of RCTs, precision indicates how close the results are to the true value on average. RCTs are notoriously imprecise, as illustrated by studies that have documented large errors in both directions, such as those involving hormone replacement therapy (HRT). This lack of precision is well-known, and economists often seek to enhance precision by incorporating \"biased\" and \"subjective\" expert opinions. Measure Theory and the ATEAll misunderstandings about probability come from the confidence of individuals who have never had the wind knocked out of them by measure theory. And so in this section that is what we will do. Fundamentally, the treatment effect, ’s equation is given by, Where the boolean variable is or accordingly as whether the th individual is in treatment or control. Ideally we would like to measure . That is, we would like to observe the same individual in treatment and control and measure the difference in outcome in the two cases. In absence of this we can only observe i.e. the difference in population mean between the treated and un-treated population. It is a remarkable theorem from statistical theory that says that, the difference in these means is an unbiased estimator of the treatment effect. This is remarkable because it requires very few if any assumptions. Recall, that unbiased-ness buys us relatively little for a study done once as it could be a completely random effect we observe in any one study. Below, is a measure theoretic proof of why this is the case, Where this last equation follows from the linear nature of the expectation operator. This is another vital weakness of the RCT, you can only ever get at the mean effect. You cannot get a meaningful measure of any other statistic. As an economist we are very often concerned with the median and below is a similar proof of why this is not the case, one can immediately see that this is a lot hairier to linearly separate than before, i.e. This is another critical weakness of an RCT, you can only tell what the treatment effect is in expectation. Though not useless, this is far from usual when a statistician would rather know the entire distribution of outcomes. This is generally the case with other forms of studies. RandomizationRandomization is often looked at as this perfect tool that answers all questions related to variance between the treatment and control group but as we will see this is often not the case in practice. Recall, Usually, the second term on the right is equated to . But there is considerable slight of hand involved here. While un-biasedness guarantees that the second term is in expectation. In any one trial we have no idea about the size of this term. This is referred to in the clinical trials literature as random confounding or realized (as in one realization of a trial) confounding. Randomization and BalanceExactly what randomization does is lost in popular parlance. There is often a misconception that randomization (in the sense of a laboratory clinical experiment) does as much as a perfect control. This is not the case, randomization is often always far worse than a good control. This fact is often lost in popular literature and can be captured by this quote in a World Bank manual attributed to Gerter et al 2016. We can be confident that our estimated impact contributes the true impact of the program, since we have eliminated all observed and the unobserved factors that might plausibly explain the difference in outcomes. This statement confuses the fact that the second term is zero in expectation over many hypothetical trials (which this study did not do) and with it being zero in any one trial. Popular economics literature is littered with such statements. It is this lack of nuance (and grace) that is the cause for RCTs being as widely misunderstood as they are. Post Mortem While, I have many misgivings about Bayesian epistemology in general, I think it is a very narrow way of viewing the world. It is also pseudo mathematical for a variety of reasons (see Pollock for a mathematical discussion of why it is not a tractable mathematical theory, but rather a subjective philosophy that appropriates real mathematics). For the reasons mentioned here, I think that RCTs do not often contribute enough signal to update probabilities about hypotheses in a meaningful way. In addition, it has become fashionable to quote a counter intuitive or counter-theoretical result from an RCT to suggest that theory needs to change, rather than requiring more studies or more information about causal pathways to improve the body of scientific evidence in either direction. Finally, the idea that RCTs require little to theory is patently false. Since a good RCT requires a good control which often requires a good theory which in turn is subject to all the shortcomings of the human experience such as political bias and subjectivity. No causality in, no causality out. Words to live by indeed. ReferencesCritique of Bayesian Epistemology (https://johnpollock.us/ftp/PAPERS/Problems%20for%20Bayesian%20Epistemology.pdf)Understanding and misunderstanding randomized controlled trials (https://pubmed.ncbi.nlm.nih.gov/29331519/)","link":"/2024/08/04/rct-your-way-to-policy/"},{"title":"What does Game Theory say about voting for RFK?","text":"IntroductionIt is perhaps better to start this article off by clarifying what it is not rather than what it is. First, this is not a comprehensive review of RFK’s policies and what he stands for (there are far better places to seek that information). Second, this is not meant to convince you to vote one way or another based on policy and beliefs (again, there are far better places for that too). So then what the blazes did I write this for? Well, the motivation for this article comes from multiple conversations with friends and family who want to know more about voting for independents in general and RFK in particular. Addressing issues such as, “Is it a wasted vote?” “Do I vote for RFK to make a point?” / “If we do not vote for Independents, then how will they ever win?” I believe that these are important questions to ask and I hope to address them in this article. In order to answer those questions I will first explain the voting system in place and various strategies that can be used to win and election, Differences between Parliamentary (such as the UK and India) and Winner-Takes-All Democracy (USA). Splitting the vote - what it really means. Different kinds of potential RFK voters and why they matter. Strategic Misreporting - why people who say they might vote for RFK might not actually vote for RFK but simply want you to vote for him. As a recovering Game Theorist, I love to look at elections as “games” and therefore I will use the word “strategy” a lot. A strategy in this sense is an action (in this context voting for a candidate). In the game theoretic structure, we assume that a player (i.e. YOU) is playing to win. But what does it mean to win? In this context, winning means getting policies you care about enacted. I will also address a little, the issue of voting “to make a point” about the current system and why I feel like that is a bad idea. But for the most part, I assume that the reader, wants to get policies they care about enacted. Equally important, I will assume that political parties have atleast some motivation to get elected. While getting elected is not the only motivation of political parties, it is certainly a very important one and allows us to separate out our strategies for voting for them. Differences in DemocracyPerhaps the least understood part of this discussion is the inherent difference between Parliamentary democracy and Winner-Takes-All Democracy (this is technically called Representative Democracy, but I feel that the term obscures its meaning). Before understanding what you should do, it is perhaps worthwhile to understand what the system you are voting within intended for voters to think about. This could be quite different in both systems and have vastly different implications. Usually, the choice of system has more to do with the history and socio-cultural context at the time of setting up the democracy. It is very difficult to argue (vehemently, at least) for one over the other. But certainly, one should try to understand why a particular system was chosen and at least try to engage with viable strategies within that system. For much of this article, I will consider hypothetical political parties, the first two are large and usually get most of the vote share, the independent is small. KH, DJT and RFK. KH, DJT and RFK (an independent). I will consider two hypothetical elections, one in a Parliamentary democracy and one in a Winner Takes All democracy. Parliamentary DemocracyConsider candidates with the following vote shares and a seats in the “Parliament” in a hypothetical parliamentary democracy (number of seats won, in brackets). KH : ( seats) DJT : ( seats) RFK : ( seats) In a parliamentary democracy, KH narrowly wins the election. However, (and this is a big caveat), every time a decision is needed to be made, any one party would need to form an “alliance” with some or all of the other parties to reach the mark. This means that a significant number of independents need to be swayed in order to pass a law (by either side). By the same token, DJT’s influence is not insignificant as they need to sway just more (than KH) independents to pass laws they want. This system comes with a clear message to the voting population’s strategy, you can (and should, if you want to) vote for a party that is smaller than the other two and their voice will be heard at every vote. This system also comes with a clear disadvantage, you need to appeal to independents at every voting instance. This is particularly worse when you consider a situation like this, KH : ( seats) DJT : ( seats) RFK : ( seats) In situations like this, RFK can hold up legislation that almost of the country wants. Bear in mind, that bills in any democracy do not work in isolation, so RFK can hold up a super important bill (Free Childcare) that even their want in exchange for a bill that only their want (Bitcoin deregulation). There are two other future implications that are essential to understanding the Parliamentary system. The first, is that representative democracies encourage a proliferation of independent parties. They do this to the extent that the word independent party loses all meaning, and there are just a large number of parties that cater to ever more niche demographics that can sometimes seem hilariously contradictory (Pro Environment, Pro Socialism) and (Anti Environment, Pro Socialism). The second, is that “winning” in a representative democracy ends up being one of two things. You either get of the seats in parliament or you form a coalition that adds up to using various smaller parties. In such a coalition, parties will often “give up” a few of their essential ideas or concepts (Environment) in exchange for passing laws that support another (perhaps more important) essential idea (Socialism). Notice, that voting for more and more independent parties does not lead to more diversity in voting ideologies, it just means that the reduction in diversity is left up to the party representative not the voters. For example, say you voted for a pro-Environment, Pro Socialist party. Since they are a niche party they formed a coalition with a Socialist party and gave up on Environmental regulation. Now had you known the full result of the election in advance, you might not have wanted to give up on Environmentalism, you might have given up on Socialism instead. For instance, you could think, if I cannot live in a cleaner environment I might as well have free markets. This paints a picture of a democracy that is very unstable. It is. Since the resolution or tolerance between conflicting ideas takes place at the parliament it is very difficult to gauge what issues are deal breakers for the voting population. But over time Parliamentary democracies tend to form major parties with a constellation of smaller parties that reflect minor interest groups. Governments are formed by one of the two major parties and a collection of smaller parties. We now turn to the other case. To fix the issue of stability and to reduce the outsized influence of smaller parties, another form of democracy has been proposed that addresses these issues directly. Winner Takes All DemocracyIt is a bit complicated to show an exact example of representative democracy in the US, but this example is a pretty good representation. In this example, there is no parliament, there is just a president, who can do whatever they want for the length of their term. Consider the vote share example as before, KH : ( seats) DJT : ( seats) RFK : (11 seats) In this example KH, can pass all the laws they want. It does not matter that they do not have of the vote share. Notice, also that more people did not want KH to be in power. Potentially all of RFK supporters (more on this later) could have preferred DJT to KH had they known the results of the election before hand. What are the implications of this kind of democracy? First, notice that after the election the elected person is essentially a dictator. There is no need for any negotiation or working with any other parties. This is not a bad thing, since much of the confusion and instability of Parliamentary democracy is done away with. Second, notice that there is a strong disincentive for other political parties to form since even at fairly high levels of representation you can end up with seats. Consider this example, KH : ( seats) DJT : ( seats) RFK : ( seats) While people who voted for KH might definitely consider voting for her again, some of the supporters of RFK might consider either : Not voting at all - which is why voter turnout is such an issue in the US elections trying to persuade DJT to accept them into their party and fight for change in some of it’s core values (maybe considering the environment more). Summary of Differences in Democracy StylesThe key takeaway is that in both systems you have to eventually reconcile your differences to reach that mark. In the Parliamentary system you leave it up to the person you vote for, no matter how small their party is. But in the Winner Take All system, you have to do it yourself, or you risk coming away with nothing (hence the Winner Takes ALL!). Again, either way, some (or most) of your ideologies will be resolved to reach a decision. Opinion : So What Should You Do?Well, one thing is clear, since the US is a Winner Take All system you should reconcile your differences with the major parties and place your vote there. While it was not clear to me why this system was chosen in the US, it seems that the pressure of reconciling one’s differences is on oneself. This system is perhaps why we have a two party system in the first place. The motivation for a voter to vote for an independent is very low (but there is one situation in which it makes sense, more on that later) to the point that it has prevented the formation of more parties. Which is why it is ironic that many independents run on a ticket of plurality of opinion but do not actually advocate to change the actual voting system so that more political parties are motivated to coalesce around different combinations of ideas. But short of that, it is up to you to vote for a major party after giving up on some of your ideals. Implications for Reconciling DifferencesIf you are reading this far it means you are at least considering voting for the major parties. One thing is clear when reconciling your differences, you need to figure out which party you would vote for if your top choice did not exist. Thus two kinds of voters exist,\\ Where, means is that if you would vote for over . For instance, if after casting your vote for RFK and seeing he lost you would rather DJT won (had you known RFK would not have won), that means DJT is your second choice. Thus, imagine a world in which RFK lost and think about who you would have preferred. That is who you should vote for. Similarly, if you voted for RFK and DJT won, and you wished that you voted for KH, then your second choice is KH. There is however, one (and only one) situation in which you should vote for RFK and that is the situation in which you are truly indifferent between DJT and KH. That is,IF, on the day after the election you truly do not care if RFK lost. I think that such candidates are likely to be of two kinds (and I do not think readers of this article are likely to be either). Non-voters : They would probably have not voted any way. If you are going to vote if RFK was not running then this is NOT you. Ideologically inconsistent : Since independents and RFK generally seek to appeal to both parties and therefor take centrist positions, it is not possible for someone to be truly indifferent between KH and DJT. For example consider the following policy positions, - RFK (Pro-Life, Pro-Environment) - DJT (Pro-Life, Anti-Environment) - KH (Pro-Choice, Pro-Environment) If you really are indifferent between KH and DJT then you are indifferent between (Pro-Life, Anti-Environment) and (Pro-Choice, Pro-Environment). This is unlikely, since these are such salient issues, you would certainly have an opinion on which you would rather have. If you really are indifferent about such important issues you are not an ideological voter and are motivated by something other than getting policies you care about enacted. This could be someone who votes for RFK to “make a point” about the current system. But equally this could be someone who votes based on personality rather than someone voting on issues alone. Strategic ImplicationsInterestingly, it is in the interest of the party that thinks they will lose to promote the independent candidate. Consider the following strategy by DJT, Promote RFK as an independent (ask your donors to donate to him). Appear as similar as possible to RFK (public appearances, phone calls etc). Make sure that RFK is on the ballot in as many states as possible. With this strategy it will be possible to make it appear like RFK is very similar to you but different enough from KH thereby ensuring that your vote base is intact but people will defect from KH. Strategic MisreportingThere is another more complex issue that is known to occur in voting. The best way to understand it is to understand that people voluntarily disclose their voting strategy and that this strategy is never verified. Essentially you can say you are going to vote for any candidate and no one will ever know if you did or not. People misreport for a variety of reasons, including embarrassment, social pressure and privacy. With the rise in far right parties in Europe, people are less likely to admit that they voted for them. However, one of the most interesting reasons to misreport is for strategic reasons. Consider the following strategy, You are a DJT voter and you know that RFK is more likely to take votes away from KH than DJT. You tell people you are going to vote for RFK, this will encourage other people to vote for RFK. This will make it more likely that KH will lose votes to RFK but not DJT. Thus when discussing your voting strategy it is important to remember that a person whose second choice candidate is KH and whose second choice is DJT are fundamentally different people. Conclusion “Is it a wasted vote?” Yes it is, for reasons above the American system expects you to reconcile your differences with the major parties and then cast your vote. If not, you will come away with either : - your third choice candidate winning implementing policies that are objectively worse for you. - you vote for an independent but the people telling you to do not (strategic misreporting). “Do I vote for RFK to make a point?” / “If I do not then no independent will ever win?” No you should not. The reason that independents do not win has more to do with the system than the fact that they do not get enough votes. Even if an independent ends up with very very high percentages of vote share they can end up with no representation. The system is inherently Winner-Takes-All, now you could ask, “why not change the system?” and that is a good question. Unfortunately that would need to be done by the major parties and they have no incentive to do so. But guess what, the best way to do that is to vote for a candidate from the major parties who has a policy of changing the voting system. Best of luck with that. In the past many candidates have been independent and have garnered huge amounts of popular support (at the primary stage), but these candidates have inevitably joined either of the two parties. So what ends up happening is one of two things, if the major parties think an independent is popular and risks a big chunk of vote share, they offer them a ticket. if the major parties do not view them as a risk they ignore them and hope they do not take too much vote share. If they do take vote share this has the effect of penalizing the candidate who has less fanatical (nationalistic/ personality driven) supporters since they are more open to truly voting based on ideology. I think that rank order voting is a good system to implement in the US, and advocating directly for that is a better strategy than voting for an independent. As I said, it is funny that independents do not directly advocate for this system, but it is likely that they are not able to get enough votes to be taken seriously. Let us conclude with an example of rank order voting. In this voting system, instead of voting for candidates you express your preferences for all the candidates. And the candidate with the least points WINs. That is, not only do you care about how many ballots had your name at the top, but also considers how many people had you at the bottom.KH\\succ DJT\\succ RFK (1)KH\\succ RFK\\succ DJT (40)DJT\\succ KH\\succ RFK (1)DJT\\succ RFK\\succ KH (36)RFK\\succ KH\\succ DJT (15)RFK\\succ DJT\\succ KH (7) KH points : 41 * 1 + 16 * 2 + 43 * 3 = 1 + 32 + 129 = 162DJT points : 37 * 1 + 8 *2 + 55 * 3 = 37 + 16 + 165 = 218RFK points : 22 * 1 + 76 * 2 + 2 * 3 = 22 + 152 + 6 = 180 This example proves the benefits of rank order voting since you can notice several things. KH wins in both systems, if you have enough first place votes you are the winner pure and simple. DJT’s loss was made worse by this system because of the huge number of people who had him at the bottom. This is not surprising for the people who had KH on top of their ballot. But because of the huge number of people who had RFK on the top of their ballot but DJT at the bottom of the ballot. RFK is not as bad a candidate as it seems, even though he had only 22 first place votes, when considering his second place votes he is actually not a bad candidate. In the rank order system you can use your third place vote to essentially veto a bad candidate, it essentially says this is who I prefer at the top (RFK) but I definitely don’t want my 3rd place candidate (DJT) I would rather have (KH). This essentially allows the two different kinds of RFK voters to express both their preferences.","link":"/2024/08/12/rfk/"},{"title":"A Short Note on Singularities in Physics and Mathematics","text":"IntroductionIt is often difficult to speak about things like singularities because of their prevalence in pop culture. Oftentimes a concept like this takes a life of its own, forever ingrained in ones imagination as a still from a movie (for me this is that scene from Inception where they encounter Gargantua for the first time). Like many concepts in theoretical physics, popular culture is often better at bringing them into light than it is at bringing them into focus. In this article I will try to explain in simple terms what a singularity is and how that relates to physical reality. As always, I will give an exact example of the singularity by means of an equation. At the end, once the mathematics is clear, I will try to explain what the physical reality of the singularity is. Mathematical SingularitiesSingularity of 1. Behavior of the Function: - As (approaching from the positive side): - As (approaching from the negative side): At , the function becomes infinitely large (or small), making a singularity. This is a pole of the function where the value tends to infinity. 2. Undefined at the Singularity: The function is undefined at , which is the point of discontinuity. In mathematics, singularities are not a problem. Physics SingularitiesThe singularity of a black hole can be described by the Schwarzschild metric, which is the solution to Einstein’s field equations for a non-rotating, uncharged black hole. The Schwarzschild metric is given by: Where: is the spacetime interval, is the speed of light, is the gravitational constant, is the mass of the black hole, is the radial coordinate, and are angular coordinates. Be careful though these are not polar co-ordinates, these are coordinates for the Schwarzschild metric. They are a kind of nested spherical coordinate system, this does not seem to affect the solution but helpful to know. The singularity occurs at . As , the term grows without bound, leading to an infinite curvature of spacetime. This represents the physical singularity of the black hole. Additionally, the component of the Schwarzschild metric, which is the time-time component, becomes singular as : As , , indicating the breakdown of spacetime and the presence of a singularity. You can create another singularity by setting in the metric, this is the event horizon of the black hole. This is the point at which light can no longer escape the black hole. However, this is solely a mathematical singularity, since you can still define the metric at this point by a change of coordinates. One such set of coordinates is the Kruskal-Szekeres coordinates, which are used to describe the Schwarzschild metric in a way that is regular across the event horizon. The Schwarzschild metric in Kruskal-Szekeres coordinates is given by: where is a function of and , implicitly determined by: Here, is the Schwarzschild radius: The coordinate singularity at in the Schwarzschild metric is removed by transforming to Kruskal-Szekeres coordinates, and the metric remains regular across the event horizon. Another Physics SingularityAgain, starting from yet another solution for the field equations we can derive FLRW metric (Friedmann-Lemaître-Robertson-Walker metric) which describes the universe as a whole. The words homogenous and isotropic, effectively mean that instead of considering each individual planet in the universe as an actual individual body, we consider them to be individual particles in a fluid (in fact, the FLRW metric considers each galaxy to be a particle). We do this so that we can use equations for fluids to simplify the stress energy tensor in the Field Equations. Our strategy to solve the field equations is as follows, Assume the universe is some kind of fluid (so basically zoom out till all the galaxies look like a fluid) From 1, you can write down the stress energy tensor for the fluid, this is a simple equation (This is for the Schwarzschild metric, and for many other useful metrics, so we never really had this problem before, but when you zoom out you need it) The FLRW metric, which describes a homogeneous and isotropic universe, is given by: Where: is the spacetime interval, is the speed of light, is the cosmic time, is the scale factor of the universe, is the radial coordinate, and are angular coordinates, is the curvature of space, which can be , , or . The scale factor describes how the universe expands or contracts with time. The curvature parameter determines the geometry of space: negative curvature for , flat curvature for , and positive curvature for . Friedmann Equations RecapThe Big Bang is represented in the Friedmann equations as a singularity at the beginning of time when the scale factor approaches zero. This signifies an initial state of infinite density, temperature, and curvature. The Friedmann equations in cosmology are derived from Einstein’s field equations for a homogeneous and isotropic universe. Assuming zero cosmological constant (), they are: First Friedmann Equation: Second Friedmann Equation (acceleration equation): Continuity Equation (conservation of energy): where: - is the scale factor (the “size” of the universe at a given time ), - is the energy density, - is the pressure, - is the gravitational constant, - is the curvature parameter (for a flat universe, for closed, and for open). Representation of the Big Bang SingularityIn the context of the Friedmann equations, the Big Bang is identified by the conditions: - as , - as (implying infinite density and temperature), - Curvature becomes infinite, signaling a physical singularity. Explanation Using the First Friedmann EquationIn the first Friedmann equation: As : - The scale factor approaches zero. - For a positive energy density , the term (known as the Hubble parameter) goes to infinity, meaning the rate of expansion is initially unbounded. - If , then the energy density since is inversely related to the volume of the universe. Thus, at , the universe is in a state of infinite density and infinite curvature, which we identify as the Big Bang singularity. Continuity Equation and Energy ConservationThe continuity equation: implies that as approaches zero, the rapid change in the scale factor causes the energy density to increase sharply, reinforcing the singularity concept. Physical InterpretationAt , when the scale factor , the energy density theoretically becomes infinite, meaning all mass, energy, and curvature are compressed into a single point. This condition marks the beginning of the universe, as described by the Big Bang theory, before which classical descriptions of time and space may no longer apply due to quantum gravitational effects. In short, the Big Bang singularity in the Friedmann equations marks the initial state of the universe at , where , density and temperature are infinite, and classical general relativity predicts a breakdown in spacetime structure. Connection to RealityWhile all of the above can be found in a basic undergraduate textbook, I think the goal of me writing this post was to have a collection of examples of singularities both from mathematics and physics to reinforce the idea of reality. While in the mathematical examples, the does not represent an actual place that we can go and take measurements of , but what if we did? What if we indeed knew a physical place in the world, where the function really described the behavior of the world. This is not hard, you could imagine this as the share that each person gets (of a cake or similarly sweet treat) if there are people. If there are people, each person gets of a share. What does it mean to have people? This is the kind of question that the mathematical singularity is trying to answer. But it is physically impossible to have people, so the singularity is not a real place. If you had people and a cake, the question of dividing it does not make sense. In much the same way, the singularity of the Schwarzschild metric is not a real place, it is a place where the equations break down. This does not mean that some wild stuff happens at the singularity, it means that the equations we are using to describe the world are not valid at that point. This is the same as saying that the function is not defined at . Very often in movies, the singularity is portrayed as a place where the laws of physics break down. This is not true it is just that the laws of physics defined by the equations work everywhere else but not at that point. This could mean one of two things, 1. The equations are not valid at that point, so we need to find new equations that are valid at that point. 2. Some wild stuff happens at that point, and we need to find out what that is. And rework our equations to include that. But simply by looking at the equations, we cannot say which of the two is true. We need to go out and measure the world to find out. Referenceshttps://diposit.ub.edu/dspace/bitstream/2445/59759/1/TFG-Arnau-Romeu-Joan.pdf","link":"/2024/10/22/singularities/"},{"title":"Soft Actor Critic (Visualized) : From Scratch in Torch for Inverted Pendulum","text":"IntroductionIn this post, I will implement the Soft Actor Critic (SAC) algorithm from scratch in PyTorch. I will use the OpenAI Gym environment for the Inverted Pendulum task.The goal of this post is to provide a Torch code follow along for the original paper by Haarnoja et al. (2018) [1]. Many implementations of Soft Actor Critic exist, in this code we implement the one outlines in the paper.You can follow along by starting from main_sac.py at the following link:https://github.com/FranciscoRMendes/soft-actor-critic Inverted Pendulum v0 Environment Set UpEnvironment Set UpLink to the environment here : https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/gym_pendulum_envs.py Example DataThe data from playing the game looks something like this, with each instant of game play denoted by a row. Note this data is sampled from many different games, so it is not ordered as if coming from one game.The dashes in the column name denote the next state, for example, Position’ is the position at the next time step. Position Velocity Cos Pole Angle Sine Pole Angle Pole Angle Time Step Force L/R Position’ Velocity’ Cos Pole Angle’ Sine Pole Angle’ Pole Angle’ Done 0.0002 0.0085 0.9974 -0.0722 -0.0647 1 0.0137 0.0004 0.0133 0.9973 -0.0738 -0.0985 FALSE 0.0174 0.0954 0.9964 -0.0842 -0.4624 1 0.0389 0.0191 0.1039 0.9957 -0.0926 -0.5079 FALSE 0.0031 0.0427 0.9969 -0.0785 -0.2768 1 0.0290 0.0040 0.0497 0.9965 -0.0837 -0.3173 FALSE 0.0046 0.0540 0.9965 -0.0840 -0.3380 1 0.0327 0.0056 0.0617 0.9959 -0.0902 -0.3818 FALSE 0.0008 0.0195 0.9967 -0.0813 -0.1428 1 0.0203 0.0012 0.0255 0.9964 -0.0843 -0.1822 FALSE 0.0071 0.0438 0.9994 -0.0359 -0.1959 1 0.0196 0.0079 0.0478 0.9992 -0.0395 -0.2158 FALSE 0.0133 0.1056 0.9928 -0.1194 -0.6067 1 0.0512 0.0153 0.1171 0.9915 -0.1304 -0.6702 FALSE State Description in InvertedPendulumBulletEnv-v0 Cart Position – The horizontal position of the cart. Cart Velocity – The speed of the cart. Cosine of Pendulum Angle – , where is the angle relative to the vertical. It equals 1 when upright and decreases as it tilts. Sine of Pendulum Angle – complements , providing a full representation of the angle. Pendulum Angular Velocity – The rate of change of . ActionThe action space is continuous and consists of a single action that can be applied to the cart. The action is a force that can be applied to the cart in the left or right direction. The force can be any value between and . Reward &amp; TerminationThe reward is for every time step the pole is upright. The episode ends (Done is TRUE) when the pole is more than degrees from the vertical axis or the cart moves more than units from the center. Game play GIFAn example of game play would look like this, not the most exciting thing in the world, I know. The Neural Networks in Soft Actor Critic NetworkThe Lucid chart below encapsulates the major neural networks in the code and their relationships. Forward relationships (i.e. forward pass) are given by solid arrows. While backward relationships (i.e. backpropagation) are given by dashed arrows.I recommend using this chart to keep a track of which outputs train which networks. Note however, that these backward arrows describe merely that some relationship exists. There are differences in the backpropagation used to train the policy network itself (uses the reparameterization trick) and the Value networks (does not). The main object in the code is the object called SoftActorCritic.py. It consists of the neural networks and all the hyperparameters that potentially need tuning. As per the paper the most important one is reward scale. This is a hyperparameter that balances the explore-exploit tradeoff. Higher values of the reward will make the agent exploit more. This class contains the following Neural Networks, their relationships are illustrated in the Lucid Chart above: self.pi_phi: The actor network, which outputs the action given the state. In the paper this is denoted by the function , where is the policy, are the parameters of the policy, is the action at time , and is the state at time . This neural network will take in the state vector in this case the dimensional state vector, it can output two things action : a continuous vector of size to take in the environment (no re-parameterization trick) The mean and variance of the action to take in the environment, and respectively (re-parameterization trick) self.Q_theta_1 : The first Q-network, this is also known as the critic network. It takes in the state and action as input and outputs the Q-value. In the paper this is denoted by the function , where is the Q-function, are the parameters of the first Q-network, is the state at time , and is the action at time . self.Q_theta_2 : The second Q-network, this is also known as the critic network. It takes in the state and action as input and outputs the Q-value. In the paper this is denoted by the function , where is the Q-function, are the parameters of the second Q-network, is the state at time , and is the action at time . self.V_psi : The Value network parameterized by in the paper. It takes in the state as input and outputs the value of the state. In the paper this is denoted by the function , where is the value function, are the parameters of the value network, and is the state at time . self.V_psi_bar : The target value parameterized by in the paper. It takes in the state as input and outputs the value of the state. In the paper this is denoted by the function , where is the value function, are the parameters of the target value network, and is the state at time . Couple of things to watch out for in these neural networks that can be quite different from the usual classification use, Forward pass and inference (i.e. using the SoftActorCritic Network) are different, in the forward pass you are still using outputs to improve the policy network so that it plays better. However, to play the game you only ever need the policy network. In the classification case, the forward pass and inference are the same and hence used interchangeably. The backward dashed arrows for backpropagation are important because it is not always clear what the “target” to train one of these neural networks is. The “target” is often from a combination of outputs from different networks and the rewards. The top row of nodes, States, Actions, Rewards and Next States are the “data” on which the neural networks are to be trained.123456789101112131415class SoftActorCritic: def __init__(self, alpha=0.0003, beta=0.0003, input_dims=[8], env=None, gamma=0.99, n_actions=2, max_size=1000000, tau=0.005, batch_size=256, reward_scale=2): self.gamma = gamma self.tau = tau self.memory = ReplayBuffer(max_size, input_dims, n_actions) self.batch_size = batch_size self.n_actions = n_actions self.pi_phi = ActorNetwork(alpha, input_dims, n_actions=n_actions, name='actor', max_action=env.action_space.high) # 1 self.Q_theta_1 = CriticNetwork(beta, input_dims, n_actions=n_actions, name='critic_1') self.Q_theta_2 = CriticNetwork(beta, input_dims, n_actions=n_actions, name='critic_2') self.V_psi = ValueNetwork(beta, input_dims, name='value') self.V_psi_bar = ValueNetwork(beta, input_dims, name='target_value') self.scale = reward_scale # You will find this in the ablation study section of the paper this balances the explore/exploit tradeoff self.update_psi_bar_using_psi(tau=1) Learning in SACThe learning in the model is handled by the learn function. This function takes in the batch of data from the replay buffer and updates the parameters of the networks. The learning is done in the following steps: Sample a batch of data from the replay buffer. If the data is not enough i.e. smaller than batch size, return. Optimize the Value Network using the soft Bellman equation (equation ) Optimize the Policy Network using the policy gradient (equation ) Optimize the Q Network using the Bellman equation (equation ) Couple of asides here, The words network and function can be used interchangeably. The neural network serves as a function approximator for the functions we are trying to learn (Value, Q, Policy). The Value Networks and Policy Networks are dependent on the current state of the Q network. Only after these are updated can we update the Q network. All loss functions are denoted by in the paper. The subscript denotes the network that is being optimized. For example, is the loss function for the Value Network, is the loss function for the Policy Network, and is the loss function for the Q Network. The Target Network is simply a lagged duplicate of the current Value Network. Thus, it does not actually ever “learn” but simply updates it weights through a weighted average between the latest weights from the value network and its own weights, this is given by the parameter in the code. This is done to stabilize the learning process. Variable names can be read as one would read the variable from the paper for instance is given by V_psi_bar_s_t_plus_1. It is unfortunate that python does not allow for more scientific notation, but this is the best I could do. Re-parameterization TrickOne of the most confusing things to implement in python. You can skip this section if you are just starting out but its use will become clear later. Adding the details here for completeness. The main problem we are trying to solve here is that Torch requires a computational graph to perform backpropagation of the gradients. rsample() preserves the graph information whereas sample() does not. This is because rsample() uses the reparameterization trick to sample from the distribution. The reparameterization trick is a way to sample from a distribution while preserving the gradient information. It is done by expressing the random variable as a deterministic function of a parameter and a noise variable. In this case, we are using the reparameterization trick to sample from the normal distribution. The normal distribution is parameterized by its mean and standard deviation. We can express the random variable as a deterministic function of the mean, standard deviation, and a noise variable. This allows us to sample from the distribution while preserving the gradient information. sample(): Performs random sampling, cutting off the computation graph (i.e., no backpropagation). Uses torch.normal within torch.no_grad(), ensuring the result is detached. rsample(): Enables backpropagation using the reparameterization trick, separating randomness into an independent variable (eps). The computation graph remains intact as the transformation (loc + eps * scale) is differentiable. Key Idea: eps is sampled once and remains fixed, while loc and scale change during optimization, allowing gradients to flow. Used in algorithms like SAC (Soft Actor-Critic) for reinforcement learning.If you want to sample both the values and plot their distributions they will be identical (or as identical as two samples sampled from the same distribution can be). A good explanation can be found here : https://stackoverflow.com/questions/60533150/what-is-the-difference-between-sample-and-rsample 123456789101112131415def sample_normal(self, state, reparameterize=True): mu, sigma = self.forward(state) probabilities = Normal(mu, sigma) if reparameterize: actions = probabilities.rsample() else: actions = probabilities.sample() action = T.tanh(actions)*T.tensor(self.max_action).to(self.device) log_probs = probabilities.log_prob(actions) log_probs -= T.log(1-action.pow(2)+self.reparam_noise) log_probs = log_probs.sum(1, keepdim=True) return action, log_probs Learning the Value FunctionWith all the caveats and fine print out of the way we can begin the learn function.Here we take a sample of data from the replay buffer. Now recall, that we need to take a random sample and not just the values because the data is not i.i.d. and we need to break the correlation between the data points. 12sample = self.memory.sample_buffer(self.batch_size)s_t, a_t_rb, r_t, s_t_plus_1, done = self.process_sample(sample, self.pi_phi.device) Let us first state the loss function of the value function. This is equation 5 of the Haarnoja et al. (2018) paper. Comments, is the output of the value function, which would just be a forward pass through the value neural network denoted by self.V_psi(s_t) in the code. is the output of the target value function, which would just be a forward pass through the target value neural network for the next state denoted by self.V_psi_bar(s_t_plus_1) in the code. We also need the output of the Q function, which would just be a forward pass through the Q neural network denoted by self.Q_theta_1.forward(s_t, a_t) in the code. But since we have two Q networks, we need to take the minimum of the two. This is done to reduce the overestimation bias in the Q function. 1234567891011121314151617181920212223V_psi_s_t = self.V_psi(s_t).view(-1)V_psi_bar_s_t_plus_1 = self.V_psi_bar(s_t_plus_1).view(-1)V_psi_bar_s_t_plus_1[done] = 0.0a_t_D, log_pi_t_D = self.pi_phi.sample_normal(s_t, reparameterize=False) # here we are not using the reparameterization trick because we are not backpropagating through the policy networklog_pi_t_D = log_pi_t_D.view(-1)# Find the value of the Q function for the current state and action, since we have two networks we take the minimum of the twoQ_theta_1_s_t_a_t_D = self.Q_theta_1.forward(s_t, a_t_D)Q_theta_2_s_t_a_t_D = self.Q_theta_2.forward(s_t, a_t_D)Q_theta_min_s_t_a_t_D = T.min(Q_theta_1_s_t_a_t_D, Q_theta_2_s_t_a_t_D)# This is the Q value to be used in equation 5Q_theta_min_s_t_a_t_D = Q_theta_min_s_t_a_t_D.view(-1)self.V_psi.optimizer.zero_grad()# This is exactly equation 5J_V_psi = 0.5 * F.mse_loss(V_psi_s_t, Q_theta_min_s_t_a_t_D - log_pi_t_D)J_V_psi.backward(retain_graph=True) # again, we don't need to backpropagate through the policy networkself.V_psi.optimizer.step() # Update the value network Learning the Policy FunctionThe policy function is learned using the policy gradient. This is equation 12 of the Haarnoja et al. (2018) paper.The expectation means that we can use the mean of the observed values to approximate the expectation.For performing the optimization on the policy network we need to do two things to get a prediction, Perform a forward pass through the network to get and . Sample an action from the policy network using the reparameterization trick. This ensures that the computational graph is preserved and we can backpropagate through the policy network. This was not true in the previous case.Here it may seems like the values for and are the same as the ones we used for the value function. This is not the case, we need to sample a new action from the policy network and use that to compute the Q value and log probability. This is because we are trying to learn the policy function, which is a stochastic process. We need to sample a new action from the policy network and use that to compute the Q value and log probability. This is done using the reparameterization trick. 123456789101112131415# a_t_D refers to actions drawn from a sample of the actor network and not the true actions taken from the replay buffera_t_D, log_pi_t_D = self.pi_phi.sample_normal(s_t, reparameterize=True) # here we are using the reparameterization trick because we are backpropagating through the policy networklog_pi_t_D = log_pi_t_D.view(-1)Q_theta_1_s_t_a_t_D = self.Q_theta_1.forward(s_t, a_t_D)Q_theta_2_s_t_a_t_D = self.Q_theta_2.forward(s_t, a_t_D)Q_theta_min_s_t_a_t_D = T.min(Q_theta_1_s_t_a_t_D, Q_theta_2_s_t_a_t_D)Q_theta_min_s_t_a_t_D = Q_theta_min_s_t_a_t_D.view(-1)# This is equation 12 in the paper# note that this is identical to the original loss function given by equation 10# after doing the re-parameterization trickJ_pi_phi = T.mean(log_pi_t_D - Q_theta_min_s_t_a_t_D)self.pi_phi.optimizer.zero_grad()J_pi_phi.backward(retain_graph=True)self.pi_phi.optimizer.step() Learning the Q-NetworkIn this section we will optimize the critic network. This would correspond to equation 7 in the paper. Noting that, This is somewhat different from equation 7 in the paper, First, does not depend on in this case. This is because we are using the Inverted Pendulum environment, which gives a constant reward for each step. Second, we drop the expectation over because we are using a single sample from the replay buffer for each (technically you should take the mean over multiple but this is a good enough approximation). We use the actual actions taken from the replay buffer to compute the Q value. This is because we are trying to learn the Q function, which is a deterministic process. We need to use the actual actions taken from the replay buffer to compute the Q value. This is given by a_t_rb in the code. We have two Q networks so we need to apply this individually to both networks. 1234567891011121314151617# In this section we will optimize the two critic networks# We will use the bellman equation to calculate the target Q valueself.Q_theta_1.optimizer.zero_grad()self.Q_theta_2.optimizer.zero_grad()# Equation 8 in the paper, in the paper the reward also depends on a_t# but in this case we get a constant reward for each step, so we can just use r_t# consequently, Q_hat_s_t AND NOT Q_hat_s_t_a_tQ_hat_s_t = self.scale*r_t + self.gamma*V_psi_bar_s_t_plus_1Q_theta_1_s_t_rb_at = self.Q_theta_1.forward(s_t, a_t_rb).view(-1) # this is the only place where actions from the replay buffer are usedQ_theta_2_s_t_rb_at = self.Q_theta_2.forward(s_t, a_t_rb).view(-1)# this is equation 7 in the paper, one for each Q networkJ_Q_theta_1_loss = 0.5 * F.mse_loss(Q_theta_1_s_t_rb_at, Q_hat_s_t)J_Q_theta_2_loss = 0.5 * F.mse_loss(Q_theta_2_s_t_rb_at, Q_hat_s_t)J_Q_theta_12 = J_Q_theta_1_loss + J_Q_theta_2_lossJ_Q_theta_12.backward()self.Q_theta_1.optimizer.step()self.Q_theta_2.optimizer.step() Learning the target value networkThe final piece of this puzzle is learning of the target value network. Now, there is no actual “learning” taking place in this network.This network is simply a weighted lagged duplicate of the current value network. Thus, it does not actually ever “learn” but simply updates it weights through a weighted average between the latest weights from the value network and its own weights, this is given by the parameter in the code. This is done to stabilize the learning process.This takes place in the line self.update_psi_bar_using_psi(tau=None) of the learn function.The parameter tau is used to weight the copying, with tau = 1 being a complete copy and tau = 0 being no copy. Obviously for the learning to take place tau&gt;0 but usually a vale of is used.This function corresponds to the last line in the algorithm, 1234567891011121314151617def update_psi_bar_using_psi(self, tau=None): # This function corresponds to the update step inside algorithm 1 # this is the last line in the algorithm # psi_bar = tau* psi + (1-tau)*psi_bar if tau is None: tau = self.tau psi_bar = self.V_psi_bar.named_parameters() psi = self.V_psi.named_parameters() target_value_state_dict = dict(psi_bar) value_state_dict = dict(psi) for name in value_state_dict: value_state_dict[name] = tau*value_state_dict[name].clone() + (1-tau)*target_value_state_dict[name].clone() self.V_psi_bar.load_state_dict(value_state_dict) ConclusionThis post has been a detailed walk through of the Soft Actor Critic algorithm using inverted pendulum as an example. Other implementations of this algorithm exist. The best one I have found is Phil Tabor’s implementation.However, there was not a very good connection between the code and the paper. This post was an attempt to bridge that gap by using notation that exactly matches the paper, while keeping the overall structure simple to understand.In my next post, I will implement the Soft Actor Critic Algorithm on the Lunar Lander game, this will hopefully make for a more interesting visualization of how the algorithm learns better. References Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv preprint arXiv:1801.01290. https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/SAC Phil’s Youtube video https://www.youtube.com/watch?v=ioidsRlf79o Oliver Sigaud’s video https://www.youtube.com/watch?v=_nFXOZpo50U (check out his channel and research for more) https://youtube.com/playlist?list=PLYpLNGpDoiMSMrvgVhgNRwOHTVYbX2lOa&amp;si=unvWxJsJm_w4OcD- https://www.youtube.com/watch?v=kJ9CL7asR94&amp;list=LL&amp;index=22&amp;t=41s (accent might be unclear, but trust me one of the best videos)","link":"/2025/02/17/soft-actor-critic-inverted-pendulum-v0/"},{"title":"The Debate on Free Will: Determinism, the &#39;Hard Problem&#39; of Consciousness, Randomness, and the Computational Nature of the Human Mind","text":"Introduction: The concept of free will has long been a subject of profound philosophical inquiry, engendering discourse, and contemplation over the centuries. It occupies a pivotal position in our comprehension of human agency, moral responsibility, and the quest for a meaningful existence. This essay delves into the intricate tapestry of debate surrounding the concept of free will, examining the perspectives of determinism, the enigma encapsulated by the “hard problem” of consciousness, the role of randomness, and the computational attributes characterizing the human mind. Determinism and Causality: Determinism, as a philosophical viewpoint, posits that every event, encompassing human actions, is intrinsically linked to antecedent events through the unerring chain of causality. From this perspective, the precise modeling and prognostication of the cerebral and neural processes would seemingly challenge the existence of free will. This notion finds reinforcement in the observation that the causality underlying many of our actions can be traced back to physical inputs, including cerebral states and external stimuli. The “Easy Problem” and the “Hard Problem” of Consciousness: To elucidate this perspective, proponents of determinism segregate the dilemma into the “easy problem” and the “hard problem” of consciousness. The “easy problem” is concerned with deciphering the physical mechanisms underpinning our conscious experiences, suggesting that a successful resolution might pave the way for deterministic explanations of human conduct. However, the “hard problem” of consciousness, as delineated by the philosopher David Chalmers, introduces a more profound quandary. This problem revolves around the query as to why and how the physical processes within the human brain give rise to subjective experiences, thereby introducing an element of mystery and insusceptibility to reduction. Challenges to Determinism: The “hard problem” of consciousness introduces a formidable challenge to the principles of strict determinism, requiring that consciousness and subjective experiences transcend facile reduction to mere physical processes. Furthermore, the arguments posited against determinism offer significant insights into the ongoing debate. The following essay attempts to summarize prevailing notions of free will and reasons to believe in its absence. This is not meant to be a summary of current philosophical stances on this debate, nor is it meant to be an authority on the subject itself but rather it is meant to over emphasize arguments for and against the issue that stand out in my mind as relevant based on our discussion, in particular, the tone of this essay is meant to be scientific and not philosophical, with issues surrounding the metaphysics of the debate largely left to the readers own research. 1. Weight of Intuition: It is posited that, in the absence of empirical evidence establishing a concrete link between physical processes and human conduct, a measure of importance must be ascribed to the intuitive sentiment of free will that is experienced by all human beings. This underscores the concept that, pending further scientific exploration, one ought to remain ambivalent before conclusive determinations concerning the presence or absence of free will are made. Intuition, while not constituting an unequivocal argument, functions as a reminder of the unfolding progress in our comprehension of complex phenomena. Human intuition does not (in and of itself) run counter to scientific reason, it is the guiding principle against which scientific accuracy is verified. If human intuition does not align with a scientific theory, great care is taken to experimentally verify the theory. Thus, human intuition is a motivating factor, not a limiting element for scientific discovery. Human experience not aligning with scientific theory drives scientific enquiry, decrying the human experience as an illusion to fit scientific theory (which itself might be at a nascent stage) can be counterproductive to furthering scientific enquiry. 2. Complexity of Modeling: This discourse delves into the intricacies involved in modeling systems with a plurality of variables, emphasizing the critical distinction between modeling a solitary variable leading to a definitive outcome and modeling a multitude of variables collectively influencing an outcome. If human behavior can be reduced to modelling inputs and outputs, then it is fair to say that a comprehensive model of human behavior would end the notion of free will. The evidence does exist for this to be true, multiple studies have shown links between physical processes in the human body and behavioral outcomes. Most notably, the case of a man developing a taste for child pornography due to a tumor in the frontal lobe. Arguments against generalizing this can certainly be made, but here we seek to show that even if such a model and such input output mappings can be established, the complexity of such mappings is a non-trivial issue, since one such complex mapping is “free will”. Thus, simply saying there “could be” a complex mapping is not enough, some bounds must be imposed on the mapping for it to be considered a viable and testable scientific theory (indeed if one does not place such a bound of complexity, one runs the risk of indulging in scientism). It is astutely observed that the simplicity of a model is an essential criterion for it to be recognized as a viable theory of the functioning of the world. In practice, models featuring a multitude of variables may render assessments unfeasible, thereby rendering it arduous to confidently pronounce the presence or absence of free will within such systems. For instance, let us say we have a model for whether someone buys apples based on their blood sugar, Apples = 3 * blood sugar + epsilon. Then for sufficiently large numbers of people, we can observe the data (Apples, blood sugar), posit a model, and verify its accuracy. Suppose that this fails, suppose we are unable to create a model for apple purchases, Apple = f(blood sugar) + epsilon. We then can introduce more and more variables, Apples = f(blood sugar, weight, color of the flag of Mexico, …., day of the week), at some point along this journey the problem does not become that of whether one has the right model or the variables, it becomes that of whether the process being modelled can be modelled at all, as the amount of data required to verify this model increases exponentially and the theory might never be tested. Thus, a highly complex model would be indistinguishable from “free will” and indeed the latter might be a simpler explanation than hypothesizing a complex model without experimentally verifying it. 3. Role of Randomness in Subjective Experience: A contrary perspective asserts that subjective experiences might be due to random occurrences amidst the sequence of physical processes, yet this does not intrinsically proffer evidence for the existence of free will. This viewpoint underscores the notion that the mere recognition of randomness does not necessarily imply the existence of free will. In this context, randomness might be construed as a contributing factor in the intricacies of human conduct, sans signifying genuine free will. Here again, we can take recourse to models of human behavior, if Apple = epsilon i.e. the model of apple buying seems to be truly random, then it is also indistinguishable from free will. Thus, randomness of human behavior could certainly be due to a lack of free will or an important empirical consequence of it. It certainly raises the bar for proof of the absence of free will. Any theory of Free Will should necessarily disentangle randomness from Free Will. 4. Computational Nature of the Human Mind: The perspective articulated herein underscores that the essence of free will resides as the most distinguishing feature of the human mind. The contention posits that contemporary computational devices remain bereft of the emergent property of free will, thus demarcating the human mind as distinctive. The counterpoint presented by an opposing viewpoint underscores the core notion that, notwithstanding the intricacies and emergent properties inherent to the human brain, it ultimately finds its basis in physical processes. This reductionist standpoint emphasizes the primacy of physical processes as the fundamental foundation upon which human cognition and conduct are predicated. 6. Distinguishing Worlds with and without Free Will: A thought-provoking analogy is proffered to discriminate between a world with free will and a world bereft of it. In a world governed by free will, human actions assume an unpredictable character, rendering the task of forecasting the movements of individual humans a challenging pursuit for an external observer. This analogy accentuates the complex and unforeseeable nature of human conduct in a world animated by free will and underscores the formidable challenge of disentangling unpredictability from randomness. 7. Free Will and Science: Neither viewpoint, whether it’s the belief in the compatibility of free will with the laws of physics or the belief in their incompatibility, is inherently more “scientific” than the other. The compatibility of free will with the laws of physics is ultimately a philosophical and metaphysical question, and it falls outside the realm of empirical scientific inquiry in the traditional sense. The scientific method is primarily concerned with empirical observation, experimentation, and the formulation of testable hypotheses. It is well-suited for investigating and understanding the natural world and the physical laws that govern it. However, the question of free will, particularly when it pertains to its compatibility with the laws of physics, involves conceptual and philosophical issues that go beyond empirical observation. Scientists may conduct experiments to study decision-making processes, brain activity, and related phenomena, but the interpretation of these findings in the context of free will remains a matter of philosophical interpretation. Some scientists may hold philosophical positions on the matter, but those positions are not inherently more scientific simply because of their scientific background. In summary, the question of free will’s compatibility with the laws of physics is a philosophical and metaphysical inquiry, and it does not fall strictly within the domain of empirical science. Scientific methods can inform this debate by providing insights into decision-making processes and neural activity, but the question itself is inherently philosophical in nature. Conclusion: In summary, the question of whether free will is a tangible phenomenon remains unresolved. The deterministic perspective, grounded in the principle of causality, offers a compelling viewpoint, suggesting that an in-depth understanding of physical processes may limit human agency. However, the scientific landscape on free will is dynamic and inconclusive. Recent neuroscience advancements have shed light on the neural processes underpinning decision-making, indicating that actions can be predicted based on neural activity before conscious awareness. This sparks debate on the extent of conscious control over choices, yet it doesn’t provide a definitive answer. The debate encompasses philosophical, ethical, and psychological aspects, with some arguing that neural underpinnings don’t negate free will if choices align with desires and values. It is essential to note that while recent advancements in neuroscience may suggest neural pathways for simpler decisions, this doesn’t necessarily apply to more complex decision-making processes. For example, while hunger might correlate with increased spending in a shopping mall, it’s not reasonable to assert that someone’s overall spending patterns can be directly linked to the food they eat. This highlights the intricate nature of human behavior and decision-making, further emphasizing the complexity of the free will debate. Free will is a multifaceted subject, challenging our understanding of the human experience. It requires sustained scientific and philosophical inquiry to unravel its intricacies. For the most current insights, consult recent scientific literature, recognizing the evolving and interpretive nature of this topic.","link":"/2024/02/02/the-debate-on-free-will/"},{"title":"Book Review : We - Evgeny Zamyatin","text":"It is hard to write this book review without overusing superlatives. Widely regarded as the inspiration for both 1984 and Brave New World, this book serves as the template for revolution in a dystopian world ruled by an all-encompassing police state. Written during the early years of the Soviet Union, it holds the dubious distinction of being one of the first (if not the first) books to be banned by the Communist Party. I’ve been on a bit of a Soviet literature tear lately, and this book has been on my list for some time. I’m probably not the target demographic for most modern science fiction, but this book is so much more than that. PlotThe translation conveys an enthusiastic, eager, albeit halting and fragmented tone. This accurately reflects D-503’s (the protagonist’s) hurried journal entries, which form the chapters of the novel. The book is set in 2600 AD, just after the 200 Years War (a war to end all wars), which concluded with the formation of One State, a totalitarian regime where everything, including sex, follows a fixed schedule regulated by a complex network of bureaucracies. Refreshingly, for a science fiction novel, the protagonist is not unhappy with the status quo—he embraces and enjoys it. The One State aligns with his ideals of order, method, and rationality. Everything changes when he meets and subsequently falls in love with I-333. While both 1984 and Brave New World feature similar female characters, the sexual attraction of I-333 is but a small facet of her complex appeal to D-503. She represents a curve in his life of straight lines and right angles. She is a violation of the order he holds so dear, yet he finds himself irresistibly drawn to her. Major ThemesThe obvious metaphors of One State to the Soviet police state are compelling, as are D-503’s eulogies to their necessity and importance in daily life. This is a significant difference to most dystopian fiction that often reflects the downsides of dystopia but none of their motivating factors. Here we have the diary and journal entries of a true believer, we are treated with extensive philosophical insight into why One State exists and why it is good. “The only means of ridding man of crime is ridding him of freedom.” The idea that elimination of freedoms is the only way to rid man of crime is a recurrent theme in the book, of the many references to Christianity in this book, I found this to be the most compelling. Religion (and personal morality) only exists when there is freedom, without freedom there is no need for personal morality. Which is why One State was so successful, it obviated the need for religion, by eliminating all personal freedoms. In the modern Western world, we often take it for granted that individual freedoms are the most basic right. This book asks the reader to challenge that idea substantively via conversations with I-333. One of the less understood ideas of communism is the importance of the collective, an idea almost incomprehensible to those educated primarily in Western philosophy. There is a scene in HBO’s Chernobyl where hundreds of workers scrape the top of the Nuclear plant of radioactive debris in what seems to be an irrational disregard for personal safety all for the greater good of the Soviet Union that captures this sentiment, this book re-iterates that theme across many different mundane freedoms, such as the right to privacy, right to procreate and the right to choose how to lead one’s life. “We comes from God, I from the Devil.” The above quote captures that sentiment better than the scene from Chernobyl does. It is interesting that Zamyatin chose this particular turn of phrase, but it corresponds to an idea in the Eastern Orthodox church that faith can only exist as a collective not as an individual. When referring to belief in God, “I” is almost never used in the Orthodox Church. That is why there is no “I believe in God”, there is only “we believe in God”, in Orthodox prayer. This theme often occurs in juxtaposition with another one, the contrast and similarity between religion and science. This is directly at odds with most of what we now experience in the West. Science and Individual Freedom are the core tenets of most modern societies, however, Zamyatin portrays these ideas as fundamentally in conflict with each other. To One State, science would measure every aspect of the human experience, and make cold blooded calculations of cost and benefit. And remove the benefit at any cost. What does it matter if one life is lost as long as the lives of countless others are preserved? Science allows us to measure everything, why not the human experience? Gradually the belief in the Science of One State obscures its rationale and assumptions. “knowledge, absolutely sure of its infallibility, is faith” And that a move away from the safety and comfort of rational science is a nightmare to him “Now I no longer live in our clear, rational world; I live in the ancient nightmare world, the world of square roots of minus one.” In addition, to the philosophical metaphors the books is rife with references to mathematics in the form of the Taylor and Maclaurin series, which form part of the broader mathematical narrative that is woven around life in One State. This is still relevant almost a hundred years after the book was written, the desire to quantify and measure the human experience is something innate to those who seek to mimic how science is applied to other more tangible fields. SummaryI left this book with a greater appreciation for the randomness and disorder inherent in human beings, and how that is our defining characteristic. Modern capitalist societies can benefit from redistribution through centralization and a stronger sense of community. However, this book offers valuable insight into the dangers of excessive centralization. The fact that the Soviet State remains the only large-scale implementation of communism and serves as the inspiration for One State is a humbling reminder that the economic Left is vulnerable to a complete lack of freedom, even when successful in achieving its overarching ideals. Often, the line between utopia and dystopia is blurred. It seems fitting that my next review could very well be Westad’s The Cold War.","link":"/2024/09/07/we-evgeny-zamyatin/"},{"title":"Soft Actor Critic (Visualized) Part 2: Lunar Lander Example from Scratch in Torch","text":"IntroductionJust like in the previous example using the CartPole environment, we will be using the Lunar Lander environment from OpenAI Gym. The goal of this example is to implement the Soft Actor Critic (SAC) algorithm from scratch using PyTorch. The SAC algorithm is a model-free, off-policy actor-critic algorithm that uses a stochastic policy and a value function to learn optimal policies in continuous action spaces.Like before, I will be using notation that matches the original paper (Haarnoja et al., 2018) and the code will be structured in a similar way to the previous example. The main difference is that we will be using a different environment and a different algorithm.Since the paper’s notation is critical to the understanding of the code, I highly recommend reading that alongside (or before) diving into the code.Part 1 of this series provides extensive details linking the theory to the code. In this part, we will focus on the implementation of the SAC algorithm in PyTorch for Lunar Lander. https://github.com/FranciscoRMendes/soft-actor-critic/blob/main/lunar-lander/LL_main_sac.py Example Data table { border-collapse: collapse; width: 100%; } th, td { border: 1px solid black; padding: 5px; text-align: center; } .action { background-color: #ffcccc; } /* Light Red */ .reward { background-color: #ccffcc; } /* Light Green */ .state { background-color: #ccccff; } /* Light Blue */ .done { background-color: #ffffcc; } /* Light Yellow */ .next-state { background-color: #ffccff; } /* Light Pink */ Action Reward State Done Next State Main Lateral x y v_x v_y angle angular velocity left contact right contact x y v_x v_y angle angular velocity left contact right contact 0.66336113 -0.485024 -1.56 0.00716772 1.4093536 0.7259957 -0.06963848 -0.0082988 -0.16444895 0 0 False 0.01442766 1.4081073 0.73378086 -0.05545701 -0.01600615 -0.15416077 0 0 0.87302077 0.8565877 -2.85810149 0.01442766 1.4081073 0.73378086 -0.05545701 -0.01600615 -0.15416077 0 0 False 0.02185297 1.4071543 0.7518369 -0.04247425 -0.02521554 -0.18420467 0 0 0.4880578 0.18216014 -2.248854395 0.02185297 1.4071543 0.7518369 -0.04247425 -0.02521554 -0.18420467 0 0 False 0.02941189 1.4065428 0.7646336 -0.02735517 -0.03385869 -0.17287907 0 0 0.0541396 -0.70224154 -0.765160122 0.02941189 1.4065428 0.7646336 -0.02735517 -0.03385869 -0.17287907 0 0 False 0.03697386 1.4056652 0.7634756 -0.03918146 -0.04105976 -0.14403483 0 0 Lunar Lander Dataset ExplanationThis dataset captures the experience of an agent in the Lunar Lander environment from OpenAI Gym. Each row represents a single transition (state, action, reward, next state) in the environment. Environment Details Action Main Engine: The thrust applied to the main engine. Lateral Thruster: The thrust applied to the left/right thrusters. Reward The reward received in this step. It is based on: Proximity to the landing pad. Smoothness of the landing. Fuel consumption. Avoiding crashes. State x, y: Position coordinates. v_x, v_y: Velocity components. theta: The lander’s rotation angle. omega: The rate of change of the angle. left contact, right contact: Binary indicators (0 or 1) showing whether the lander has made contact with the ground. Done True: The episode has ended (either successful landing or crash). False: The episode is still ongoing. Next State The same attributes as State, but after the action has been applied. Sample Game Play Game play 500 gamesYouTube video embedded Game play 500k games","link":"/2025/02/28/soft-actor-critic-lunar-lander/"}],"tags":[{"name":"Bayesian Statistics","slug":"Bayesian-Statistics","link":"/tags/Bayesian-Statistics/"},{"name":"economics","slug":"economics","link":"/tags/economics/"},{"name":"career","slug":"career","link":"/tags/career/"},{"name":"book review","slug":"book-review","link":"/tags/book-review/"},{"name":"politics","slug":"politics","link":"/tags/politics/"},{"name":"fiction","slug":"fiction","link":"/tags/fiction/"},{"name":"statistics","slug":"statistics","link":"/tags/statistics/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Low Rank Approximation","slug":"Low-Rank-Approximation","link":"/tags/Low-Rank-Approximation/"},{"name":"Matrix Factorization","slug":"Matrix-Factorization","link":"/tags/Matrix-Factorization/"},{"name":"Neural Networks","slug":"Neural-Networks","link":"/tags/Neural-Networks/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","link":"/tags/Convolutional-Neural-Networks/"},{"name":"Graph Neural Networks","slug":"Graph-Neural-Networks","link":"/tags/Graph-Neural-Networks/"},{"name":"Graph Convolutional Neural Networks","slug":"Graph-Convolutional-Neural-Networks","link":"/tags/Graph-Convolutional-Neural-Networks/"},{"name":"football","slug":"football","link":"/tags/football/"},{"name":"SVD","slug":"SVD","link":"/tags/SVD/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"LORA","slug":"LORA","link":"/tags/LORA/"},{"name":"fixed-point-theorem","slug":"fixed-point-theorem","link":"/tags/fixed-point-theorem/"},{"name":"matching","slug":"matching","link":"/tags/matching/"},{"name":"pure-mathematics","slug":"pure-mathematics","link":"/tags/pure-mathematics/"},{"name":"game-theory","slug":"game-theory","link":"/tags/game-theory/"},{"name":"signal processing","slug":"signal-processing","link":"/tags/signal-processing/"},{"name":"physics","slug":"physics","link":"/tags/physics/"},{"name":"gravity","slug":"gravity","link":"/tags/gravity/"},{"name":"mathematics","slug":"mathematics","link":"/tags/mathematics/"},{"name":"Quantization","slug":"Quantization","link":"/tags/Quantization/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"philosophy","slug":"philosophy","link":"/tags/philosophy/"}],"categories":[{"name":"statistics","slug":"statistics","link":"/categories/statistics/"},{"name":"opinion","slug":"opinion","link":"/categories/opinion/"},{"name":"book-review","slug":"book-review","link":"/categories/book-review/"},{"name":"machine learning","slug":"statistics/machine-learning","link":"/categories/statistics/machine-learning/"},{"name":"artificial-intelligence","slug":"artificial-intelligence","link":"/categories/artificial-intelligence/"},{"name":"economics","slug":"economics","link":"/categories/economics/"},{"name":"physics","slug":"physics","link":"/categories/physics/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"mathematics","slug":"physics/mathematics","link":"/categories/physics/mathematics/"}],"pages":[{"title":"About Me","text":"Hello! Welcome to my blog. I am a senior AI Engineer at Renesas Electronics America. In my current role I focus primarily on implementing deep neural networks on embedded systems. Most of my work is in the field of computer vision and signal processing, and requires a deep fundamental understanding of the math behind these fields to implement a neural network that is both light and fast. In previous lives, I have been an AI Consultant at the Boston Consulting Group and a researcher at the University of Chicago’s Thirty Million Words Center (under John List and Dana Suskind). I love mathematics and algorithms and was hoping to use this blog to share some of my thoughts and ideas on these topics. In particular, due to my background in mathematics I prefer to teach myself using small toy examples and proofs, that make the main idea clear and easy to understand without any code. To me, learning AI is all about implementing the algorithm by hand with a pen and paper! In my free time I play a lot of basketball and football (soccer) and you can find me in rec leagues across Chicago.","link":"/index.html"},{"title":"Curriculum Vitae","text":"Overall Experience Summary Experienced Data Scientist/ ML Engineer (8+ years) with subject expertise in building and deploying ML Algorithms built on sensor data for various use cases using advanced signal processing techniques (spectral analysis, cepstral analysis etc.). I have extensive experience building classifiers and regression models on all kinds of sensor data, most of my project experience has involved the 5 steps below: [Engineering Context] Gaining an understanding of the physics, chemistry or biology of the problem [Sensors] Understanding the nature of the sensors deployed to estimate various features that are predictive of the problem, their sample rates, short comings and power consumption. [Modeling] Iteratively building signal processing features that differentiate classes (or are statistically significant in predicting a scalar value) that help solve the problem [Model Refinement] Taking client feedback from the real world (either business, unit economics, physics), translating that to the ML model and fine tuning it (capture more TPR at the expense of FPs, find profitable points on the ROC curve). [Model Deployment] Once client is happy with the solution I assist in model deployment, this includes implementing mathematical techniques to reduce model size (low rank optimization in case of linear transforms), reduce latency using numerical representations (FP32 to INT8) and studying the tradeoffs between size, accuracy and latency for a given problem Project Experience At Renesas Electronics I lead the deployment of our neural network deployment on our own micro-controllers. This involves the creation and maintenance of an optimized ML pipeline that can ingest client sensor data, extract features (including our own proprietary signal processing features), build/ optimize a model and deploy the model. The following are some interesting technical things that I work on day to day: Finding FNNs, CNNs, DNNs that are small enough to fit on our smallest MCUs but complex enough to learn highly non-linear and complex patterns that differentiate classes. Using the client’s domain knowledge, to build features that translate the physics to the signal processing/ statistical domain. Sometimes this is a simple peak detector (turbine failure) but can be more complex such as differentiating two out of phase noisy sine waves (arc fault detection). Many of our feature extraction techniques are simply linear transforms of the form Ax + b, which is the same as a fully connected neural network layer, I spend a lot of time optimizing these linear operations so that they can be done faster or so that the matrix A can use less space Either by moving these operations from FP32 to INT8 Or approximating a series of matrix or tensor multiplies by a series of lower rank matrix multiplies Select Projects Here are a representative list of projects I have worked on in my over 8 year career as a data scientist. I have tried to be brief, but please do reach out for more information. Grass level detection for a large lawnmower manufacturer Current electric lawnmowers consume a lot of power and are often curtailed by their battery storage capacity. Our client was interested in adjusting RPM of the cutting blade based on grass length using a camera/ LIDAR sensor to detect grass length, with the goal of using less power for shorter grass lengths. We used a low-quality camera in conjunction with LIDAR data to classify grass into one of many discrete grass lengths. I was responsible for the following: Understanding the limits imposed by various sensor suites, since the goal was to save power, high quality sensors would solve the problem with high accuracy but would also consume more power Reducing the memory limits of deploying a CNN based algorithm using both low rank approximation as well as carrying out most matrix multiplies in INT8. LENA Wearable device for Parentese detection John List and Dana Suskind hypothesized that speaking to children in parentese leads to increased cognitive development and early verbalization. In my role as a grad student with an expertise in cepstral analysis, signal processing and Neural networks I was responsible for the following tasks Building a neural network that was capable of identifying parentese so that parents would receive a parentese score when using the device. In order to do this I had to build the following detectors Trigger/ wake point detection so that the device begins recording only when the parent is speaking Gender identification, since parentese characteristics differ in men and women Translating subjective definitions of parentese (sweeter sounding, baby talk sounding) into signal processing features, such as low rate of syllable generation, higher tones (relative to gender) and vowel elongation Working with the embedded product team to deploy the neural network on an embedded solution in a cost-effective manner (on Arduino in this case) In addition to ML, I led the design of several experiments to prove our hypothesis (economic experiments are very conceptually like drug trials). This included the creation of treatment and control groups (no parentese) controlling for a variety of factors such as education, language, acoustic features (controlling for naturally lower toned voices), race and tonal languages (Chinese). Arc Fault Detection Arc faults are a leading driver of cost in the power industry. For this project, I oversaw the development of a new signal processing feature based on sinusoidal wave forms of sensor data, since in “normal” conditions sensors that detect arc fault are almost perfectly sinusoidal but in arc fault condition they are not. In addition to the technical requirements of this role, I was responsible for Presenting results to the client in a very competitive environment (we were pitted against a competitor in a bake off) Since false positives in arc fault involve a fairly high cost, I was responsible for inferring and tuning our model to optimize for the clients exact costs (this ended up differentiating us from our competitors). Skills My primary skills are mathematics, statistics and python (with C++ for embedded side). I am adept at reading through and understanding mathematical concepts and then translating them into python code (that resembles mathematics in an elegant way). My mathematical skills allow me to customize and troubleshoot the inner workings of any pre-made python package as well as code up an algorithm from scratch from a research paper. On the python side, I am adept at the following key skills that I believe are necessary to support any ML effort Creating a positive and happy environment that is conducive to learning (and making mistakes). Writing clean, object oriented, modular code that can be tested Implementing extensive unit testing for code maintenance Implementing extensive regression testing to maintain accuracy, size and latency benchmarks on previously used data sets CI/CD tasks Project management tasks such as Jira, GitHub Issues, closing PRs, working with stakeholders.","link":"/curriculum/index.html"}]}